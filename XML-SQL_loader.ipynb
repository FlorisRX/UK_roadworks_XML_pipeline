{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6160d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to data/roadworks_data.duckdb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "import glob\n",
    "import os\n",
    "import polars as pl\n",
    "from IPython.display import display\n",
    "from lxml import etree\n",
    "\n",
    "# --- Configuration ---\n",
    "DUCKDB_FILE = 'data/roadworks_data.duckdb'\n",
    "RAW_NEW_TABLE_NAME = 'raw_new_roadworks'\n",
    "RAW_OLD_TABLE_NAME = 'raw_old_roadworks'\n",
    "UNIFIED_TABLE_NAME = 'uk_roadworks'\n",
    "\n",
    "NEW_DATA_DIRECTORY = 'data/new_format'     # data from Sept 2017 onwards\n",
    "OLD_DATA_DIRECTORY = 'data/old_format'     # data from August 2017 and earlier\n",
    "\n",
    "# Define the namespace map\n",
    "NSMAP = {'d': 'WebTeam'}\n",
    "\n",
    "# XPath to find the repeating record element\n",
    "NEW_ROADWORK_RECORD_XPATH = './/d:HE_PLANNED_WORKS'\n",
    "OLD_ROADWORK_RECORD_XPATH = './/ha_planned_works'\n",
    "\n",
    "# Columns for the 'new' format raw table\n",
    "RAW_NEW_COLUMNS = [\n",
    "    'source_filename', 'NEW_EVENT_NUMBER', 'OLD_REFERENCE_NUMBER', 'SDATE', 'EDATE',\n",
    "    'EXPDEL', 'DESCRIPTION', 'CLOSURE_TYPE', 'STATUS', 'PUBLISHED_DATE',\n",
    "    'CENTRE_EASTING', 'CENTRE_NORTHING', 'ROAD_NUMBERS'\n",
    "]\n",
    "\n",
    "# Columns for the 'old' format raw table\n",
    "RAW_OLD_COLUMNS = [\n",
    "    'source_filename', 'reference_number', 'start_date', 'end_date', 'expected_delay',\n",
    "    'description', 'closure_type', 'status', 'published_date', 'centre_easting',\n",
    "    'centre_northing', 'road', 'location', 'local_authority', 'traffic_management'\n",
    "]\n",
    "\n",
    "# Define XPaths for nested data relative to the NEW format HE_PLANNED_WORKS element\n",
    "NEW_COORD_XPATH = './d:EASTNORTH/d:Report/d:EASTINGNORTHING/d:EASTNORTH_Collection/d:EASTNORTH'\n",
    "NEW_ROAD_XPATH = './d:ROADS/d:Report/d:ROADS/d:ROAD_Collection/d:ROAD'\n",
    "\n",
    "\n",
    "# --- Helper function to run queries (optional, or use con.sql().pl() directly) ---\n",
    "def run_query_df(connection, sql_query):\n",
    "    \"\"\"Helper function to run a query and return a Polars DataFrame.\"\"\"\n",
    "    if not connection:\n",
    "        print(\"Error: Database connection is not established.\")\n",
    "        return None\n",
    "    try:\n",
    "        return connection.sql(sql_query).pl()\n",
    "    except duckdb.Error as e:\n",
    "        print(f\"Error running query:\\n{sql_query}\\nError: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Connect to DuckDB ---\n",
    "con = None\n",
    "try:\n",
    "    con = duckdb.connect(database=DUCKDB_FILE, read_only=False)\n",
    "    print(f\"Successfully connected to {DUCKDB_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to DuckDB: {e}\")\n",
    "\n",
    "pl.Config.set_tbl_rows(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a8697",
   "metadata": {},
   "source": [
    "## A. Define XML Parsing and Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0286a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record_new_format(record_element, source_filename):\n",
    "    \"\"\"\n",
    "    Extracts raw data from a 'new' format <HE_PLANNED_WORKS> element\n",
    "    into a dictionary matching RAW_NEW_COLUMNS.\n",
    "    \"\"\"\n",
    "    data = {col: None for col in RAW_NEW_COLUMNS} \n",
    "    data['source_filename'] = source_filename\n",
    "\n",
    "    data['NEW_EVENT_NUMBER'] = record_element.get('NEW_EVENT_NUMBER')\n",
    "    data['OLD_REFERENCE_NUMBER'] = record_element.get('OLD_REFERENCE_NUMBER')\n",
    "    data['SDATE'] = record_element.get('SDATE')\n",
    "    data['EDATE'] = record_element.get('EDATE')\n",
    "    data['EXPDEL'] = record_element.get('EXPDEL')\n",
    "    data['DESCRIPTION'] = record_element.get('DESCRIPTION')\n",
    "    data['CLOSURE_TYPE'] = record_element.get('CLOSURE_TYPE')\n",
    "    data['STATUS'] = record_element.get('STATUS')\n",
    "    data['PUBLISHED_DATE'] = record_element.get('PUBLISHED_DATE')\n",
    "\n",
    "    if data.get('NEW_EVENT_NUMBER') is None:\n",
    "        return None\n",
    "\n",
    "    coord_elements = record_element.xpath(NEW_COORD_XPATH, namespaces=NSMAP)\n",
    "    if coord_elements:\n",
    "        coord_element = coord_elements[0]\n",
    "        data['CENTRE_EASTING'] = coord_element.get('CENTRE_EASTING')\n",
    "        data['CENTRE_NORTHING'] = coord_element.get('CENTRE_NORTHING')\n",
    "\n",
    "    road_elements = record_element.xpath(NEW_ROAD_XPATH, namespaces=NSMAP)\n",
    "    if road_elements:\n",
    "        road_numbers_list = [road.get('ROAD_NUMBER') for road in road_elements if road.get('ROAD_NUMBER')]\n",
    "        data['ROAD_NUMBERS'] = '; '.join(road_numbers_list) if road_numbers_list else None\n",
    "    return data\n",
    "\n",
    "def extract_record_old_format(record_element, source_filename):\n",
    "    \"\"\"\n",
    "    Extracts raw data from an 'old' format <ha_planned_works> element\n",
    "    into a dictionary matching RAW_OLD_COLUMNS.\n",
    "    \"\"\"\n",
    "    data = {col: None for col in RAW_OLD_COLUMNS}\n",
    "    data['source_filename'] = source_filename\n",
    "\n",
    "    def get_text(tag_name):\n",
    "        element = record_element.find(tag_name)\n",
    "        return element.text.strip() if element is not None and element.text else None\n",
    "\n",
    "    for col_name in RAW_OLD_COLUMNS:\n",
    "        if col_name != 'source_filename':\n",
    "             data[col_name] = get_text(col_name)\n",
    "\n",
    "    if data.get('reference_number') is None:\n",
    "        return None\n",
    "    return data\n",
    "\n",
    "def process_directory(directory_path, record_xpath, extraction_func, nsmap=None):\n",
    "    \"\"\"\n",
    "    Processes all XML files in a directory, yielding each processed record.\n",
    "    \"\"\"\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "    parser = etree.XMLParser(recover=True, ns_clean=True)\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Warning: No XML files found in directory: {directory_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Processing Directory: {directory_path} ---\")\n",
    "    total_yielded_records = 0\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            records = root.xpath(record_xpath, namespaces=nsmap)\n",
    "            for record in records:\n",
    "                extracted_dict = extraction_func(record, filename)\n",
    "                if extracted_dict:\n",
    "                    yield extracted_dict\n",
    "                    total_yielded_records += 1\n",
    "        except Exception as e_file:\n",
    "            print(f\"  Error processing file {filename}: {e_file}. Skipping file.\")\n",
    "    print(f\"--- Directory Scan Complete: {directory_path}. Yielded {total_yielded_records} records. ---\")\n",
    "\n",
    "def load_data_in_batches(db_connection, table_name, target_columns, data_iterator, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Loads data from an iterator into a DuckDB table in batches.\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    total_inserted = 0\n",
    "    placeholders = ', '.join(['?'] * len(target_columns))\n",
    "    insert_sql = f'INSERT INTO \"{table_name}\" VALUES ({placeholders})'\n",
    "\n",
    "    print(f\"Starting batch insertion into '{table_name}'...\")\n",
    "    for record_dict in data_iterator:\n",
    "        row_values = [record_dict.get(col_name) for col_name in target_columns]\n",
    "        batch_data.append(row_values)\n",
    "        if len(batch_data) >= batch_size:\n",
    "            try:\n",
    "                db_connection.executemany(insert_sql, batch_data)\n",
    "                total_inserted += len(batch_data)\n",
    "                batch_data = []\n",
    "            except duckdb.Error as e:\n",
    "                print(f\"  Error inserting batch: {e}\")\n",
    "                batch_data = [] \n",
    "    if batch_data:\n",
    "        try:\n",
    "            db_connection.executemany(insert_sql, batch_data)\n",
    "            total_inserted += len(batch_data)\n",
    "        except duckdb.Error as e:\n",
    "            print(f\"  Error inserting final batch: {e}\")\n",
    "    print(f\"Batch insertion complete for '{table_name}'. Total records inserted: {total_inserted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4daaff",
   "metadata": {},
   "source": [
    "## B. Load Raw XML Data into Staging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or replacing table: raw_new_roadworks\n",
      "Table 'raw_new_roadworks' created/replaced successfully.\n",
      "\n",
      "Processing NEW format data...\n",
      "Starting batch insertion into 'raw_new_roadworks'...\n",
      "\n",
      "--- Processing Directory: data/new_format ---\n"
     ]
    }
   ],
   "source": [
    "if con:\n",
    "    # --- Create/Replace RAW NEW Table Structure ---\n",
    "    print(f\"Creating or replacing table: {RAW_NEW_TABLE_NAME}\")\n",
    "    new_column_defs = [f'\"{col}\" VARCHAR' for col in RAW_NEW_COLUMNS]\n",
    "    create_new_table_sql = f'CREATE OR REPLACE TABLE \"{RAW_NEW_TABLE_NAME}\" ({\", \".join(new_column_defs)})'\n",
    "    con.execute(create_new_table_sql)\n",
    "    print(f\"Table '{RAW_NEW_TABLE_NAME}' created/replaced successfully.\")\n",
    "\n",
    "    # --- Process and Load New Format Raw Data ---\n",
    "    print(\"\\nProcessing NEW format data...\")\n",
    "    new_data_iterator = process_directory(\n",
    "        directory_path=NEW_DATA_DIRECTORY,\n",
    "        record_xpath=NEW_ROADWORK_RECORD_XPATH,\n",
    "        extraction_func=extract_record_new_format,\n",
    "        nsmap=NSMAP\n",
    "    )\n",
    "    load_data_in_batches(con, RAW_NEW_TABLE_NAME, RAW_NEW_COLUMNS, new_data_iterator)\n",
    "\n",
    "    # --- Create/Replace RAW OLD Table Structure ---\n",
    "    print(f\"\\nCreating or replacing table: {RAW_OLD_TABLE_NAME}\")\n",
    "    old_column_defs = [f'\"{col}\" VARCHAR' for col in RAW_OLD_COLUMNS]\n",
    "    create_old_table_sql = f'CREATE OR REPLACE TABLE \"{RAW_OLD_TABLE_NAME}\" ({\", \".join(old_column_defs)})'\n",
    "    con.execute(create_old_table_sql)\n",
    "    print(f\"Table '{RAW_OLD_TABLE_NAME}' created/replaced successfully.\")\n",
    "\n",
    "    # --- Process and Load Old Format Raw Data ---\n",
    "    print(\"\\nProcessing OLD format data...\")\n",
    "    old_data_iterator = process_directory(\n",
    "        directory_path=OLD_DATA_DIRECTORY,\n",
    "        record_xpath=OLD_ROADWORK_RECORD_XPATH,\n",
    "        extraction_func=extract_record_old_format,\n",
    "        nsmap=None \n",
    "    )\n",
    "    load_data_in_batches(con, RAW_OLD_TABLE_NAME, RAW_OLD_COLUMNS, old_data_iterator)\n",
    "\n",
    "    con.commit()\n",
    "    print(\"\\nRaw data loading transaction committed.\")\n",
    "\n",
    "    # Verify final counts\n",
    "    count_new_df = run_query_df(con, f'SELECT COUNT(*) as count FROM \"{RAW_NEW_TABLE_NAME}\"')\n",
    "    count_old_df = run_query_df(con, f'SELECT COUNT(*) as count FROM \"{RAW_OLD_TABLE_NAME}\"')\n",
    "    if count_new_df is not None: print(f\"Verification: Table '{RAW_NEW_TABLE_NAME}' now contains {count_new_df[0, 'count']} rows.\")\n",
    "    if count_old_df is not None: print(f\"Verification: Table '{RAW_OLD_TABLE_NAME}' now contains {count_old_df[0, 'count']} rows.\")\n",
    "else:\n",
    "    print(\"Database connection not established. Skipping raw data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7862c",
   "metadata": {},
   "source": [
    "## C. Perform Data Type Conversions on Staging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Conversions\n",
    "if con:\n",
    "    print(f\"--- Adding Numeric Columns and Converting Data ---\")\n",
    "    # --- Process RAW_NEW_TABLE_NAME ---\n",
    "    print(f\"\\n--- Processing table for numeric conversion: {RAW_NEW_TABLE_NAME} ---\")\n",
    "    cols_to_convert_new_numeric = { \"OLD_REFERENCE_NUMBER\": \"BIGINT\", \"CENTRE_EASTING\": \"INTEGER\", \"CENTRE_NORTHING\": \"INTEGER\" }\n",
    "    for original_col, numeric_type in cols_to_convert_new_numeric.items():\n",
    "        new_col_name = f\"{original_col}_NUMERIC\"\n",
    "        con.execute(f'ALTER TABLE \"{RAW_NEW_TABLE_NAME}\" ADD COLUMN IF NOT EXISTS \"{new_col_name}\" {numeric_type};')\n",
    "        con.execute(f'UPDATE \"{RAW_NEW_TABLE_NAME}\" SET \"{new_col_name}\" = TRY_CAST(\"{original_col}\" AS {numeric_type});')\n",
    "    print(f\"Numeric columns processed for {RAW_NEW_TABLE_NAME}.\")\n",
    "\n",
    "    # --- Process RAW_OLD_TABLE_NAME ---\n",
    "    print(f\"\\n--- Processing table for numeric conversion: {RAW_OLD_TABLE_NAME} ---\")\n",
    "    cols_to_convert_old_numeric = { \"reference_number\": \"BIGINT\", \"centre_easting\": \"INTEGER\", \"centre_northing\": \"INTEGER\" }\n",
    "    for original_col, numeric_type in cols_to_convert_old_numeric.items():\n",
    "        new_col_name = f\"{original_col}_numeric\"\n",
    "        con.execute(f'ALTER TABLE \"{RAW_OLD_TABLE_NAME}\" ADD COLUMN IF NOT EXISTS \"{new_col_name}\" {numeric_type};')\n",
    "        con.execute(f'UPDATE \"{RAW_OLD_TABLE_NAME}\" SET \"{new_col_name}\" = TRY_CAST(\"{original_col}\" AS {numeric_type});')\n",
    "    print(f\"Numeric columns processed for {RAW_OLD_TABLE_NAME}.\")\n",
    "    con.commit()\n",
    "    print(\"\\nNumeric conversion changes committed.\")\n",
    "else:\n",
    "    print(\"Database connection not established. Skipping numeric conversions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60758fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime Conversions\n",
    "if con:\n",
    "    print(f\"--- Adding Timestamp Columns and Converting Data ---\")\n",
    "    # --- Process RAW_NEW_TABLE_NAME ---\n",
    "    print(f\"\\n--- Processing table for datetime conversion: {RAW_NEW_TABLE_NAME} ---\")\n",
    "    cols_to_convert_new_dt = {\n",
    "        \"SDATE\": {\"new_col\": \"SDATE_DT\", \"format\": \"%d-%b-%Y %H:%M\"},\n",
    "        \"EDATE\": {\"new_col\": \"EDATE_DT\", \"format\": \"%d-%b-%Y %H:%M\"},\n",
    "        \"PUBLISHED_DATE\": {\"new_col\": \"PUBLISHED_DATE_DT\", \"format\": \"ISO\"}\n",
    "    }\n",
    "    for original_col, details in cols_to_convert_new_dt.items():\n",
    "        new_col_name = details[\"new_col\"]\n",
    "        original_format = details[\"format\"]\n",
    "        con.execute(f'ALTER TABLE \"{RAW_NEW_TABLE_NAME}\" ADD COLUMN IF NOT EXISTS \"{new_col_name}\" TIMESTAMP;')\n",
    "        if original_format == \"ISO\":\n",
    "            con.execute(f'UPDATE \"{RAW_NEW_TABLE_NAME}\" SET \"{new_col_name}\" = TRY_CAST(\"{original_col}\" AS TIMESTAMP);')\n",
    "        else:\n",
    "            con.execute(f'UPDATE \"{RAW_NEW_TABLE_NAME}\" SET \"{new_col_name}\" = TRY_STRPTIME(trim(\"{original_col}\"), \\'{original_format}\\');')\n",
    "    print(f\"Timestamp columns processed for {RAW_NEW_TABLE_NAME}.\")\n",
    "\n",
    "    # --- Process RAW_OLD_TABLE_NAME ---\n",
    "    print(f\"\\n--- Processing table for datetime conversion: {RAW_OLD_TABLE_NAME} ---\")\n",
    "    cols_to_convert_old_dt = {\n",
    "        \"start_date\": {\"new_col\": \"start_date_dt\", \"format\": \"ISO\"},\n",
    "        \"end_date\": {\"new_col\": \"end_date_dt\", \"format\": \"ISO\"},\n",
    "        \"published_date\": {\"new_col\": \"published_date_dt\", \"format\": \"ISO\"}\n",
    "    }\n",
    "    for original_col, details in cols_to_convert_old_dt.items():\n",
    "        new_col_name = details[\"new_col\"]\n",
    "        con.execute(f'ALTER TABLE \"{RAW_OLD_TABLE_NAME}\" ADD COLUMN IF NOT EXISTS \"{new_col_name}\" TIMESTAMP;')\n",
    "        con.execute(f'UPDATE \"{RAW_OLD_TABLE_NAME}\" SET \"{new_col_name}\" = TRY_CAST(\"{original_col}\" AS TIMESTAMP);')\n",
    "    print(f\"Timestamp columns processed for {RAW_OLD_TABLE_NAME}.\")\n",
    "    con.commit()\n",
    "    print(\"\\nDatetime conversion changes committed.\")\n",
    "else:\n",
    "    print(\"Database connection not established. Skipping datetime conversions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d3b62",
   "metadata": {},
   "source": [
    "## D. Perform Coordinate System Conversion (OSGB36 to WGS84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1834d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if con:\n",
    "    print(f\"--- Converting Coordinates to WGS84 ---\")\n",
    "    try:\n",
    "        con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "        gsb_file_path = 'OSTN15_NTv2_OSGBtoETRS.gsb' # Assumed to be in the working directory or accessible path\n",
    "        \n",
    "        source_crs_epsg27700 = 'EPSG:27700'\n",
    "        source_crs_nadgrids = f'+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +nadgrids={gsb_file_path} +type=crs'\n",
    "        target_crs_epsg = 'EPSG:4326'\n",
    "        chosen_source_crs = source_crs_epsg27700 # Default\n",
    "        crs_method_message = f\"Using '{source_crs_epsg27700}' (fallback).\"\n",
    "\n",
    "        if os.path.exists(gsb_file_path):\n",
    "            # Simple test with nadgrids to see if it works without erroring out immediately\n",
    "            try:\n",
    "                test_nad_df = run_query_df(con, f\"SELECT ST_Transform(ST_Point(529090, 179645), '{source_crs_nadgrids}', '{target_crs_epsg}', always_xy := true) AS test_geom;\")\n",
    "                if test_nad_df is not None and not test_nad_df.is_empty() and test_nad_df[0, \"test_geom\"] is not None:\n",
    "                     # Check if result is not inf\n",
    "                    test_val_x = run_query_df(con, f\"SELECT ST_X(ST_Transform(ST_Point(529090, 179645), '{source_crs_nadgrids}', '{target_crs_epsg}', always_xy := true)) as val;\")\n",
    "                    if test_val_x is not None and abs(test_val_x[0,'val']) != float('inf'):\n",
    "                        chosen_source_crs = source_crs_nadgrids\n",
    "                        crs_method_message = f\"Using '{source_crs_nadgrids}' (more accurate, with GSB file).\"\n",
    "                    else:\n",
    "                        print(f\"Warning: NADGRIDS transformation resulted in INF, falling back to {source_crs_epsg27700}.\")\n",
    "                else:\n",
    "                    print(f\"Warning: NADGRIDS transformation test failed or returned empty/null, falling back to {source_crs_epsg27700}.\")\n",
    "            except Exception as e_nad:\n",
    "                print(f\"Warning: NADGRIDS transformation test failed with error: {e_nad}. Falling back to {source_crs_epsg27700}.\")\n",
    "        else:\n",
    "            print(f\"INFO: GSB file '{gsb_file_path}' not found. Using '{source_crs_epsg27700}'.\")\n",
    "        \n",
    "        print(crs_method_message)\n",
    "\n",
    "        tables_to_transform = {\n",
    "            RAW_NEW_TABLE_NAME: {\"easting_col\": \"CENTRE_EASTING_NUMERIC\", \"northing_col\": \"CENTRE_NORTHING_NUMERIC\"},\n",
    "            RAW_OLD_TABLE_NAME: {\"easting_col\": \"centre_easting_numeric\", \"northing_col\": \"centre_northing_numeric\"}\n",
    "        }\n",
    "\n",
    "        for table_name, cols_info in tables_to_transform.items():\n",
    "            easting_col = cols_info[\"easting_col\"]\n",
    "            northing_col = cols_info[\"northing_col\"]\n",
    "            print(f\"\\nProcessing table for coordinate conversion: {table_name}\")\n",
    "            \n",
    "            con.execute(f'ALTER TABLE \"{table_name}\" ADD COLUMN IF NOT EXISTS longitude_wgs84 DOUBLE;')\n",
    "            con.execute(f'ALTER TABLE \"{table_name}\" ADD COLUMN IF NOT EXISTS latitude_wgs84 DOUBLE;')\n",
    "            con.execute(f'UPDATE \"{table_name}\" SET longitude_wgs84 = NULL, latitude_wgs84 = NULL;') # Clear existing\n",
    "\n",
    "            update_sql = f'''\n",
    "            UPDATE \"{table_name}\"\n",
    "            SET\n",
    "                longitude_wgs84 = ST_X(ST_Transform(ST_Point(\"{easting_col}\", \"{northing_col}\"), '{chosen_source_crs}', '{target_crs_epsg}', always_xy := true)),\n",
    "                latitude_wgs84 = ST_Y(ST_Transform(ST_Point(\"{easting_col}\", \"{northing_col}\"), '{chosen_source_crs}', '{target_crs_epsg}', always_xy := true))\n",
    "            WHERE \"{easting_col}\" IS NOT NULL AND \"{northing_col}\" IS NOT NULL AND \"{easting_col}\" != 0 AND \"{northing_col}\" != 0;\n",
    "            '''\n",
    "            con.execute(update_sql)\n",
    "            print(f\"Coordinate transformation complete for {table_name}.\")\n",
    "        \n",
    "        con.commit()\n",
    "        print(\"\\nCoordinate transformation changes committed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during coordinate transformation: {e}\")\n",
    "        if con: con.rollback()\n",
    "else:\n",
    "    print(\"Database connection not established. Skipping coordinate conversions.\")\n",
    "\n",
    "# Display some samples with new WGS84 coordinates\n",
    "if con and chosen_source_crs:\n",
    "    print(f\"\\nSample from '{RAW_NEW_TABLE_NAME}' with WGS84 coordinates:\")\n",
    "    sample_new_coords = run_query_df(con, f'SELECT \"NEW_EVENT_NUMBER\", \"CENTRE_EASTING_NUMERIC\", \"CENTRE_NORTHING_NUMERIC\", longitude_wgs84, latitude_wgs84 FROM \"{RAW_NEW_TABLE_NAME}\" WHERE longitude_wgs84 IS NOT NULL LIMIT 3')\n",
    "    if sample_new_coords is not None: display(sample_new_coords)\n",
    "\n",
    "    print(f\"\\nSample from '{RAW_OLD_TABLE_NAME}' with WGS84 coordinates:\")\n",
    "    sample_old_coords = run_query_df(con, f'SELECT \"reference_number\", \"centre_easting_numeric\", \"centre_northing_numeric\", longitude_wgs84, latitude_wgs84 FROM \"{RAW_OLD_TABLE_NAME}\" WHERE longitude_wgs84 IS NOT NULL LIMIT 3')\n",
    "    if sample_old_coords is not None: display(sample_old_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d0979",
   "metadata": {},
   "source": [
    "## E. Define and create unified table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b6ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'uk_roadworks' created/re-created successfully.\n",
      "Data inserted from 'raw_new_roadworks' into 'uk_roadworks'.\n",
      "Data inserted from 'raw_old_roadworks' into 'uk_roadworks'.\n",
      "\n",
      "--- Verifying uk_roadworks ---\n",
      "Total rows in 'uk_roadworks': 23421\n",
      "\n",
      "Sample of 5 rows from 'uk_roadworks':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 19)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>legacy_reference_id</th><th>start_datetime</th><th>end_datetime</th><th>published_datetime</th><th>expected_delay</th><th>description</th><th>closure_type</th><th>status</th><th>road_names</th><th>easting_osgb</th><th>northing_osgb</th><th>longitude_wgs84</th><th>latitude_wgs84</th><th>location_detail</th><th>local_authority</th><th>traffic_management_type</th><th>source_filename</th><th>data_source_format</th></tr><tr><td>str</td><td>i64</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;00026976-005&quot;</td><td>null</td><td>2018-02-26 21:00:00</td><td>2018-02-28 06:00:00</td><td>2018-02-22 16:49:17</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A3 northbound Sheet Link entry…</td><td>&quot;Area Renewals&quot;</td><td>&quot;Published&quot;</td><td>&quot;A3&quot;</td><td>475209</td><td>124975</td><td>-0.929132</td><td>51.019241</td><td>null</td><td>null</td><td>null</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;new_xml&quot;</td></tr><tr><td>&quot;00004020-008&quot;</td><td>4188720</td><td>2018-01-08 20:00:00</td><td>2018-03-10 06:00:00</td><td>2018-02-22 10:13:27</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;A14 Westbound\n",
       "Jct 58 to Jct 57…</td><td>&quot;Area Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;A14&quot;</td><td>614569</td><td>241115</td><td>1.126274</td><td>52.026925</td><td>null</td><td>null</td><td>null</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;new_xml&quot;</td></tr><tr><td>&quot;00001459-026&quot;</td><td>4215713</td><td>2017-07-31 14:47:00</td><td>2018-04-01 06:00:00</td><td>2018-02-15 14:38:05</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;M1 northbound and southbound T…</td><td>&quot;Major Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;M1&quot;</td><td>445124</td><td>364308</td><td>-1.32637</td><td>53.173976</td><td>null</td><td>null</td><td>null</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;new_xml&quot;</td></tr><tr><td>&quot;00027883-003&quot;</td><td>null</td><td>2018-02-12 20:00:00</td><td>2018-03-17 06:00:00</td><td>2018-02-21 10:36:47</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;A259, east and westbound betwe…</td><td>&quot;Area Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;A259&quot;</td><td>596442</td><td>123787</td><td>0.797101</td><td>50.979974</td><td>null</td><td>null</td><td>null</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;new_xml&quot;</td></tr><tr><td>&quot;00026799-002&quot;</td><td>null</td><td>2018-02-10 22:00:00</td><td>2018-03-22 06:00:00</td><td>2018-02-22 14:08:43</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A3 northbound Compton to Denni…</td><td>&quot;Regional Technology Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;A3&quot;</td><td>498261</td><td>150727</td><td>-0.593562</td><td>51.247262</td><td>null</td><td>null</td><td>null</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;new_xml&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 19)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ event_id  ┆ legacy_re ┆ start_dat ┆ end_datet ┆ … ┆ local_aut ┆ traffic_m ┆ source_fi ┆ data_sou │\n",
       "│ ---       ┆ ference_i ┆ etime     ┆ ime       ┆   ┆ hority    ┆ anagement ┆ lename    ┆ rce_form │\n",
       "│ str       ┆ d         ┆ ---       ┆ ---       ┆   ┆ ---       ┆ _type     ┆ ---       ┆ at       │\n",
       "│           ┆ ---       ┆ datetime[ ┆ datetime[ ┆   ┆ str       ┆ ---       ┆ str       ┆ ---      │\n",
       "│           ┆ i64       ┆ μs]       ┆ μs]       ┆   ┆           ┆ str       ┆           ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 00026976- ┆ null      ┆ 2018-02-2 ┆ 2018-02-2 ┆ … ┆ null      ┆ null      ┆ he_roadwo ┆ new_xml  │\n",
       "│ 005       ┆           ┆ 6         ┆ 8         ┆   ┆           ┆           ┆ rks_2018_ ┆          │\n",
       "│           ┆           ┆ 21:00:00  ┆ 06:00:00  ┆   ┆           ┆           ┆ 02_26.xml ┆          │\n",
       "│ 00004020- ┆ 4188720   ┆ 2018-01-0 ┆ 2018-03-1 ┆ … ┆ null      ┆ null      ┆ he_roadwo ┆ new_xml  │\n",
       "│ 008       ┆           ┆ 8         ┆ 0         ┆   ┆           ┆           ┆ rks_2018_ ┆          │\n",
       "│           ┆           ┆ 20:00:00  ┆ 06:00:00  ┆   ┆           ┆           ┆ 02_26.xml ┆          │\n",
       "│ 00001459- ┆ 4215713   ┆ 2017-07-3 ┆ 2018-04-0 ┆ … ┆ null      ┆ null      ┆ he_roadwo ┆ new_xml  │\n",
       "│ 026       ┆           ┆ 1         ┆ 1         ┆   ┆           ┆           ┆ rks_2018_ ┆          │\n",
       "│           ┆           ┆ 14:47:00  ┆ 06:00:00  ┆   ┆           ┆           ┆ 02_26.xml ┆          │\n",
       "│ 00027883- ┆ null      ┆ 2018-02-1 ┆ 2018-03-1 ┆ … ┆ null      ┆ null      ┆ he_roadwo ┆ new_xml  │\n",
       "│ 003       ┆           ┆ 2         ┆ 7         ┆   ┆           ┆           ┆ rks_2018_ ┆          │\n",
       "│           ┆           ┆ 20:00:00  ┆ 06:00:00  ┆   ┆           ┆           ┆ 02_26.xml ┆          │\n",
       "│ 00026799- ┆ null      ┆ 2018-02-1 ┆ 2018-03-2 ┆ … ┆ null      ┆ null      ┆ he_roadwo ┆ new_xml  │\n",
       "│ 002       ┆           ┆ 0         ┆ 2         ┆   ┆           ┆           ┆ rks_2018_ ┆          │\n",
       "│           ┆           ┆ 22:00:00  ┆ 06:00:00  ┆   ┆           ┆           ┆ 02_26.xml ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes committed.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define and Create Unified Table ---\n",
    "# Uses the transformed columns (_NUMERIC, _DT, WGS84) from the raw tables.\n",
    "\n",
    "create_unified_table_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE \"{UNIFIED_TABLE_NAME}\" (\n",
    "    event_id VARCHAR,                        -- NEW_EVENT_NUMBER (new) or reference_number (old)\n",
    "    legacy_reference_id BIGINT,              -- OLD_REFERENCE_NUMBER_NUMERIC (new)\n",
    "    start_datetime TIMESTAMP,                -- SDATE_DT (new) or start_date_dt (old)\n",
    "    end_datetime TIMESTAMP,                  -- EDATE_DT (new) or end_date_dt (old)\n",
    "    published_datetime TIMESTAMP,            -- PUBLISHED_DATE_DT (new) or published_date_dt (old)\n",
    "    expected_delay VARCHAR,                  -- EXPDEL (new) or expected_delay (old)\n",
    "    description VARCHAR,                     -- DESCRIPTION (new) or description (old)\n",
    "    closure_type VARCHAR,                    -- CLOSURE_TYPE (new) or closure_type (old)\n",
    "    status VARCHAR,                          -- STATUS (new) or status (old)\n",
    "    road_names VARCHAR,                      -- ROAD_NUMBERS (new) or road (old)\n",
    "    easting_osgb INTEGER,                    -- CENTRE_EASTING_NUMERIC (new) or centre_easting_numeric (old)\n",
    "    northing_osgb INTEGER,                   -- CENTRE_NORTHING_NUMERIC (new) or centre_northing_numeric (old)\n",
    "    longitude_wgs84 DOUBLE,\n",
    "    latitude_wgs84 DOUBLE,\n",
    "    location_detail VARCHAR,                 -- location (old only)\n",
    "    local_authority VARCHAR,                 -- local_authority (old only)\n",
    "    traffic_management_type VARCHAR,         -- traffic_management (old only)\n",
    "    source_filename VARCHAR,\n",
    "    data_source_format VARCHAR              -- 'new_xml' or 'old_xml'\n",
    ");\n",
    "\"\"\"\n",
    "if con:\n",
    "    con.execute(create_unified_table_sql)\n",
    "    print(f\"Table '{UNIFIED_TABLE_NAME}' created/re-created successfully.\")\n",
    "\n",
    "    # --- 2. Populate Unified Table from New Format Data ---\n",
    "    insert_from_new_sql = f\"\"\"\n",
    "    INSERT INTO \"{UNIFIED_TABLE_NAME}\"\n",
    "    SELECT\n",
    "        \"NEW_EVENT_NUMBER\" AS event_id,\n",
    "        \"OLD_REFERENCE_NUMBER_NUMERIC\" AS legacy_reference_id,\n",
    "        \"SDATE_DT\" AS start_datetime,\n",
    "        \"EDATE_DT\" AS end_datetime,\n",
    "        \"PUBLISHED_DATE_DT\" AS published_datetime,\n",
    "        \"EXPDEL\" AS expected_delay,\n",
    "        \"DESCRIPTION\" AS description,\n",
    "        \"CLOSURE_TYPE\" AS closure_type,\n",
    "        \"STATUS\" AS status,\n",
    "        \"ROAD_NUMBERS\" AS road_names,\n",
    "        \"CENTRE_EASTING_NUMERIC\" AS easting_osgb,\n",
    "        \"CENTRE_NORTHING_NUMERIC\" AS northing_osgb,\n",
    "        longitude_wgs84,\n",
    "        latitude_wgs84,\n",
    "        NULL AS location_detail,\n",
    "        NULL AS local_authority,\n",
    "        NULL AS traffic_management_type,\n",
    "        source_filename,\n",
    "        'new_xml' AS data_source_format\n",
    "    FROM \"{RAW_NEW_TABLE_NAME}\";\n",
    "    \"\"\"\n",
    "    con.execute(insert_from_new_sql)\n",
    "    print(f\"Data inserted from '{RAW_NEW_TABLE_NAME}' into '{UNIFIED_TABLE_NAME}'.\")\n",
    "\n",
    "    # --- 3. Populate Unified Table from Old Format Data ---\n",
    "    insert_from_old_sql = f\"\"\"\n",
    "    INSERT INTO \"{UNIFIED_TABLE_NAME}\"\n",
    "    SELECT\n",
    "        \"reference_number\" AS event_id, -- Using original string ID, could also use reference_number_numeric if desired and schema changed\n",
    "        NULL AS legacy_reference_id,\n",
    "        \"start_date_dt\" AS start_datetime,\n",
    "        \"end_date_dt\" AS end_datetime,\n",
    "        \"published_date_dt\" AS published_datetime,\n",
    "        \"expected_delay\" AS expected_delay,\n",
    "        \"description\" AS description,\n",
    "        \"closure_type\" AS closure_type,\n",
    "        \"status\" AS status,\n",
    "        \"road\" AS road_names,\n",
    "        \"centre_easting_numeric\" AS easting_osgb,\n",
    "        \"centre_northing_numeric\" AS northing_osgb,\n",
    "        longitude_wgs84,\n",
    "        latitude_wgs84,\n",
    "        \"location\" AS location_detail,\n",
    "        \"local_authority\" AS local_authority,\n",
    "        \"traffic_management\" AS traffic_management_type,\n",
    "        source_filename,\n",
    "        'old_xml' AS data_source_format\n",
    "    FROM \"{RAW_OLD_TABLE_NAME}\";\n",
    "    \"\"\"\n",
    "    con.execute(insert_from_old_sql)\n",
    "    print(f\"Data inserted from '{RAW_OLD_TABLE_NAME}' into '{UNIFIED_TABLE_NAME}'.\")\n",
    "\n",
    "    # --- 4. Verification (Example) ---\n",
    "    print(f\"\\n--- Verifying {UNIFIED_TABLE_NAME} ---\")\n",
    "    total_rows_unified_df = run_query_df(con, f'SELECT COUNT(*) as count FROM \"{UNIFIED_TABLE_NAME}\"')\n",
    "    if total_rows_unified_df is not None:\n",
    "        print(f\"Total rows in '{UNIFIED_TABLE_NAME}': {total_rows_unified_df[0, 'count']}\")\n",
    "\n",
    "    print(f\"\\nSample of 5 rows from '{UNIFIED_TABLE_NAME}':\")\n",
    "    sample_unified_df = run_query_df(con, f'SELECT * FROM \"{UNIFIED_TABLE_NAME}\" LIMIT 5')\n",
    "    if sample_unified_df is not None:\n",
    "        display(sample_unified_df)\n",
    "\n",
    "    con.commit()\n",
    "    print(\"Changes for unified table committed.\")\n",
    "else:\n",
    "    print(\"Database connection not established. Skipping unified table creation and population.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd069c5c",
   "metadata": {},
   "source": [
    "## F. Final Data Quality Checks on Unified Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e67ff",
   "metadata": {},
   "source": [
    "### F.1. Unique Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5422b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(20)\n",
    "\n",
    "# (Re-)open the connection\n",
    "if con:\n",
    "    con.close()\n",
    "con = duckdb.connect(database=DUCKDB_FILE, read_only=False)\n",
    "\n",
    "# Define common placeholders for checking original string values\n",
    "PLACEHOLDERS_LOWER = [\"\", \"none\", \"n/a\", \"null\", \"unknown\"]\n",
    "PLACEHOLDERS_SQL_LIST_STR = f\"({', '.join([f'{pl!r}' for pl in PLACEHOLDERS_LOWER])})\"\n",
    "\n",
    "categorical_columns_unified = [\n",
    "    'expected_delay',\n",
    "    'closure_type',\n",
    "    'status',\n",
    "    'road_names',\n",
    "    'local_authority',\n",
    "    'traffic_management_type',\n",
    "    'data_source_format'\n",
    "]\n",
    "print(f\"\\n--- Unique Categorical Values in '{UNIFIED_TABLE_NAME}' ---\")\n",
    "for col in categorical_columns_unified:\n",
    "    print(f\"\\nDistinct values for '{col}':\")\n",
    "    query = f\"\"\"\n",
    "        SELECT \"{col}\", COUNT(*) as count\n",
    "        FROM \"{UNIFIED_TABLE_NAME}\"\n",
    "        GROUP BY \"{col}\"\n",
    "        ORDER BY count DESC;\n",
    "    \"\"\"\n",
    "    df = run_query_df(con, query)\n",
    "    if df is not None:\n",
    "        display(df)\n",
    "    else:\n",
    "        print(f\"Could not retrieve distinct values for '{col}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ef780",
   "metadata": {},
   "source": [
    "### F.2. Validate Date Ranges (End Date before Start Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking for Invalid Date Ranges in 'uk_roadworks' (end_datetime < start_datetime) ---\n",
      "No records found where end_datetime is before start_datetime.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Checking for Invalid Date Ranges in '{UNIFIED_TABLE_NAME}' (end_datetime < start_datetime) ---\")\n",
    "query_invalid_dates = f\"\"\"\n",
    "    SELECT event_id, source_filename, start_datetime, end_datetime, description\n",
    "    FROM \"{UNIFIED_TABLE_NAME}\"\n",
    "    WHERE end_datetime < start_datetime;\n",
    "\"\"\"\n",
    "df_invalid_dates = run_query_df(con, query_invalid_dates)\n",
    "if df_invalid_dates is not None:\n",
    "    if not df_invalid_dates.is_empty():\n",
    "        print(f\"Found {df_invalid_dates.height} records where end_datetime is before start_datetime:\")\n",
    "        display(df_invalid_dates)\n",
    "    else:\n",
    "        print(\"No records found where end_datetime is before start_datetime.\")\n",
    "else:\n",
    "    print(\"Could not execute query to check for invalid date ranges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581965a",
   "metadata": {},
   "source": [
    "### F.3. Extreme Coordinate Check (WGS84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7bf129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extreme Coordinates in 'uk_roadworks' (WGS84) ---\n",
      "\n",
      "Northmost point:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>description</th><th>road_names</th><th>latitude_wgs84</th><th>longitude_wgs84</th><th>source_filename</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;00034223-002&quot;</td><td>&quot;A1 Berwick upon Tweed.&nbsp;&nbsp;Southb…</td><td>&quot;A1&quot;</td><td>55.806145</td><td>-2.042906</td><td>&quot;he_roadworks_2018_09_03.xml&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 6)\n",
       "┌──────────────┬─────────────────┬────────────┬────────────────┬─────────────────┬─────────────────┐\n",
       "│ event_id     ┆ description     ┆ road_names ┆ latitude_wgs84 ┆ longitude_wgs84 ┆ source_filename │\n",
       "│ ---          ┆ ---             ┆ ---        ┆ ---            ┆ ---             ┆ ---             │\n",
       "│ str          ┆ str             ┆ str        ┆ f64            ┆ f64             ┆ str             │\n",
       "╞══════════════╪═════════════════╪════════════╪════════════════╪═════════════════╪═════════════════╡\n",
       "│ 00034223-002 ┆ A1 Berwick upon ┆ A1         ┆ 55.806145      ┆ -2.042906       ┆ he_roadworks_20 │\n",
       "│              ┆ Tweed.  Southb… ┆            ┆                ┆                 ┆ 18_09_03.xml    │\n",
       "└──────────────┴─────────────────┴────────────┴────────────────┴─────────────────┴─────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Southmost point:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>description</th><th>road_names</th><th>latitude_wgs84</th><th>longitude_wgs84</th><th>source_filename</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;00331971-001&quot;</td><td>&quot;A30 Lands End Roundabout used …</td><td>&quot;A30&quot;</td><td>50.129302</td><td>-5.513296</td><td>&quot;nh_roadworks_2024_10_7.xml&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 6)\n",
       "┌──────────────┬─────────────────┬────────────┬────────────────┬─────────────────┬─────────────────┐\n",
       "│ event_id     ┆ description     ┆ road_names ┆ latitude_wgs84 ┆ longitude_wgs84 ┆ source_filename │\n",
       "│ ---          ┆ ---             ┆ ---        ┆ ---            ┆ ---             ┆ ---             │\n",
       "│ str          ┆ str             ┆ str        ┆ f64            ┆ f64             ┆ str             │\n",
       "╞══════════════╪═════════════════╪════════════╪════════════════╪═════════════════╪═════════════════╡\n",
       "│ 00331971-001 ┆ A30 Lands End   ┆ A30        ┆ 50.129302      ┆ -5.513296       ┆ nh_roadworks_20 │\n",
       "│              ┆ Roundabout used ┆            ┆                ┆                 ┆ 24_10_7.xml     │\n",
       "│              ┆ …               ┆            ┆                ┆                 ┆                 │\n",
       "└──────────────┴─────────────────┴────────────┴────────────────┴─────────────────┴─────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eastmost point:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>description</th><th>road_names</th><th>latitude_wgs84</th><th>longitude_wgs84</th><th>source_filename</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;2646860&quot;</td><td>&quot;S/B&nbsp;&nbsp;lane 1 closure for Inspec…</td><td>&quot;A12&quot;</td><td>52.485251</td><td>1.756125</td><td>&quot;ha-roadworks_2013_11_04.xml&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 6)\n",
       "┌──────────┬─────────────┬────────────┬────────────────┬─────────────────┬─────────────────────────┐\n",
       "│ event_id ┆ description ┆ road_names ┆ latitude_wgs84 ┆ longitude_wgs84 ┆ source_filename         │\n",
       "│ ---      ┆ ---         ┆ ---        ┆ ---            ┆ ---             ┆ ---                     │\n",
       "│ str      ┆ str         ┆ str        ┆ f64            ┆ f64             ┆ str                     │\n",
       "╞══════════╪═════════════╪════════════╪════════════════╪═════════════════╪═════════════════════════╡\n",
       "│ 2646860  ┆ S/B  lane 1 ┆ A12        ┆ 52.485251      ┆ 1.756125        ┆ ha-roadworks_2013_11_04 │\n",
       "│          ┆ closure for ┆            ┆                ┆                 ┆ .xml                    │\n",
       "│          ┆ Inspec…     ┆            ┆                ┆                 ┆                         │\n",
       "└──────────┴─────────────┴────────────┴────────────────┴─────────────────┴─────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Westmost point:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>description</th><th>road_names</th><th>latitude_wgs84</th><th>longitude_wgs84</th><th>source_filename</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;00331971-001&quot;</td><td>&quot;A30 Lands End Roundabout used …</td><td>&quot;A30&quot;</td><td>50.129302</td><td>-5.513296</td><td>&quot;nh_roadworks_2024_10_7.xml&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 6)\n",
       "┌──────────────┬─────────────────┬────────────┬────────────────┬─────────────────┬─────────────────┐\n",
       "│ event_id     ┆ description     ┆ road_names ┆ latitude_wgs84 ┆ longitude_wgs84 ┆ source_filename │\n",
       "│ ---          ┆ ---             ┆ ---        ┆ ---            ┆ ---             ┆ ---             │\n",
       "│ str          ┆ str             ┆ str        ┆ f64            ┆ f64             ┆ str             │\n",
       "╞══════════════╪═════════════════╪════════════╪════════════════╪═════════════════╪═════════════════╡\n",
       "│ 00331971-001 ┆ A30 Lands End   ┆ A30        ┆ 50.129302      ┆ -5.513296       ┆ nh_roadworks_20 │\n",
       "│              ┆ Roundabout used ┆            ┆                ┆                 ┆ 24_10_7.xml     │\n",
       "│              ┆ …               ┆            ┆                ┆                 ┆                 │\n",
       "└──────────────┴─────────────────┴────────────┴────────────────┴─────────────────┴─────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"\\n--- Extreme Coordinates in '{UNIFIED_TABLE_NAME}' (WGS84) ---\")\n",
    "extreme_coords_queries = {\n",
    "    \"Northmost\": f\"\"\"SELECT event_id, description, road_names, latitude_wgs84, longitude_wgs84, source_filename FROM \"{UNIFIED_TABLE_NAME}\" WHERE latitude_wgs84 IS NOT NULL ORDER BY latitude_wgs84 DESC LIMIT 1\"\"\",\n",
    "    \"Southmost\": f\"\"\"SELECT event_id, description, road_names, latitude_wgs84, longitude_wgs84, source_filename FROM \"{UNIFIED_TABLE_NAME}\" WHERE latitude_wgs84 IS NOT NULL ORDER BY latitude_wgs84 ASC LIMIT 1\"\"\",\n",
    "    \"Eastmost\":  f\"\"\"SELECT event_id, description, road_names, latitude_wgs84, longitude_wgs84, source_filename FROM \"{UNIFIED_TABLE_NAME}\" WHERE longitude_wgs84 IS NOT NULL ORDER BY longitude_wgs84 DESC LIMIT 1\"\"\",\n",
    "    \"Westmost\":  f\"\"\"SELECT event_id, description, road_names, latitude_wgs84, longitude_wgs84, source_filename FROM \"{UNIFIED_TABLE_NAME}\" WHERE longitude_wgs84 IS NOT NULL ORDER BY longitude_wgs84 ASC LIMIT 1\"\"\"\n",
    "}\n",
    "for name, query in extreme_coords_queries.items():\n",
    "    print(f\"\\n{name} point:\")\n",
    "    df_coord = run_query_df(con, query)\n",
    "    if df_coord is not None:\n",
    "        display(df_coord)\n",
    "    else:\n",
    "        print(f\"Could not retrieve {name} coordinate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d55b0a",
   "metadata": {},
   "source": [
    "### F.4. Type Conversion Discrepancy Checks (Unified NULL vs. Raw Original Not NULL/Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4005d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking Numeric Conversion Discrepancies ---\n",
      "\n",
      "Discrepancies for 'legacy_reference_id' (unified NULL, original raw was valid):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No discrepancies found.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discrepancies for 'easting_osgb' from new_xml (unified NULL, original raw was valid):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No discrepancies found.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discrepancies for 'easting_osgb' from old_xml (unified NULL, original raw was valid):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No discrepancies found.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Skipping northing_osgb check for brevity, pattern is similar to easting_osgb)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Checking Numeric Conversion Discrepancies ---\")\n",
    "# Numeric: legacy_reference_id, easting_osgb, northing_osgb\n",
    "# Check legacy_reference_id (from new format OLD_REFERENCE_NUMBER)\n",
    "query_legacy_ref_fail = f\"\"\"\n",
    "SELECT u.event_id, u.source_filename, r.\"OLD_REFERENCE_NUMBER\" as original_raw_value, u.legacy_reference_id\n",
    "FROM \"{UNIFIED_TABLE_NAME}\" u\n",
    "JOIN \"{RAW_NEW_TABLE_NAME}\" r ON u.event_id = r.\"NEW_EVENT_NUMBER\" AND u.source_filename = r.source_filename\n",
    "WHERE u.data_source_format = 'new_xml'\n",
    "    AND u.legacy_reference_id IS NULL\n",
    "    AND r.\"OLD_REFERENCE_NUMBER\" IS NOT NULL\n",
    "    AND lower(trim(r.\"OLD_REFERENCE_NUMBER\")) NOT IN {PLACEHOLDERS_SQL_LIST_STR}\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\nDiscrepancies for 'legacy_reference_id' (unified NULL, original raw was valid):\")\n",
    "df_legacy_fail = run_query_df(con, query_legacy_ref_fail)\n",
    "if df_legacy_fail is not None: display(df_legacy_fail if not df_legacy_fail.is_empty() else \"No discrepancies found.\")\n",
    "\n",
    "# Check easting_osgb\n",
    "query_easting_fail_new = f\"\"\"\n",
    "SELECT u.event_id, u.source_filename, r.\"CENTRE_EASTING\" as original_raw_value, u.easting_osgb\n",
    "FROM \"{UNIFIED_TABLE_NAME}\" u\n",
    "JOIN \"{RAW_NEW_TABLE_NAME}\" r ON u.event_id = r.\"NEW_EVENT_NUMBER\" AND u.source_filename = r.source_filename\n",
    "WHERE u.data_source_format = 'new_xml'\n",
    "    AND u.easting_osgb IS NULL\n",
    "    AND r.\"CENTRE_EASTING\" IS NOT NULL\n",
    "    AND lower(trim(r.\"CENTRE_EASTING\")) NOT IN {PLACEHOLDERS_SQL_LIST_STR}\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\nDiscrepancies for 'easting_osgb' from new_xml (unified NULL, original raw was valid):\")\n",
    "df_easting_fail_new = run_query_df(con, query_easting_fail_new)\n",
    "if df_easting_fail_new is not None: display(df_easting_fail_new if not df_easting_fail_new.is_empty() else \"No discrepancies found.\")\n",
    "\n",
    "query_easting_fail_old = f\"\"\"\n",
    "SELECT u.event_id, u.source_filename, r.\"centre_easting\" as original_raw_value, u.easting_osgb\n",
    "FROM \"{UNIFIED_TABLE_NAME}\" u\n",
    "JOIN \"{RAW_OLD_TABLE_NAME}\" r ON u.event_id = r.\"reference_number\" AND u.source_filename = r.source_filename\n",
    "WHERE u.data_source_format = 'old_xml'\n",
    "    AND u.easting_osgb IS NULL\n",
    "    AND r.\"centre_easting\" IS NOT NULL\n",
    "    AND lower(trim(r.\"centre_easting\")) NOT IN {PLACEHOLDERS_SQL_LIST_STR}\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\nDiscrepancies for 'easting_osgb' from old_xml (unified NULL, original raw was valid):\")\n",
    "df_easting_fail_old = run_query_df(con, query_easting_fail_old)\n",
    "if df_easting_fail_old is not None: display(df_easting_fail_old if not df_easting_fail_old.is_empty() else \"No discrepancies found.\")\n",
    "\n",
    "# Similar checks for northing_osgb can be added here following the pattern for easting_osgb\n",
    "print(\"\\n(Skipping northing_osgb check for brevity, pattern is similar to easting_osgb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a81595e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking Datetime Conversion Discrepancies ---\n",
      "\n",
      "Discrepancies for 'start_datetime' from new_xml (unified NULL, original raw was valid):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No discrepancies found.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discrepancies for 'start_datetime' from old_xml (unified NULL, original raw was valid):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No discrepancies found.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Skipping end_datetime and published_datetime checks for brevity, pattern is similar)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Checking Datetime Conversion Discrepancies ---\")\n",
    "# Datetime: start_datetime, end_datetime, published_datetime\n",
    "# Check start_datetime\n",
    "query_start_dt_fail_new = f\"\"\"\n",
    "SELECT u.event_id, u.source_filename, r.\"SDATE\" as original_raw_value, u.start_datetime\n",
    "FROM \"{UNIFIED_TABLE_NAME}\" u\n",
    "JOIN \"{RAW_NEW_TABLE_NAME}\" r ON u.event_id = r.\"NEW_EVENT_NUMBER\" AND u.source_filename = r.source_filename\n",
    "WHERE u.data_source_format = 'new_xml'\n",
    "    AND u.start_datetime IS NULL\n",
    "    AND r.\"SDATE\" IS NOT NULL\n",
    "    AND lower(trim(r.\"SDATE\")) NOT IN {PLACEHOLDERS_SQL_LIST_STR}\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\nDiscrepancies for 'start_datetime' from new_xml (unified NULL, original raw was valid):\")\n",
    "df_start_dt_fail_new = run_query_df(con, query_start_dt_fail_new)\n",
    "if df_start_dt_fail_new is not None: display(df_start_dt_fail_new if not df_start_dt_fail_new.is_empty() else \"No discrepancies found.\")\n",
    "\n",
    "query_start_dt_fail_old = f\"\"\"\n",
    "SELECT u.event_id, u.source_filename, r.\"start_date\" as original_raw_value, u.start_datetime\n",
    "FROM \"{UNIFIED_TABLE_NAME}\" u\n",
    "JOIN \"{RAW_OLD_TABLE_NAME}\" r ON u.event_id = r.\"reference_number\" AND u.source_filename = r.source_filename\n",
    "WHERE u.data_source_format = 'old_xml'\n",
    "    AND u.start_datetime IS NULL\n",
    "    AND r.\"start_date\" IS NOT NULL\n",
    "    AND lower(trim(r.\"start_date\")) NOT IN {PLACEHOLDERS_SQL_LIST_STR}\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\nDiscrepancies for 'start_datetime' from old_xml (unified NULL, original raw was valid):\")\n",
    "df_start_dt_fail_old = run_query_df(con, query_start_dt_fail_old)\n",
    "if df_start_dt_fail_old is not None: display(df_start_dt_fail_old if not df_start_dt_fail_old.is_empty() else \"No discrepancies found.\")\n",
    "\n",
    "# Similar checks for end_datetime and published_datetime can be added here\n",
    "print(\"\\n(Skipping end_datetime and published_datetime checks for brevity, pattern is similar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af78d6",
   "metadata": {},
   "source": [
    "### F.5. WGS84 Coordinate Conversion Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "881514e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking WGS84 Coordinate Conversion Failures (WGS84 NULL but OSGB Not NULL) ---\n",
      "No records found where WGS84 is NULL but OSGB was validly populated (and non-zero).\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Checking WGS84 Coordinate Conversion Failures (WGS84 NULL but OSGB Not NULL) ---\")\n",
    "query_wgs84_fail = f\"\"\"\n",
    "    SELECT event_id, source_filename, easting_osgb, northing_osgb, longitude_wgs84, latitude_wgs84, description\n",
    "    FROM \"{UNIFIED_TABLE_NAME}\"\n",
    "    WHERE (longitude_wgs84 IS NULL OR latitude_wgs84 IS NULL)\n",
    "        AND easting_osgb IS NOT NULL AND northing_osgb IS NOT NULL \n",
    "        AND easting_osgb != 0 AND northing_osgb != 0 /* Exclude (0,0) OSGB as potentially invalid input */\n",
    "    LIMIT 20;\n",
    "\"\"\"\n",
    "df_wgs84_fail = run_query_df(con, query_wgs84_fail)\n",
    "if df_wgs84_fail is not None:\n",
    "    if not df_wgs84_fail.is_empty():\n",
    "        print(f\"Found {df_wgs84_fail.height} records where WGS84 coordinates are NULL but OSGB coordinates were present and non-zero:\")\n",
    "        display(df_wgs84_fail)\n",
    "    else:\n",
    "        print(\"No records found where WGS84 is NULL but OSGB was validly populated (and non-zero).\")\n",
    "else:\n",
    "    print(\"Could not execute check for WGS84 conversion failures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462b291",
   "metadata": {},
   "source": [
    "## G. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703bf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a057c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if con:\n",
    "    con.close()\n",
    "    print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
