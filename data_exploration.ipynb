{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa3ab55",
   "metadata": {},
   "source": [
    "# Load and explore XML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614cee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "#import pyproj\n",
    "import duckdb\n",
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9faecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "NEW_DATA_DIRECTORY = 'data/new_format_samples'     # data from Sept 2017 onwards\n",
    "OLD_DATA_DIRECTORY = 'data/old_format_samples'     # data from August 2017 and earlier\n",
    "\n",
    "DUCKDB_FILE = 'roadworks_sample_data.duckdb'  # Name for your DuckDB database file\n",
    "# Define separate table names for new and old formats\n",
    "RAW_NEW_TABLE_NAME = 'raw_new_roadworks'\n",
    "RAW_OLD_TABLE_NAME = 'raw_old_roadworks'\n",
    "\n",
    "# Define the namespace map\n",
    "NSMAP = {'d': 'WebTeam'}\n",
    "\n",
    "# XPath to find the repeating record element\n",
    "NEW_ROADWORK_RECORD_XPATH = './/d:HE_PLANNED_WORKS'\n",
    "OLD_ROADWORK_RECORD_XPATH = './/ha_planned_works' # XPath for the old format record\n",
    "\n",
    "# --- Define Raw Columns based on exploration ---\n",
    "\n",
    "# Columns for the 'new' format raw table\n",
    "# Includes source_filename and handles nested elements\n",
    "RAW_NEW_COLUMNS = [\n",
    "    'source_filename',\n",
    "    # Attributes from HE_PLANNED_WORKS\n",
    "    'NEW_EVENT_NUMBER',\n",
    "    'OLD_REFERENCE_NUMBER',\n",
    "    'SDATE',\n",
    "    'EDATE',\n",
    "    'EXPDEL',\n",
    "    'DESCRIPTION',\n",
    "    'CLOSURE_TYPE',\n",
    "    'STATUS',\n",
    "    'PUBLISHED_DATE',\n",
    "    # Nested attributes (will be extracted)\n",
    "    'CENTRE_EASTING',\n",
    "    'CENTRE_NORTHING',\n",
    "    'ROAD_NUMBERS' # Potentially multiple, joined by ';'\n",
    "]\n",
    "\n",
    "# Columns for the 'old' format raw table\n",
    "# Includes source_filename and direct child element tags\n",
    "RAW_OLD_COLUMNS = [\n",
    "    'source_filename',\n",
    "    # Child elements of ha_planned_works\n",
    "    'reference_number',\n",
    "    'start_date',\n",
    "    'end_date',\n",
    "    'expected_delay',\n",
    "    'description',\n",
    "    'closure_type',\n",
    "    'status',\n",
    "    'published_date',\n",
    "    'centre_easting',\n",
    "    'centre_northing',\n",
    "    'road',\n",
    "    'location',\n",
    "    'local_authority',\n",
    "    'traffic_management'\n",
    "]\n",
    "\n",
    "# Define XPaths for nested data relative to the NEW format HE_PLANNED_WORKS element\n",
    "NEW_COORD_XPATH = './d:EASTNORTH/d:Report/d:EASTINGNORTHING/d:EASTNORTH_Collection/d:EASTNORTH'\n",
    "NEW_ROAD_XPATH = './d:ROADS/d:Report/d:ROADS/d:ROAD_Collection/d:ROAD'\n",
    "\n",
    "# Define SQL run-query function\n",
    "def run_query(connection, sql_query):\n",
    "    \"\"\"Helper function to run a query and return a Polars DataFrame.\"\"\"\n",
    "    if not connection:\n",
    "        print(\"Error: Database connection is not established.\")\n",
    "        return None\n",
    "    try:\n",
    "        # print(f\"Running query:\\n{sql_query}\") # Optional: print query being run\n",
    "        return connection.sql(sql_query).pl()\n",
    "    except duckdb.Error as e:\n",
    "        print(f\"Error running query:\\n{sql_query}\\nError: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a148eef",
   "metadata": {},
   "source": [
    "### Find all unique attributes in many XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21064cb3",
   "metadata": {},
   "source": [
    "##### For 'new' format (attributes-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546d879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_record_attributes_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Parses all XML files (new format) in a directory and finds all unique attribute names\n",
    "    used across all elements matching the NEW_ROADWORK_RECORD_XPATH in any file.\n",
    "    \"\"\"\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Error: No XML files found in directory: {directory_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"--- Finding All Unique Attributes in Directory: {directory_path} ---\")\n",
    "    print(f\"Found {len(xml_files)} XML files to scan.\")\n",
    "\n",
    "    all_attribute_names = set() # Use a set to automatically store unique names across all files\n",
    "    parser = etree.XMLParser(recover=True, ns_clean=True) # Define parser once\n",
    "\n",
    "    processed_files = 0\n",
    "    files_with_errors = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        # print(f\"Scanning file: {filename}...\") # Uncomment for more verbose output\n",
    "        try:\n",
    "            # Parse the XML file\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Use xpath with the namespace map to find all records in this file\n",
    "            records = root.xpath(NEW_ROADWORK_RECORD_XPATH, namespaces=NSMAP)\n",
    "\n",
    "            if not records:\n",
    "                # print(f\"  Warning: No records found matching XPath in {filename}.\")\n",
    "                continue # Move to the next file if no records found\n",
    "\n",
    "            # Iterate through ALL found records in the current file\n",
    "            for record in records:\n",
    "                # Get the keys (attribute names) from the current record's attributes\n",
    "                attribute_keys = record.attrib.keys()\n",
    "                all_attribute_names.update(attribute_keys)\n",
    "\n",
    "                # Additionally: find attributes in DESCENDANT elements\n",
    "                # Use iterdescendants() to visit every element below the current record\n",
    "                for descendant in record.iterdescendants():\n",
    "                    all_attribute_names.update(descendant.attrib.keys())\n",
    "\n",
    "            processed_files += 1\n",
    "\n",
    "        except etree.XMLSyntaxError as e:\n",
    "            print(f\"  Error parsing XML file {filename}: {e}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  An unexpected error occurred scanning file {filename}: {e}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "\n",
    "    print(f\"\\n--- Scan Complete ---\")\n",
    "    print(f\"Successfully scanned {processed_files} files.\")\n",
    "    if files_with_errors > 0:\n",
    "        print(f\"Skipped {files_with_errors} files due to errors.\")\n",
    "\n",
    "    if not all_attribute_names:\n",
    "        print(\"No attributes found in any successfully processed files.\")\n",
    "        return None\n",
    "\n",
    "    # Sort the results for readability\n",
    "    sorted_attributes = sorted(list(all_attribute_names))\n",
    "\n",
    "    print(f\"\\nFound {len(sorted_attributes)} unique attributes across all scanned files:\")\n",
    "    return sorted_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfe7734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding All Unique Attributes in Directory: data/new_format_samples ---\n",
      "Found 8 XML files to scan.\n",
      "\n",
      "--- Scan Complete ---\n",
      "Successfully scanned 8 files.\n",
      "\n",
      "Found 13 unique attributes across all scanned files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CENTRE_EASTING',\n",
       " 'CENTRE_NORTHING',\n",
       " 'CLOSURE_TYPE',\n",
       " 'DESCRIPTION',\n",
       " 'EDATE',\n",
       " 'EXPDEL',\n",
       " 'NEW_EVENT_NUMBER',\n",
       " 'Name',\n",
       " 'OLD_REFERENCE_NUMBER',\n",
       " 'PUBLISHED_DATE',\n",
       " 'ROAD_NUMBER',\n",
       " 'SDATE',\n",
       " 'STATUS']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_record_attributes_in_directory(NEW_DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f527b21",
   "metadata": {},
   "source": [
    "##### For 'old' format (child element-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de44832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_record_elements_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Parses all XML files (old format) in a directory and finds all\n",
    "    unique child element tag names used across all elements matching the\n",
    "    OLD_ROADWORK_RECORD_XPATH in any file.\n",
    "    \"\"\"\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Error: No XML files found in directory: {directory_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"--- Finding All Unique Child Element Tags in Directory: {directory_path} ---\")\n",
    "    print(f\"Found {len(xml_files)} XML files to scan.\")\n",
    "\n",
    "    all_element_tags = set() # Use a set to automatically store unique tag names\n",
    "    # Use a simpler parser if namespaces are not expected/needed for old format\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "\n",
    "    processed_files = 0\n",
    "    files_with_errors = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        # print(f\"Scanning file: {filename}...\")\n",
    "        try:\n",
    "            # Parse the XML file\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Check if the root tag matches the expected old format root\n",
    "            if root.tag != 'ha_planned_roadworks':\n",
    "                # print(f\"  Skipping file {filename}: Root tag '{root.tag}' does not match expected 'ha_planned_roadworks'.\")\n",
    "                continue # Skip files that don't match the old root tag\n",
    "\n",
    "            # Use xpath to find all records in this file (no namespace needed)\n",
    "            records = root.xpath(OLD_ROADWORK_RECORD_XPATH) # Use the XPath for the old format\n",
    "\n",
    "            if not records:\n",
    "                # print(f\"  Warning: No records found matching XPath '{OLD_ROADWORK_RECORD_XPATH}' in {filename}.\")\n",
    "                continue # Move to the next file if no records found\n",
    "\n",
    "            # Iterate through ALL found records in the current file\n",
    "            for record in records:\n",
    "                # Iterate through the child elements of the record\n",
    "                for child_element in record:\n",
    "                    # Add the tag name of the child element to the set\n",
    "                    all_element_tags.add(child_element.tag)\n",
    "\n",
    "            processed_files += 1\n",
    "\n",
    "        except etree.XMLSyntaxError as e:\n",
    "            print(f\"  Error parsing XML file {filename}: {e}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  An unexpected error occurred scanning file {filename}: {e}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "\n",
    "    print(f\"\\n--- Scan Complete ---\")\n",
    "    print(f\"Successfully scanned {processed_files} files (matching root tag).\")\n",
    "    if files_with_errors > 0:\n",
    "        print(f\"Skipped {files_with_errors} files due to errors during parsing.\")\n",
    "    skipped_non_matching = len(xml_files) - processed_files - files_with_errors\n",
    "    if skipped_non_matching > 0:\n",
    "         print(f\"Skipped {skipped_non_matching} files because their root tag did not match 'ha_planned_roadworks'.\")\n",
    "\n",
    "\n",
    "    if not all_element_tags:\n",
    "        print(\"No child element tags found in any successfully processed files.\")\n",
    "        return None\n",
    "\n",
    "    # Sort the results for readability\n",
    "    sorted_tags = sorted(list(all_element_tags))\n",
    "\n",
    "    print(f\"\\nFound {len(sorted_tags)} unique child element tags across all scanned files:\")\n",
    "    return sorted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf9d62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding All Unique Child Element Tags in Directory: data/old_format_samples ---\n",
      "Found 7 XML files to scan.\n",
      "\n",
      "--- Scan Complete ---\n",
      "Successfully scanned 7 files (matching root tag).\n",
      "\n",
      "Found 14 unique child element tags across all scanned files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['centre_easting',\n",
       " 'centre_northing',\n",
       " 'closure_type',\n",
       " 'description',\n",
       " 'end_date',\n",
       " 'expected_delay',\n",
       " 'local_authority',\n",
       " 'location',\n",
       " 'published_date',\n",
       " 'reference_number',\n",
       " 'road',\n",
       " 'start_date',\n",
       " 'status',\n",
       " 'traffic_management']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_format_elements = find_all_record_elements_in_directory(OLD_DATA_DIRECTORY)\n",
    "old_format_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9948b",
   "metadata": {},
   "source": [
    "'New' format attributes:\n",
    "```python\n",
    "['CENTRE_EASTING',\n",
    " 'CENTRE_NORTHING',\n",
    " 'CLOSURE_TYPE',\n",
    " 'DESCRIPTION',\n",
    " 'EDATE',\n",
    " 'EXPDEL',\n",
    " 'NEW_EVENT_NUMBER',\n",
    " 'Name',\n",
    " 'OLD_REFERENCE_NUMBER',\n",
    " 'PUBLISHED_DATE',\n",
    " 'ROAD_NUMBER',\n",
    " 'SDATE',\n",
    " 'STATUS']\n",
    "```\n",
    "\n",
    "'Old' format attributes:\n",
    "```python\n",
    "['centre_easting',\n",
    " 'centre_northing',\n",
    " 'closure_type',\n",
    " 'description',\n",
    " 'end_date',\n",
    " 'expected_delay',\n",
    " 'local_authority',\n",
    " 'location',\n",
    " 'published_date',\n",
    " 'reference_number',\n",
    " 'road',\n",
    " 'start_date',\n",
    " 'status',\n",
    " 'traffic_management']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d7157",
   "metadata": {},
   "source": [
    "### Explore some records\n",
    "\n",
    "##### For 'new' format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7bd3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RECORDS_TO_INSPECT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd458644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exploring XML File (Updated): data/new_format/nh_roadworks_2025_14_4.xml ---\n",
      "\n",
      "1. Root Element Tag: {WebTeam}Report\n",
      "   Root Namespace Map: {'xsi': 'http://www.w3.org/2001/XMLSchema-instance', None: 'WebTeam'}\n",
      "\n",
      "2. Found 1429 records matching XPath './/d:HE_PLANNED_WORKS'.\n",
      "--- Inspecting first 3 records ---\n",
      "\n",
      "\n",
      "--- Record 1 ---\n",
      " Attributes of <HE_PLANNED_WORKS>:\n",
      "    NEW_EVENT_NUMBER: 00352573-001\n",
      "    SDATE: 31-DEC-2023 23:59\n",
      "    EDATE: 31-MAY-2025 23:59\n",
      "    EXPDEL: Moderate (10 - 30 mins)\n",
      "    DESCRIPTION: M25 Anticlockwise Jct 11 to Jct 9\n",
      "Narrow Lanes for Major Improvement Scheme \n",
      "    CLOSURE_TYPE: Major Schemes\n",
      "    STATUS: Published\n",
      "    PUBLISHED_DATE: 2023-12-21T14:45:07\n",
      "\n",
      " Extracting Nested Coordinates:\n",
      "    CENTRE_EASTING: 507930\n",
      "    CENTRE_NORTHING: 159334\n",
      "\n",
      "Extracting Nested Roads:\n",
      "    ROAD_NUMBER(s): ['M25']\n",
      "\n",
      "\n",
      "--- Record 2 ---\n",
      " Attributes of <HE_PLANNED_WORKS>:\n",
      "    NEW_EVENT_NUMBER: 00380443-001\n",
      "    SDATE: 29-MAY-2024 21:00\n",
      "    EDATE: 29-MAY-2025 23:59\n",
      "    EXPDEL: Slight (less than 10 mins)\n",
      "    DESCRIPTION: M275 southbound M27 to Tipner \n",
      "Lane closure for Portsmouth City Council.\n",
      "\n",
      "    CLOSURE_TYPE: Emergency and urgent Street/Road Works\n",
      "    STATUS: Published\n",
      "    PUBLISHED_DATE: 2024-05-30T11:39:55\n",
      "\n",
      " Extracting Nested Coordinates:\n",
      "    CENTRE_EASTING: 464494\n",
      "    CENTRE_NORTHING: 104095\n",
      "\n",
      "Extracting Nested Roads:\n",
      "    ROAD_NUMBER(s): ['M27']\n",
      "\n",
      "\n",
      "--- Record 3 ---\n",
      " Attributes of <HE_PLANNED_WORKS>:\n",
      "    NEW_EVENT_NUMBER: 00389408-001\n",
      "    SDATE: 24-JUL-2024 12:00\n",
      "    EDATE: 26-AUG-2027 05:00\n",
      "    EXPDEL: Slight (less than 10 mins)\n",
      "    DESCRIPTION: A38 both directions Streethay (Cappers Lane Jct) to Fradley.\n",
      "24/7 Fradley Park layby closure and narrow lanes with 40mph speed limit.\n",
      "\n",
      "    CLOSURE_TYPE: Developer Works\n",
      "    STATUS: Published\n",
      "    PUBLISHED_DATE: 2024-07-24T11:00:45\n",
      "\n",
      " Extracting Nested Coordinates:\n",
      "    CENTRE_EASTING: 413949\n",
      "    CENTRE_NORTHING: 309566\n",
      "\n",
      "Extracting Nested Roads:\n",
      "    ROAD_NUMBER(s): ['A38']\n",
      "\n",
      "--- End of Exploration for data/new_format/nh_roadworks_2025_14_4.xml ---\n"
     ]
    }
   ],
   "source": [
    "def explore_roadworks_xml_new(file_path):\n",
    "    \"\"\"Parses and explores the specific structure of the provided roadworks XML.\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Exploring XML File (Updated): {file_path} ---\")\n",
    "\n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        # Using recover=True can help skip over minor errors if files are slightly malformed\n",
    "        parser = etree.XMLParser(recover=True, ns_clean=True)\n",
    "        tree = etree.parse(file_path, parser)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        print(f\"\\n1. Root Element Tag: {root.tag}\") # Should be {WebTeam}Report\n",
    "        print(f\"   Root Namespace Map: {root.nsmap}\")\n",
    "\n",
    "        # Use xpath with the namespace map to find the records\n",
    "        records = root.xpath(NEW_ROADWORK_RECORD_XPATH, namespaces=NSMAP)\n",
    "\n",
    "        if not records:\n",
    "            print(f\"\\nError: Could not find any elements matching XPath '{NEW_ROADWORK_RECORD_XPATH}'.\")\n",
    "            print(\"Please double-check the NEW_ROADWORK_RECORD_XPATH and the XML structure.\")\n",
    "            # Print children with their full {namespace}tag names to help debug\n",
    "            print(\"\\nFirst few children of the root (with full tags):\")\n",
    "            for i, child in enumerate(root[:5]):\n",
    "                 print(f\"  Child {i+1}: {child.tag}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n2. Found {len(records)} records matching XPath '{NEW_ROADWORK_RECORD_XPATH}'.\")\n",
    "        print(f\"--- Inspecting first {min(NUM_RECORDS_TO_INSPECT, len(records))} records ---\")\n",
    "\n",
    "        for i, record in enumerate(records[:NUM_RECORDS_TO_INSPECT]):\n",
    "            print(f\"\\n\\n--- Record {i+1} ---\")\n",
    "\n",
    "            # --- Accessing Attributes of <HE_PLANNED_WORKS> ---\n",
    "            print(\" Attributes of <HE_PLANNED_WORKS>:\")\n",
    "            record_attrs = record.attrib\n",
    "            for key, value in record_attrs.items():\n",
    "                 print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "            # --- Accessing Nested Coordinates ---\n",
    "            print(\"\\n Extracting Nested Coordinates:\")\n",
    "            # Define the precise XPath relative to the current 'record' element\n",
    "            coord_xpath = './d:EASTNORTH/d:Report/d:EASTINGNORTHING/d:EASTNORTH_Collection/d:EASTNORTH'\n",
    "            coord_elements = record.xpath(coord_xpath, namespaces=NSMAP)\n",
    "\n",
    "            if coord_elements:\n",
    "                # Usually expect only one coordinate block per record\n",
    "                coord_element = coord_elements[0]\n",
    "                easting = coord_element.get('CENTRE_EASTING')\n",
    "                northing = coord_element.get('CENTRE_NORTHING')\n",
    "                print(f\"    CENTRE_EASTING: {easting}\")\n",
    "                print(f\"    CENTRE_NORTHING: {northing}\")\n",
    "            else:\n",
    "                print(\"    Coordinate elements not found.\")\n",
    "\n",
    "            # --- Accessing Nested Roads ---\n",
    "            print(\"\\nExtracting Nested Roads:\")\n",
    "            # Define the precise XPath relative to the current 'record' element\n",
    "            road_xpath = './d:ROADS/d:Report/d:ROADS/d:ROAD_Collection/d:ROAD'\n",
    "            road_elements = record.xpath(road_xpath, namespaces=NSMAP)\n",
    "\n",
    "            if road_elements:\n",
    "                road_numbers = [road.get('ROAD_NUMBER') for road in road_elements]\n",
    "                print(f\"    ROAD_NUMBER(s): {road_numbers}\") # Might be multiple roads\n",
    "            else:\n",
    "                print(\"    Road elements not found.\")\n",
    "\n",
    "\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        print(f\"\\nError parsing XML file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "\n",
    "    print(f\"\\n--- End of Exploration for {file_path} ---\")\n",
    "\n",
    "\n",
    "explore_roadworks_xml_new(\"data/new_format/nh_roadworks_2025_14_4.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f7aca",
   "metadata": {},
   "source": [
    "##### For 'old' format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6e842fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file 'data/old_format_samples\\he_roadworks_2017_06_05' not found. Using first available file: data/old_format_samples\\ha-roadworks_2011_10_10.xml\n",
      "--- Exploring XML File (Old Format): data/old_format_samples\\ha-roadworks_2011_10_10.xml ---\n",
      "\n",
      "1. Root Element Tag: ha_planned_roadworks\n",
      "\n",
      "2. Found 1425 records matching XPath './/ha_planned_works'.\n",
      "--- Inspecting first 3 records ---\n",
      "\n",
      "\n",
      "--- Record 1 ---\n",
      " Record Element Tag: ha_planned_works\n",
      " Child Elements:\n",
      "    reference_number: 972963\n",
      "    road: M1\n",
      "    local_authority: Leicestershire / Northamptonshire\n",
      "    location: Catthorpe\n",
      "    start_date: 2010-07-12T07:00:00\n",
      "    end_date: 2013-03-23T06:00:00\n",
      "    expected_delay: Moderate (10 - 30 mins)\n",
      "    description: Major junction works will include lane closures, contraflow, full closures and 50 MPH speed restrictions on the M1 and M6.\n",
      "    traffic_management: Other\n",
      "    closure_type: Planned Works\n",
      "    centre_easting: 456252\n",
      "    centre_northing: 278173\n",
      "    status: Firm\n",
      "    published_date: 2011-10-09T21:08:32\n",
      "\n",
      "\n",
      "--- Record 2 ---\n",
      " Record Element Tag: ha_planned_works\n",
      " Child Elements:\n",
      "    reference_number: 978905\n",
      "    road: M1\n",
      "    local_authority: Bedfordshire / Buckinghamshire\n",
      "    location: Jct 13 to Jct 12\n",
      "    start_date: 2011-04-01T22:00:00\n",
      "    end_date: 2011-12-31T05:00:00\n",
      "    expected_delay: Moderate (10 - 30 mins)\n",
      "    description: Contraflow with speed restriction southbound 24 hrs due to improvement works.\n",
      "    traffic_management: Contraflow\n",
      "    closure_type: Planned Works\n",
      "    centre_easting: 499082\n",
      "    centre_northing: 235992\n",
      "    status: Firm\n",
      "    published_date: 2010-04-23T10:18:30\n",
      "\n",
      "\n",
      "--- Record 3 ---\n",
      " Record Element Tag: ha_planned_works\n",
      " Child Elements:\n",
      "    reference_number: 998294\n",
      "    road: M1\n",
      "    local_authority: Northamptonshire\n",
      "    location: Approach to Junction 16 (210113)\n",
      "    start_date: 2009-09-24T06:00:00\n",
      "    end_date: 2013-09-24T05:00:00\n",
      "    expected_delay: Slight (less than 10 mins)\n",
      "    description: Lane 1 closure and 24/7 Hardshoulder closure Southbound 21:00 to 06:00 hrs for surveys.\n",
      "    traffic_management: Lane Closure\n",
      "    closure_type: Planned Works\n",
      "    centre_easting: 465924\n",
      "    centre_northing: 260154\n",
      "    status: Firm\n",
      "    published_date: 2010-06-19T05:03:50\n",
      "\n",
      "--- End of Exploration for data/old_format_samples\\ha-roadworks_2011_10_10.xml ---\n"
     ]
    }
   ],
   "source": [
    "def explore_roadworks_xml_old(file_path):\n",
    "    \"\"\"Parses and explores the structure of an old-format roadworks XML.\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Exploring XML File (Old Format): {file_path} ---\")\n",
    "\n",
    "    try:\n",
    "        # Use a simpler parser, potentially without namespace handling if not needed\n",
    "        parser = etree.XMLParser(recover=True)\n",
    "        tree = etree.parse(file_path, parser)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        print(f\"\\n1. Root Element Tag: {root.tag}\") # Should be ha_planned_roadworks\n",
    "\n",
    "        # Check if the root tag is as expected for the old format\n",
    "        if root.tag != 'ha_planned_roadworks':\n",
    "            print(f\"  Warning: Root tag '{root.tag}' does not match expected 'ha_planned_roadworks'.\")\n",
    "            # Optionally, still try to find records if the XPath might work\n",
    "            # return # Or uncomment to stop if root tag is wrong\n",
    "\n",
    "        # Use xpath to find the records (no namespace typically needed for old format)\n",
    "        records = root.xpath(OLD_ROADWORK_RECORD_XPATH)\n",
    "\n",
    "        if not records:\n",
    "            print(f\"\\nError: Could not find any elements matching XPath '{OLD_ROADWORK_RECORD_XPATH}'.\")\n",
    "            print(\"Please double-check the OLD_ROADWORK_RECORD_XPATH and the XML structure.\")\n",
    "            print(\"\\nFirst few children of the root:\")\n",
    "            for i, child in enumerate(root[:5]):\n",
    "                 print(f\"  Child {i+1}: {child.tag}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n2. Found {len(records)} records matching XPath '{OLD_ROADWORK_RECORD_XPATH}'.\")\n",
    "        print(f\"--- Inspecting first {min(NUM_RECORDS_TO_INSPECT, len(records))} records ---\")\n",
    "\n",
    "        for i, record in enumerate(records[:NUM_RECORDS_TO_INSPECT]):\n",
    "            print(f\"\\n\\n--- Record {i+1} ---\")\n",
    "            print(f\" Record Element Tag: {record.tag}\") # Should be ha_planned_works\n",
    "\n",
    "            # --- Accessing Child Elements ---\n",
    "            print(\" Child Elements:\")\n",
    "            record_data = {}\n",
    "            for child in record:\n",
    "                # Clean up text content (strip whitespace, handle None)\n",
    "                text_content = (child.text or '').strip()\n",
    "                print(f\"    {child.tag}: {text_content}\")\n",
    "                record_data[child.tag] = text_content # Store for easier access later\n",
    "\n",
    "\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        print(f\"\\nError parsing XML file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "\n",
    "    print(f\"\\n--- End of Exploration for {file_path} ---\")\n",
    "\n",
    "\n",
    "example_old_file = os.path.join(OLD_DATA_DIRECTORY, 'he_roadworks_2017_06_05')\n",
    "if os.path.exists(example_old_file):\n",
    "    explore_roadworks_xml_old(example_old_file)\n",
    "else:\n",
    "    # Find the first available XML file in the old data directory if the example doesn't exist\n",
    "    old_files = glob.glob(os.path.join(OLD_DATA_DIRECTORY, '*.xml'))\n",
    "    if old_files:\n",
    "        print(f\"Example file '{example_old_file}' not found. Using first available file: {old_files[0]}\")\n",
    "        explore_roadworks_xml_old(old_files[0])\n",
    "    else:\n",
    "        print(f\"Error: No XML files found in {OLD_DATA_DIRECTORY} to explore.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b732248",
   "metadata": {},
   "source": [
    "### Define XML-record extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91a577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record_new_format(record_element, source_filename):\n",
    "    \"\"\"\n",
    "    Extracts raw data from a 'new' format <HE_PLANNED_WORKS> element\n",
    "    into a dictionary matching RAW_NEW_COLUMNS.\n",
    "    \"\"\"\n",
    "    data = {col: None for col in RAW_NEW_COLUMNS} # Initialize with None\n",
    "    data['source_filename'] = source_filename\n",
    "\n",
    "    # --- Extract direct attributes ---\n",
    "    data['NEW_EVENT_NUMBER'] = record_element.get('NEW_EVENT_NUMBER')\n",
    "    data['OLD_REFERENCE_NUMBER'] = record_element.get('OLD_REFERENCE_NUMBER')\n",
    "    data['SDATE'] = record_element.get('SDATE')\n",
    "    data['EDATE'] = record_element.get('EDATE')\n",
    "    data['EXPDEL'] = record_element.get('EXPDEL')\n",
    "    data['DESCRIPTION'] = record_element.get('DESCRIPTION')\n",
    "    data['CLOSURE_TYPE'] = record_element.get('CLOSURE_TYPE')\n",
    "    data['STATUS'] = record_element.get('STATUS')\n",
    "    data['PUBLISHED_DATE'] = record_element.get('PUBLISHED_DATE')\n",
    "\n",
    "    # Basic check - skip if no event number (essential identifier)\n",
    "    if data.get('NEW_EVENT_NUMBER') is None:\n",
    "        # print(f\"Warning: New format record missing NEW_EVENT_NUMBER in {source_filename}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # --- Extract nested coordinates ---\n",
    "    coord_elements = record_element.xpath(NEW_COORD_XPATH, namespaces=NSMAP)\n",
    "    if coord_elements:\n",
    "        coord_element = coord_elements[0]\n",
    "        data['CENTRE_EASTING'] = coord_element.get('CENTRE_EASTING')\n",
    "        data['CENTRE_NORTHING'] = coord_element.get('CENTRE_NORTHING')\n",
    "\n",
    "    # --- Extract nested roads ---\n",
    "    road_elements = record_element.xpath(NEW_ROAD_XPATH, namespaces=NSMAP)\n",
    "    if road_elements:\n",
    "        road_numbers_list = [road.get('ROAD_NUMBER') for road in road_elements if road.get('ROAD_NUMBER')]\n",
    "        # Join multiple roads with a separator\n",
    "        data['ROAD_NUMBERS'] = '; '.join(road_numbers_list) if road_numbers_list else None\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_record_old_format(record_element, source_filename):\n",
    "    \"\"\"\n",
    "    Extracts raw data from an 'old' format <ha_planned_works> element\n",
    "    into a dictionary matching RAW_OLD_COLUMNS.\n",
    "    \"\"\"\n",
    "    data = {col: None for col in RAW_OLD_COLUMNS} # Initialize with None\n",
    "    data['source_filename'] = source_filename\n",
    "\n",
    "    # Helper to get text content safely\n",
    "    def get_text(tag_name):\n",
    "        element = record_element.find(tag_name)\n",
    "        return element.text.strip() if element is not None and element.text else None\n",
    "\n",
    "    # --- Map child elements to raw columns ---\n",
    "    # Iterate through expected raw old columns (excluding source_filename)\n",
    "    for col_name in RAW_OLD_COLUMNS:\n",
    "        if col_name != 'source_filename':\n",
    "             data[col_name] = get_text(col_name)\n",
    "\n",
    "    # Basic check - skip if no reference number (essential identifier)\n",
    "    if data.get('reference_number') is None:\n",
    "        # print(f\"Warning: Old format record missing reference_number in {source_filename}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd130cd",
   "metadata": {},
   "source": [
    "### Generic directory processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "940c7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD METHOD: Returns a list (memory inefficient for large datasets)\n",
    "def process_directory(directory_path, record_xpath, extraction_func, nsmap=None):\n",
    "    \"\"\"\n",
    "    Processes all XML files in a directory using a specific XPath and extraction function.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing XML files.\n",
    "        record_xpath (str): XPath expression to find record elements.\n",
    "        extraction_func (callable): Function to call for each record element found.\n",
    "                                    It should accept (record_element, source_filename)\n",
    "                                    and return a dictionary or None.\n",
    "        nsmap (dict, optional): Namespace map for XPath evaluation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a processed record.\n",
    "    \"\"\"\n",
    "    all_records_data_dicts = []\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "    parser = etree.XMLParser(recover=True, ns_clean=True)\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Warning: No XML files found in directory: {directory_path}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n--- Processing Directory: {directory_path} ---\")\n",
    "    print(f\"Found {len(xml_files)} XML files.\")\n",
    "\n",
    "    total_processed_records = 0\n",
    "    total_skipped_records = 0\n",
    "    files_with_errors = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        # print(f\"Processing file: {filename}...\") # Optional verbose output\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            # Find records using the provided XPath and namespace map\n",
    "            records = root.xpath(record_xpath, namespaces=nsmap)\n",
    "\n",
    "            if not records:\n",
    "                # print(f\"  Warning: No records found matching XPath in {filename}.\")\n",
    "                continue\n",
    "\n",
    "            file_record_count = 0\n",
    "            file_skipped_count = 0\n",
    "            for record in records:\n",
    "                try:\n",
    "                    extracted_dict = extraction_func(record, filename)\n",
    "                    if extracted_dict:\n",
    "                        all_records_data_dicts.append(extracted_dict)\n",
    "                        file_record_count += 1\n",
    "                    else:\n",
    "                        file_skipped_count += 1 # Count records skipped by extraction func\n",
    "                except Exception as e_rec:\n",
    "                    # Try to get an ID for logging, adapt based on potential extraction func errors\n",
    "                    event_id = \"UNKNOWN_ID\"\n",
    "                    try:\n",
    "                        if nsmap: # Likely new format\n",
    "                             event_id = record.get('NEW_EVENT_NUMBER', event_id)\n",
    "                        else: # Likely old format\n",
    "                             ref_num_el = record.find('reference_number')\n",
    "                             if ref_num_el is not None and ref_num_el.text:\n",
    "                                 event_id = ref_num_el.text.strip()\n",
    "                    except: pass # Ignore errors getting ID for logging\n",
    "                    print(f\"  Error processing record {event_id} in {filename}: {e_rec}\")\n",
    "                    file_skipped_count += 1\n",
    "\n",
    "            # if file_record_count > 0 or file_skipped_count > 0: # Only print if something happened\n",
    "            #    print(f\"  Extracted {file_record_count} valid records from {filename}. Skipped {file_skipped_count}.\")\n",
    "\n",
    "            total_processed_records += file_record_count\n",
    "            total_skipped_records += file_skipped_count\n",
    "\n",
    "        except etree.XMLSyntaxError as e_xml:\n",
    "            print(f\"  Error parsing XML file {filename}: {e_xml}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "        except Exception as e_file:\n",
    "            print(f\"  An unexpected error occurred processing file {filename}: {e_file}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "\n",
    "    print(f\"--- Directory Scan Complete: {directory_path} ---\")\n",
    "    print(f\"Successfully extracted {total_processed_records} records.\")\n",
    "    if total_skipped_records > 0:\n",
    "        print(f\"Skipped {total_skipped_records} records (missing ID or processing error).\")\n",
    "    if files_with_errors > 0:\n",
    "        print(f\"Skipped {files_with_errors} files due to parsing/file errors.\")\n",
    "\n",
    "    return all_records_data_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc95985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  UPDATED GENERATOR FUNCTION: Yields records one by one instead of returning a list\n",
    "def process_directory(directory_path, record_xpath, extraction_func, nsmap=None):\n",
    "    \"\"\"\n",
    "    Processes all XML files in a directory using a specific XPath and extraction function,\n",
    "    yielding each processed record as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing XML files.\n",
    "        record_xpath (str): XPath expression to find record elements.\n",
    "        extraction_func (callable): Function to call for each record element found.\n",
    "                                    It should accept (record_element, source_filename)\n",
    "                                    and return a dictionary or None.\n",
    "        nsmap (dict, optional): Namespace map for XPath evaluation. Defaults to None.\n",
    "\n",
    "    Yields:\n",
    "        dict: A dictionary representing a processed record, if valid.\n",
    "    \"\"\"\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "    parser = etree.XMLParser(recover=True, ns_clean=True) # Use robust parser\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Warning: No XML files found in directory: {directory_path}\")\n",
    "        return # Return early if no files\n",
    "\n",
    "    print(f\"\\n--- Processing Directory: {directory_path} ---\")\n",
    "    print(f\"Found {len(xml_files)} XML files.\")\n",
    "\n",
    "    total_yielded_records = 0\n",
    "    total_skipped_records = 0\n",
    "    files_with_errors = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            records = root.xpath(record_xpath, namespaces=nsmap)\n",
    "\n",
    "            if not records:\n",
    "                continue\n",
    "\n",
    "            file_yielded_count = 0\n",
    "            file_skipped_count = 0\n",
    "            for record in records:\n",
    "                try:\n",
    "                    extracted_dict = extraction_func(record, filename)\n",
    "                    if extracted_dict:\n",
    "                        yield extracted_dict\n",
    "                        file_yielded_count += 1\n",
    "                    else:\n",
    "                        file_skipped_count += 1\n",
    "                except Exception as e_rec:\n",
    "                    event_id = \"UNKNOWN_ID\"\n",
    "                    try: # Attempt to get ID for logging\n",
    "                        if nsmap: event_id = record.get('NEW_EVENT_NUMBER', event_id)\n",
    "                        else:\n",
    "                             ref_num_el = record.find('reference_number')\n",
    "                             if ref_num_el is not None and ref_num_el.text: event_id = ref_num_el.text.strip()\n",
    "                    except: pass\n",
    "                    print(f\"  Error processing record {event_id} in {filename}: {e_rec}\")\n",
    "                    file_skipped_count += 1\n",
    "\n",
    "            total_yielded_records += file_yielded_count\n",
    "            total_skipped_records += file_skipped_count\n",
    "\n",
    "        except etree.XMLSyntaxError as e_xml:\n",
    "            print(f\"  Error parsing XML file {filename}: {e_xml}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "        except Exception as e_file:\n",
    "            print(f\"  An unexpected error occurred processing file {filename}: {e_file}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "\n",
    "    print(f\"--- Directory Scan Complete: {directory_path} ---\")\n",
    "    print(f\"Successfully yielded {total_yielded_records} records.\") \n",
    "    if total_skipped_records > 0:\n",
    "        print(f\"Skipped {total_skipped_records} records (missing ID or processing error).\")\n",
    "    if files_with_errors > 0:\n",
    "        print(f\"Skipped {files_with_errors} files due to parsing/file errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054af20",
   "metadata": {},
   "source": [
    "### Process data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00af52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_batches(con, table_name, target_columns, data_iterator, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Loads data from an iterator into a DuckDB table in batches.\n",
    "\n",
    "    Args:\n",
    "        con: Active DuckDB connection object.\n",
    "        table_name (str): Name of the target table.\n",
    "        target_columns (list): List of column names in the target table order.\n",
    "        data_iterator (iterator): An iterator yielding dictionaries of data.\n",
    "        batch_size (int): Number of records to insert per batch.\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    total_inserted = 0\n",
    "    num_columns = len(target_columns)\n",
    "    placeholders = ', '.join(['?'] * num_columns)\n",
    "    insert_sql = f'INSERT INTO \"{table_name}\" VALUES ({placeholders})'\n",
    "\n",
    "    print(f\"Starting batch insertion into '{table_name}' (batch size: {batch_size})...\")\n",
    "\n",
    "    for record_dict in data_iterator:\n",
    "        # Convert dict to list/tuple in the correct column order\n",
    "        row_values = [record_dict.get(col_name) for col_name in target_columns]\n",
    "        batch_data.append(row_values)\n",
    "\n",
    "        if len(batch_data) >= batch_size:\n",
    "            try:\n",
    "                con.executemany(insert_sql, batch_data)\n",
    "                total_inserted += len(batch_data)\n",
    "                print(f\"  Inserted batch of {len(batch_data)}. Total inserted: {total_inserted}\")\n",
    "                batch_data = [] # Clear the batch\n",
    "            except duckdb.Error as e:\n",
    "                print(f\"  Error inserting batch: {e}\")\n",
    "                # Decide how to handle batch errors (e.g., log, skip, stop)\n",
    "                # For now, just print and continue trying next batch\n",
    "                batch_data = [] # Clear potentially problematic batch\n",
    "\n",
    "    # Insert any remaining records in the last batch\n",
    "    if batch_data:\n",
    "        try:\n",
    "            con.executemany(insert_sql, batch_data)\n",
    "            total_inserted += len(batch_data)\n",
    "            print(f\"  Inserted final batch of {len(batch_data)}. Total inserted: {total_inserted}\")\n",
    "        except duckdb.Error as e:\n",
    "            print(f\"  Error inserting final batch: {e}\")\n",
    "\n",
    "    print(f\"Batch insertion complete. Total records inserted: {total_inserted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5efc5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to DuckDB database: roadworks_sample_data.duckdb\n",
      "Creating or replacing table: raw_new_roadworks\n",
      "Table 'raw_new_roadworks' created/replaced successfully.\n",
      "\n",
      "Processing NEW format data...\n",
      "Starting batch insertion into 'raw_new_roadworks' (batch size: 1000)...\n",
      "\n",
      "--- Processing Directory: data/new_format_samples ---\n",
      "Found 8 XML files.\n",
      "  Inserted batch of 1000. Total inserted: 1000\n",
      "  Inserted batch of 1000. Total inserted: 2000\n",
      "  Inserted batch of 1000. Total inserted: 3000\n",
      "  Inserted batch of 1000. Total inserted: 4000\n",
      "  Inserted batch of 1000. Total inserted: 5000\n",
      "  Inserted batch of 1000. Total inserted: 6000\n",
      "  Inserted batch of 1000. Total inserted: 7000\n",
      "  Inserted batch of 1000. Total inserted: 8000\n",
      "  Inserted batch of 1000. Total inserted: 9000\n",
      "  Inserted batch of 1000. Total inserted: 10000\n",
      "  Inserted batch of 1000. Total inserted: 11000\n",
      "--- Directory Scan Complete: data/new_format_samples ---\n",
      "Successfully yielded 11353 records.\n",
      "  Inserted final batch of 353. Total inserted: 11353\n",
      "Batch insertion complete. Total records inserted: 11353\n",
      "\n",
      "Creating or replacing table: raw_old_roadworks\n",
      "Table 'raw_old_roadworks' created/replaced successfully.\n",
      "\n",
      "Processing OLD format data...\n",
      "Starting batch insertion into 'raw_old_roadworks' (batch size: 1000)...\n",
      "\n",
      "--- Processing Directory: data/old_format_samples ---\n",
      "Found 7 XML files.\n",
      "  Inserted batch of 1000. Total inserted: 1000\n",
      "  Inserted batch of 1000. Total inserted: 2000\n",
      "  Inserted batch of 1000. Total inserted: 3000\n",
      "  Inserted batch of 1000. Total inserted: 4000\n",
      "  Inserted batch of 1000. Total inserted: 5000\n",
      "  Inserted batch of 1000. Total inserted: 6000\n",
      "  Inserted batch of 1000. Total inserted: 7000\n",
      "  Inserted batch of 1000. Total inserted: 8000\n",
      "  Inserted batch of 1000. Total inserted: 9000\n",
      "  Inserted batch of 1000. Total inserted: 10000\n",
      "  Inserted batch of 1000. Total inserted: 11000\n",
      "  Inserted batch of 1000. Total inserted: 12000\n",
      "--- Directory Scan Complete: data/old_format_samples ---\n",
      "Successfully yielded 12068 records.\n",
      "  Inserted final batch of 68. Total inserted: 12068\n",
      "Batch insertion complete. Total records inserted: 12068\n",
      "\n",
      "Committing transaction...\n",
      "Transaction committed.\n",
      "\n",
      "Verification: Table 'raw_new_roadworks' now contains 11353 rows.\n",
      "Verification: Table 'raw_old_roadworks' now contains 12068 rows.\n",
      "Database connection closed.\n",
      "\n",
      "--- Raw Data Pipeline Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Main Data Processing and Loading (Batch Mode) ---\n",
    "\n",
    "print(f\"Connecting to DuckDB database: {DUCKDB_FILE}\")\n",
    "\n",
    "con = None # Initialize connection variable\n",
    "try:\n",
    "    con = duckdb.connect(database=DUCKDB_FILE, read_only=False)\n",
    "\n",
    "    # --- Create/Replace RAW NEW Table Structure ---\n",
    "    print(f\"Creating or replacing table: {RAW_NEW_TABLE_NAME}\")\n",
    "    # Quote column names\n",
    "    new_column_defs = [f'\"{col}\" VARCHAR' for col in RAW_NEW_COLUMNS]\n",
    "    create_new_table_sql = f'CREATE OR REPLACE TABLE \"{RAW_NEW_TABLE_NAME}\" ({\", \".join(new_column_defs)})'\n",
    "    con.execute(create_new_table_sql)\n",
    "    print(f\"Table '{RAW_NEW_TABLE_NAME}' created/replaced successfully.\")\n",
    "\n",
    "    # --- Process and Load New Format Raw Data ---\n",
    "    print(\"\\nProcessing NEW format data...\")\n",
    "    new_data_iterator = process_directory(\n",
    "        directory_path=NEW_DATA_DIRECTORY,\n",
    "        record_xpath=NEW_ROADWORK_RECORD_XPATH,\n",
    "        extraction_func=extract_record_new_format,\n",
    "        nsmap=NSMAP\n",
    "    )\n",
    "    # Load into the raw new table using the specific columns\n",
    "    load_data_in_batches(con, RAW_NEW_TABLE_NAME, RAW_NEW_COLUMNS, new_data_iterator)\n",
    "\n",
    "    # --- Create/Replace RAW OLD Table Structure ---\n",
    "    print(f\"\\nCreating or replacing table: {RAW_OLD_TABLE_NAME}\")\n",
    "    # Quote column names\n",
    "    old_column_defs = [f'\"{col}\" VARCHAR' for col in RAW_OLD_COLUMNS]\n",
    "    create_old_table_sql = f'CREATE OR REPLACE TABLE \"{RAW_OLD_TABLE_NAME}\" ({\", \".join(old_column_defs)})'\n",
    "    con.execute(create_old_table_sql)\n",
    "    print(f\"Table '{RAW_OLD_TABLE_NAME}' created/replaced successfully.\")\n",
    "\n",
    "    # --- Process and Load Old Format Raw Data ---\n",
    "    print(\"\\nProcessing OLD format data...\")\n",
    "    old_data_iterator = process_directory(\n",
    "        directory_path=OLD_DATA_DIRECTORY,\n",
    "        record_xpath=OLD_ROADWORK_RECORD_XPATH,\n",
    "        extraction_func=extract_record_old_format,\n",
    "        nsmap=None # No namespace needed for old format XPath\n",
    "    )\n",
    "    # Load into the raw old table using the specific columns\n",
    "    load_data_in_batches(con, RAW_OLD_TABLE_NAME, RAW_OLD_COLUMNS, old_data_iterator)\n",
    "\n",
    "    # --- Finalize ---\n",
    "    print(\"\\nCommitting transaction...\")\n",
    "    con.commit()\n",
    "    print(\"Transaction committed.\")\n",
    "\n",
    "    # Verify final counts\n",
    "    count_new = con.execute(f'SELECT COUNT(*) FROM \"{RAW_NEW_TABLE_NAME}\"').fetchone()\n",
    "    count_old = con.execute(f'SELECT COUNT(*) FROM \"{RAW_OLD_TABLE_NAME}\"').fetchone()\n",
    "    print(f\"\\nVerification: Table '{RAW_NEW_TABLE_NAME}' now contains {count_new[0]} rows.\")\n",
    "    print(f\"Verification: Table '{RAW_OLD_TABLE_NAME}' now contains {count_old[0]} rows.\")\n",
    "\n",
    "\n",
    "except duckdb.Error as e_db:\n",
    "    print(f\"\\nDatabase error occurred: {e_db}\")\n",
    "    if con:\n",
    "        try:\n",
    "            print(\"Attempting to rollback transaction.\")\n",
    "            con.rollback()\n",
    "        except duckdb.Error as e_tx: # More specific exception type if available\n",
    "            print(f\"Rollback failed: {e_tx}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "    if con:\n",
    "        try:\n",
    "            con.rollback()\n",
    "        except duckdb.Error as e_tx:\n",
    "            print(f\"Rollback failed: {e_tx}\")\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "        print(\"Database connection closed.\")\n",
    "\n",
    "print(\"\\n--- Raw Data Pipeline Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf80de",
   "metadata": {},
   "source": [
    "## Analyze data quality\n",
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2898f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to roadworks_sample_data.duckdb for quality checks...\n",
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# --- Quality Checks Setup ---\n",
    "\n",
    "con = None\n",
    "\n",
    "# Establish connection (read-only)\n",
    "try:\n",
    "    print(f\"Connecting to {DUCKDB_FILE} for quality checks...\")\n",
    "    con = duckdb.connect(database=DUCKDB_FILE, read_only=True)\n",
    "    print(\"Connection successful.\")\n",
    "except duckdb.Error as e:\n",
    "    print(f\"Error connecting to database: {e}\")\n",
    "    con = None # Ensure con_check is None if connection failed\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during connection: {e}\")\n",
    "    con = None\n",
    "\n",
    "# Define common placeholders to check\n",
    "PLACEHOLDERS = [\"''\", \"'none'\", \"'n/a'\", \"'null'\", \"'unknown'\"]\n",
    "#PLACEHOLDER_FILTER = \" OR \".join([f'lower(\"{col}\") = {p}' for p in PLACEHOLDERS])\n",
    "\n",
    "# Define tables and columns to iterate over\n",
    "TABLES_INFO = {\n",
    "    RAW_NEW_TABLE_NAME: RAW_NEW_COLUMNS,\n",
    "    RAW_OLD_TABLE_NAME: RAW_OLD_COLUMNS\n",
    "}\n",
    "\n",
    "pl.Config.set_tbl_rows(50)\n",
    "\n",
    "new_table = RAW_NEW_TABLE_NAME\n",
    "old_table = RAW_OLD_TABLE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a45e42",
   "metadata": {},
   "source": [
    "### Describe & view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32d6ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting DuckDB Database: roadworks_sample_data.duckdb ---\n",
      "--- Inspecting Table: raw_new_roadworks ---\n",
      "`DESCRIBE \"raw_new_roadworks\"` returned:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (13, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_name</th><th>column_type</th><th>null</th><th>key</th><th>default</th><th>extra</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;source_filename&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;NEW_EVENT_NUMBER&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;OLD_REFERENCE_NUMBER&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;SDATE&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;EDATE&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;EXPDEL&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;DESCRIPTION&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;CLOSURE_TYPE&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;STATUS&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;PUBLISHED_DATE&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;CENTRE_EASTING&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;CENTRE_NORTHING&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;ROAD_NUMBERS&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (13, 6)\n",
       "\n",
       " column_name           column_type  null  key   default  extra \n",
       " ---                   ---          ---   ---   ---      ---   \n",
       " str                   str          str   str   str      str   \n",
       "\n",
       " source_filename       VARCHAR      YES   null  null     null  \n",
       " NEW_EVENT_NUMBER      VARCHAR      YES   null  null     null  \n",
       " OLD_REFERENCE_NUMBER  VARCHAR      YES   null  null     null  \n",
       " SDATE                 VARCHAR      YES   null  null     null  \n",
       " EDATE                 VARCHAR      YES   null  null     null  \n",
       " EXPDEL                VARCHAR      YES   null  null     null  \n",
       " DESCRIPTION           VARCHAR      YES   null  null     null  \n",
       " CLOSURE_TYPE          VARCHAR      YES   null  null     null  \n",
       " STATUS                VARCHAR      YES   null  null     null  \n",
       " PUBLISHED_DATE        VARCHAR      YES   null  null     null  \n",
       " CENTRE_EASTING        VARCHAR      YES   null  null     null  \n",
       " CENTRE_NORTHING       VARCHAR      YES   null  null     null  \n",
       " ROAD_NUMBERS          VARCHAR      YES   null  null     null  \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in 'raw_new_roadworks': 11353\n",
      "\n",
      "First 5 rows from 'raw_new_roadworks':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>NEW_EVENT_NUMBER</th><th>OLD_REFERENCE_NUMBER</th><th>SDATE</th><th>EDATE</th><th>EXPDEL</th><th>DESCRIPTION</th><th>CLOSURE_TYPE</th><th>STATUS</th><th>PUBLISHED_DATE</th><th>CENTRE_EASTING</th><th>CENTRE_NORTHING</th><th>ROAD_NUMBERS</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00026976-005&quot;</td><td>null</td><td>&quot;26-FEB-2018 21:00&quot;</td><td>&quot;28-FEB-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A3 northbound Sheet Link entry</td><td>&quot;Area Renewals&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-22T16:49:17&quot;</td><td>&quot;475209&quot;</td><td>&quot;124975&quot;</td><td>&quot;A3&quot;</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00004020-008&quot;</td><td>&quot;4188720&quot;</td><td>&quot;08-JAN-2018 20:00&quot;</td><td>&quot;10-MAR-2018 06:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;A14 Westbound\n",
       "Jct 58 to Jct 57</td><td>&quot;Area Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-22T10:13:27&quot;</td><td>&quot;614569&quot;</td><td>&quot;241115&quot;</td><td>&quot;A14&quot;</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00001459-026&quot;</td><td>&quot;4215713&quot;</td><td>&quot;31-JUL-2017 14:47&quot;</td><td>&quot;01-APR-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;M1 northbound and southbound T</td><td>&quot;Major Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-15T14:38:05&quot;</td><td>&quot;445124&quot;</td><td>&quot;364308&quot;</td><td>&quot;M1&quot;</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00027883-003&quot;</td><td>null</td><td>&quot;12-FEB-2018 20:00&quot;</td><td>&quot;17-MAR-2018 06:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;A259, east and westbound betwe</td><td>&quot;Area Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-21T10:36:47&quot;</td><td>&quot;596442&quot;</td><td>&quot;123787&quot;</td><td>&quot;A259&quot;</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00026799-002&quot;</td><td>null</td><td>&quot;10-FEB-2018 22:00&quot;</td><td>&quot;22-MAR-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A3 northbound Compton to Denni</td><td>&quot;Regional Technology Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-22T14:08:43&quot;</td><td>&quot;498261&quot;</td><td>&quot;150727&quot;</td><td>&quot;A3&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "\n",
       " source_fi  NEW_EVENT  OLD_REFER  SDATE        PUBLISHED  CENTRE_EA  CENTRE_NO  ROAD_NUM \n",
       " lename     _NUMBER    ENCE_NUMB  ---           _DATE      STING      RTHING     BERS     \n",
       " ---        ---        ER         str           ---        ---        ---        ---      \n",
       " str        str        ---                      str        str        str        str      \n",
       "                       str                                                                \n",
       "\n",
       " he_roadwo  00026976-  null       26-FEB-20    2018-02-2  475209     124975     A3       \n",
       " rks_2018_  005                   18 21:00      2T16:49:1                                 \n",
       " 02_26.xml                                      7                                         \n",
       " he_roadwo  00004020-  4188720    08-JAN-20    2018-02-2  614569     241115     A14      \n",
       " rks_2018_  008                   18 20:00      2T10:13:2                                 \n",
       " 02_26.xml                                      7                                         \n",
       " he_roadwo  00001459-  4215713    31-JUL-20    2018-02-1  445124     364308     M1       \n",
       " rks_2018_  026                   17 14:47      5T14:38:0                                 \n",
       " 02_26.xml                                      5                                         \n",
       " he_roadwo  00027883-  null       12-FEB-20    2018-02-2  596442     123787     A259     \n",
       " rks_2018_  003                   18 20:00      1T10:36:4                                 \n",
       " 02_26.xml                                      7                                         \n",
       " he_roadwo  00026799-  null       10-FEB-20    2018-02-2  498261     150727     A3       \n",
       " rks_2018_  002                   18 22:00      2T14:08:4                                 \n",
       " 02_26.xml                                      3                                         \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inspecting Table: raw_old_roadworks ---\n",
      "\n",
      "Schema for table 'raw_old_roadworks':\n",
      "`DESCRIBE \"raw_old_roadworks\"` returned:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (15, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_name</th><th>column_type</th><th>null</th><th>key</th><th>default</th><th>extra</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;source_filename&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;reference_number&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;start_date&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;end_date&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;expected_delay&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;description&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;closure_type&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;status&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;published_date&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;centre_easting&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;centre_northing&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;road&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;location&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;local_authority&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;traffic_management&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (15, 6)\n",
       "\n",
       " column_name         column_type  null  key   default  extra \n",
       " ---                 ---          ---   ---   ---      ---   \n",
       " str                 str          str   str   str      str   \n",
       "\n",
       " source_filename     VARCHAR      YES   null  null     null  \n",
       " reference_number    VARCHAR      YES   null  null     null  \n",
       " start_date          VARCHAR      YES   null  null     null  \n",
       " end_date            VARCHAR      YES   null  null     null  \n",
       " expected_delay      VARCHAR      YES   null  null     null  \n",
       " description         VARCHAR      YES   null  null     null  \n",
       " closure_type        VARCHAR      YES   null  null     null  \n",
       " status              VARCHAR      YES   null  null     null  \n",
       " published_date      VARCHAR      YES   null  null     null  \n",
       " centre_easting      VARCHAR      YES   null  null     null  \n",
       " centre_northing     VARCHAR      YES   null  null     null  \n",
       " road                VARCHAR      YES   null  null     null  \n",
       " location            VARCHAR      YES   null  null     null  \n",
       " local_authority     VARCHAR      YES   null  null     null  \n",
       " traffic_management  VARCHAR      YES   null  null     null  \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in 'raw_old_roadworks': 12068\n",
      "\n",
      "First 5 rows from 'raw_old_roadworks':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>reference_number</th><th>start_date</th><th>end_date</th><th>expected_delay</th><th>description</th><th>closure_type</th><th>status</th><th>published_date</th><th>centre_easting</th><th>centre_northing</th><th>road</th><th>location</th><th>local_authority</th><th>traffic_management</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;972963&quot;</td><td>&quot;2010-07-12T07:00:00&quot;</td><td>&quot;2013-03-23T06:00:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;Major junction works will incl</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2011-10-09T21:08:32&quot;</td><td>&quot;456252&quot;</td><td>&quot;278173&quot;</td><td>&quot;M1&quot;</td><td>&quot;Catthorpe&quot;</td><td>&quot;Leicestershire / Northamptonsh</td><td>&quot;Other&quot;</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;978905&quot;</td><td>&quot;2011-04-01T22:00:00&quot;</td><td>&quot;2011-12-31T05:00:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;Contraflow with speed restrict</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2010-04-23T10:18:30&quot;</td><td>&quot;499082&quot;</td><td>&quot;235992&quot;</td><td>&quot;M1&quot;</td><td>&quot;Jct 13 to Jct 12&quot;</td><td>&quot;Bedfordshire / Buckinghamshire&quot;</td><td>&quot;Contraflow&quot;</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;998294&quot;</td><td>&quot;2009-09-24T06:00:00&quot;</td><td>&quot;2013-09-24T05:00:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;Lane 1 closure and 24/7 Hardsh</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2010-06-19T05:03:50&quot;</td><td>&quot;465924&quot;</td><td>&quot;260154&quot;</td><td>&quot;M1&quot;</td><td>&quot;Approach to Junction 16 (21011</td><td>&quot;Northamptonshire&quot;</td><td>&quot;Lane Closure&quot;</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;1172899&quot;</td><td>&quot;2011-10-10T22:00:00&quot;</td><td>&quot;2011-12-03T06:00:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;Lane closures during the day w</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2011-09-28T15:40:36&quot;</td><td>&quot;446842&quot;</td><td>&quot;324130&quot;</td><td>&quot;M1&quot;</td><td>&quot;Junction 23a (220116)&quot;</td><td>&quot;Leicestershire&quot;</td><td>&quot;Carriageway Closure&quot;</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;1306529&quot;</td><td>&quot;2010-08-04T00:00:00&quot;</td><td>&quot;2012-07-05T00:00:00&quot;</td><td>&quot;No Delay&quot;</td><td>&quot;24hrs, lane 1 closure, northbo</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2011-08-22T16:47:52&quot;</td><td>&quot;511897&quot;</td><td>&quot;202047&quot;</td><td>&quot;M1&quot;</td><td>&quot;Jct 6 Exit Slip&quot;</td><td>&quot;Hertfordshire&quot;</td><td>&quot;Lane Closure&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 15)\n",
       "\n",
       " source_fil  reference_  start_date  end_date      road  location   local_aut  traffic_m \n",
       " ename       number      ---         ---            ---   ---        hority     anagement \n",
       " ---         ---         str         str            str   str        ---        ---       \n",
       " str         str                                                     str        str       \n",
       "\n",
       " ha-roadwor  972963      2010-07-12  2013-03-23    M1    Catthorpe  Leicester  Other     \n",
       " ks_2011_10              T07:00:00   T06:00:00                       shire /              \n",
       " _10.xml                                                             Northampt            \n",
       "                                                                     onsh                \n",
       " ha-roadwor  978905      2011-04-01  2011-12-31    M1    Jct 13 to  Bedfordsh  Contraflo \n",
       " ks_2011_10              T22:00:00   T05:00:00            Jct 12     ire / Buc  w         \n",
       " _10.xml                                                             kinghamsh            \n",
       "                                                                     ire                  \n",
       " ha-roadwor  998294      2009-09-24  2013-09-24    M1    Approach   Northampt  Lane      \n",
       " ks_2011_10              T06:00:00   T05:00:00            to         onshire    Closure   \n",
       " _10.xml                                                  Junction                        \n",
       "                                                          16                              \n",
       "                                                          (21011                         \n",
       " ha-roadwor  1172899     2011-10-10  2011-12-03    M1    Junction   Leicester  Carriagew \n",
       " ks_2011_10              T22:00:00   T06:00:00            23a        shire      ay        \n",
       " _10.xml                                                  (220116)              Closure   \n",
       " ha-roadwor  1306529     2010-08-04  2012-07-05    M1    Jct 6      Hertfords  Lane      \n",
       " ks_2011_10              T00:00:00   T00:00:00            Exit Slip  hire       Closure   \n",
       " _10.xml                                                                                  \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inspection Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Inspecting DuckDB Database: {DUCKDB_FILE} ---\")\n",
    "\n",
    "if not os.path.exists(DUCKDB_FILE):\n",
    "    print(f\"Error: Database file '{DUCKDB_FILE}' not found.\")\n",
    "elif not con: # Check if the connection from the previous cell was successful\n",
    "     print(f\"Error: Cannot inspect database. Connection 'con' not established.\")\n",
    "else:\n",
    "    # Connection is already established via con in the previous cell\n",
    "\n",
    "    # --- Inspect NEW Raw Table ---\n",
    "    print(f\"--- Inspecting Table: {new_table} ---\")\n",
    "    try:\n",
    "        # Describe schema using run_query\n",
    "        schema_df_new = run_query(con, f'DESCRIBE \"{new_table}\"')\n",
    "        if schema_df_new is not None and not schema_df_new.is_empty():\n",
    "            print(f\"`DESCRIBE \\\"{new_table}\\\"` returned:\")\n",
    "            display(schema_df_new)\n",
    "        else:\n",
    "             # If DESCRIBE fails or returns empty, the table likely doesn't exist or there was an error\n",
    "             print(f\"Could not retrieve schema for table '{new_table}'. It might not exist or there was a query error.\")\n",
    "             # Skip further inspection for this table\n",
    "             raise duckdb.CatalogException(f\"Table '{new_table}' not found or query failed.\") # Raise exception to skip next steps\n",
    "\n",
    "        # Count rows using run_query\n",
    "        count_df_new = run_query(con, f'SELECT COUNT(*) as count FROM \"{new_table}\"')\n",
    "        if count_df_new is not None and not count_df_new.is_empty():\n",
    "            count_new_val = count_df_new[0, \"count\"]\n",
    "            print(f\"\\nTotal rows in '{new_table}': {count_new_val}\")\n",
    "        else:\n",
    "            print(f\"Could not count rows for table '{new_table}'.\")\n",
    "            count_new_val = 0 # Assume 0 if count fails\n",
    "\n",
    "        # Display sample rows using run_query (only if table has rows)\n",
    "        if count_new_val > 0:\n",
    "            print(f\"\\nFirst 5 rows from '{new_table}':\")\n",
    "            sample_df_new = run_query(con, f'SELECT * FROM \"{new_table}\" LIMIT 5')\n",
    "            if sample_df_new is not None and not sample_df_new.is_empty():\n",
    "                # print(type(sample_df_new)) # Type is known to be Polars DataFrame\n",
    "                display(sample_df_new)\n",
    "            elif sample_df_new is not None and sample_df_new.is_empty():\n",
    "                 print(\"Table has rows, but could not fetch sample (LIMIT 5 returned empty).\")\n",
    "            else:\n",
    "                 print(\"Could not fetch sample rows.\")\n",
    "        elif count_new_val == 0:\n",
    "             print(\"\\nTable appears to be empty.\")\n",
    "\n",
    "\n",
    "    except duckdb.CatalogException as e: # Catch specific error if DESCRIBE failed as intended\n",
    "         print(f\"Skipping further inspection for '{new_table}' due to previous error: {e}\")\n",
    "    except Exception as e: # Catch any other unexpected errors during inspection\n",
    "         print(f\"An unexpected error occurred while inspecting '{new_table}': {e}\")\n",
    "\n",
    "\n",
    "    # --- Inspect OLD Raw Table ---\n",
    "    print(f\"\\n--- Inspecting Table: {old_table} ---\")\n",
    "    try:\n",
    "        # Describe schema using run_query\n",
    "        print(f\"\\nSchema for table '{old_table}':\")\n",
    "        schema_df_old = run_query(con, f'DESCRIBE \"{old_table}\"')\n",
    "        if schema_df_old is not None and not schema_df_old.is_empty():\n",
    "            print(f'`DESCRIBE \"{old_table}\"` returned:')\n",
    "            display(schema_df_old)\n",
    "        else:\n",
    "             print(f\"Could not retrieve schema for table '{old_table}'. It might not exist or there was a query error.\")\n",
    "             raise duckdb.CatalogException(f\"Table '{old_table}' not found or query failed.\")\n",
    "\n",
    "        # Count rows using run_query\n",
    "        count_df_old = run_query(con, f'SELECT COUNT(*) as count FROM \"{old_table}\"')\n",
    "        if count_df_old is not None and not count_df_old.is_empty():\n",
    "            count_old_val = count_df_old[0, \"count\"]\n",
    "            print(f\"\\nTotal rows in '{old_table}': {count_old_val}\")\n",
    "        else:\n",
    "            print(f\"Could not count rows for table '{old_table}'.\")\n",
    "            count_old_val = 0\n",
    "\n",
    "        # Display sample rows using run_query (only if table has rows)\n",
    "        if count_old_val > 0:\n",
    "            print(f\"\\nFirst 5 rows from '{old_table}':\")\n",
    "            sample_df_old = run_query(con, f'SELECT * FROM \"{old_table}\" LIMIT 5')\n",
    "            if sample_df_old is not None and not sample_df_old.is_empty():\n",
    "                display(sample_df_old)\n",
    "            elif sample_df_old is not None and sample_df_old.is_empty():\n",
    "                 print(\"Table has rows, but could not fetch sample (LIMIT 5 returned empty).\")\n",
    "            else:\n",
    "                 print(\"Could not fetch sample rows.\")\n",
    "        elif count_old_val == 0:\n",
    "             print(\"\\nTable appears to be empty.\")\n",
    "\n",
    "\n",
    "    except duckdb.CatalogException as e:\n",
    "         print(f\"Skipping further inspection for '{old_table}' due to previous error: {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"An unexpected error occurred while inspecting '{old_table}': {e}\")\n",
    "\n",
    "    # No need to close con_inspect as we are using the global con_check\n",
    "    # The con_check connection will be closed later after all checks are done.\n",
    "    # print(\"\\nInspection connection closed.\") # Remove this line\n",
    "\n",
    "print(\"\\n--- Inspection Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21be031",
   "metadata": {},
   "source": [
    "### Basic checks\n",
    "1. Count NULLs & empty string placeholders\n",
    "1. Check string length range of each column (e.g.: Is NEW_EVENT_NUMBER fixed length?)\n",
    "1. Examine categorical values (e.g. STATUS, EXPDEL)\n",
    "1. Check identifyer uniqueness across tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a051b",
   "metadata": {},
   "source": [
    "#### NULL & Placeholder check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd920891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Basic Data Quality Checks ---\n",
      "--- Running Check 1: NULL and Placeholder Counts ---\n",
      "\n",
      "--- Analyzing Table for NULLs/Placeholders: raw_new_roadworks ---\n",
      "Total Rows: 11353\n",
      "\n",
      "1. NULL and Placeholder Counts (Summary):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (13, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Column</th><th>Null Count</th><th>Null %</th><th>Placeholder Count</th><th>Placeholder %</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;source_filename&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;NEW_EVENT_NUMBER&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;OLD_REFERENCE_NUMBER&quot;</td><td>10692</td><td>&quot;(94.18%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;SDATE&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;EDATE&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;EXPDEL&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;DESCRIPTION&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;CLOSURE_TYPE&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;STATUS&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;PUBLISHED_DATE&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;CENTRE_EASTING&quot;</td><td>7</td><td>&quot;(0.06%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;CENTRE_NORTHING&quot;</td><td>7</td><td>&quot;(0.06%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;ROAD_NUMBERS&quot;</td><td>7</td><td>&quot;(0.06%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (13, 5)\n",
       "\n",
       " Column                Null Count  Null %    Placeholder Count  Placeholder % \n",
       " ---                   ---         ---       ---                ---           \n",
       " str                   i64         str       i64                str           \n",
       "\n",
       " source_filename       0           (0.00%)   0                  (0.00%)       \n",
       " NEW_EVENT_NUMBER      0           (0.00%)   0                  (0.00%)       \n",
       " OLD_REFERENCE_NUMBER  10692       (94.18%)  0                  (0.00%)       \n",
       " SDATE                 0           (0.00%)   0                  (0.00%)       \n",
       " EDATE                 0           (0.00%)   0                  (0.00%)       \n",
       " EXPDEL                0           (0.00%)   0                  (0.00%)       \n",
       " DESCRIPTION           0           (0.00%)   0                  (0.00%)       \n",
       " CLOSURE_TYPE          0           (0.00%)   0                  (0.00%)       \n",
       " STATUS                0           (0.00%)   0                  (0.00%)       \n",
       " PUBLISHED_DATE        0           (0.00%)   0                  (0.00%)       \n",
       " CENTRE_EASTING        7           (0.06%)   0                  (0.00%)       \n",
       " CENTRE_NORTHING       7           (0.06%)   0                  (0.00%)       \n",
       " ROAD_NUMBERS          7           (0.06%)   0                  (0.00%)       \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Examples of Records with Placeholders (Limit 5 per placeholder type per column):\n",
      "    No specific placeholder examples to show for any column in raw_new_roadworks.\n",
      "\n",
      "--- Analyzing Table for NULLs/Placeholders: raw_old_roadworks ---\n",
      "Total Rows: 12068\n",
      "\n",
      "1. NULL and Placeholder Counts (Summary):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (15, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Column</th><th>Null Count</th><th>Null %</th><th>Placeholder Count</th><th>Placeholder %</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;source_filename&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;reference_number&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;start_date&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;end_date&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;expected_delay&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;description&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;closure_type&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;status&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;published_date&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;centre_easting&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;centre_northing&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;road&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;location&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;local_authority&quot;</td><td>7</td><td>&quot;(0.06%)&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td></tr><tr><td>&quot;traffic_management&quot;</td><td>0</td><td>&quot;(0.00%)&quot;</td><td>290</td><td>&quot;(2.40%)&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (15, 5)\n",
       "\n",
       " Column              Null Count  Null %   Placeholder Count  Placeholder % \n",
       " ---                 ---         ---      ---                ---           \n",
       " str                 i64         str      i64                str           \n",
       "\n",
       " source_filename     0           (0.00%)  0                  (0.00%)       \n",
       " reference_number    0           (0.00%)  0                  (0.00%)       \n",
       " start_date          0           (0.00%)  0                  (0.00%)       \n",
       " end_date            0           (0.00%)  0                  (0.00%)       \n",
       " expected_delay      0           (0.00%)  0                  (0.00%)       \n",
       " description         0           (0.00%)  0                  (0.00%)       \n",
       " closure_type        0           (0.00%)  0                  (0.00%)       \n",
       " status              0           (0.00%)  0                  (0.00%)       \n",
       " published_date      0           (0.00%)  0                  (0.00%)       \n",
       " centre_easting      0           (0.00%)  0                  (0.00%)       \n",
       " centre_northing     0           (0.00%)  0                  (0.00%)       \n",
       " road                0           (0.00%)  0                  (0.00%)       \n",
       " location            0           (0.00%)  0                  (0.00%)       \n",
       " local_authority     7           (0.06%)  0                  (0.00%)       \n",
       " traffic_management  0           (0.00%)  290                (2.40%)       \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Examples of Records with Placeholders (Limit 5 per placeholder type per column):\n",
      "\n",
      "    --- Column: 'traffic_management' ---\n",
      "      Records with placeholder 'none':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>identifier</th><th>source_filename</th><th>problematic_value</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;1853252&quot;</td><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;None&quot;</td></tr><tr><td>&quot;1837978&quot;</td><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;None&quot;</td></tr><tr><td>&quot;1862058&quot;</td><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;None&quot;</td></tr><tr><td>&quot;1848669&quot;</td><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;None&quot;</td></tr><tr><td>&quot;564571&quot;</td><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;None&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "\n",
       " identifier  source_filename              problematic_value \n",
       " ---         ---                          ---               \n",
       " str         str                          str               \n",
       "\n",
       " 1853252     ha-roadworks_2011_10_10.xml  None              \n",
       " 1837978     ha-roadworks_2011_10_10.xml  None              \n",
       " 1862058     ha-roadworks_2011_10_10.xml  None              \n",
       " 1848669     ha-roadworks_2011_10_10.xml  None              \n",
       " 564571      ha-roadworks_2011_10_10.xml  None              \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Check 1: NULL and Placeholder Counts Complete ---\n"
     ]
    }
   ],
   "source": [
    "pl.Config.set_tbl_rows(50)\n",
    "\n",
    "# Ensure connection 'con' from the previous cell is available and valid\n",
    "if con is None:\n",
    "    print(\"Error: Database connection 'con' is not established. Please run the connection cell first.\")\n",
    "\n",
    "print(\"--- Running Basic Data Quality Checks ---\")\n",
    "\n",
    "# Define common placeholders (lowercase for case-insensitive comparison)\n",
    "PLACEHOLDERS_LOWER = [\"\", \"none\", \"n/a\", \"null\", \"unknown\"]\n",
    "# Create SQL list string like \"('', 'none', 'n/a', 'null', 'unknown')\"\n",
    "PLACEHOLDERS_SQL_LIST = f\"({', '.join([f'{pl!r}' for pl in PLACEHOLDERS_LOWER])})\"\n",
    "\n",
    "\n",
    "# --- Check 1: NULL and Placeholder Counts ---\n",
    "print(\"--- Running Check 1: NULL and Placeholder Counts ---\")\n",
    "for table_name, columns in TABLES_INFO.items():\n",
    "    print(f\"\\n--- Analyzing Table for NULLs/Placeholders: {table_name} ---\")\n",
    "\n",
    "    count_df = run_query(con, f'SELECT COUNT(*) as total_rows FROM \"{table_name}\"')\n",
    "    total_rows = 0\n",
    "    if count_df is not None and not count_df.is_empty():\n",
    "        total_rows = count_df[0, \"total_rows\"]\n",
    "    print(f\"Total Rows: {total_rows}\")\n",
    "\n",
    "    if total_rows == 0:\n",
    "        print(\"Table is empty. Skipping NULL/Placeholder checks for this table.\")\n",
    "        continue\n",
    "\n",
    "    print(\"\\n1. NULL and Placeholder Counts (Summary):\")\n",
    "    null_placeholder_results = []\n",
    "    for col in columns:\n",
    "        null_query = f'SELECT COUNT(*) as null_count FROM \"{table_name}\" WHERE \"{col}\" IS NULL'\n",
    "        null_df = run_query(con, null_query)\n",
    "        null_count = null_df[0, \"null_count\"] if null_df is not None and not null_df.is_empty() else 'Error'\n",
    "\n",
    "        placeholder_query = f'''\n",
    "            SELECT COUNT(*) as placeholder_count\n",
    "            FROM \"{table_name}\"\n",
    "            WHERE lower(trim(\"{col}\")) IN {PLACEHOLDERS_SQL_LIST}\n",
    "                AND \"{col}\" IS NOT NULL\n",
    "        '''\n",
    "        placeholder_df = run_query(con, placeholder_query)\n",
    "        placeholder_count = placeholder_df[0, \"placeholder_count\"] if placeholder_df is not None and not placeholder_df.is_empty() else 'Error'\n",
    "\n",
    "        if null_count != 'Error' and placeholder_count != 'Error':\n",
    "                null_perc = f\"({(null_count / total_rows * 100):.2f}%)\" if total_rows > 0 else \"\"\n",
    "                placeholder_perc = f\"({(placeholder_count / total_rows * 100):.2f}%)\" if total_rows > 0 else \"\"\n",
    "                null_placeholder_results.append({\n",
    "                    \"Column\": col,\n",
    "                    \"Null Count\": null_count,\n",
    "                    \"Null %\": null_perc,\n",
    "                    \"Placeholder Count\": placeholder_count,\n",
    "                    \"Placeholder %\": placeholder_perc\n",
    "                })\n",
    "        else:\n",
    "                null_placeholder_results.append({\n",
    "                    \"Column\": col,\n",
    "                    \"Null Count\": null_count,\n",
    "                    \"Null %\": \"N/A\",\n",
    "                    \"Placeholder Count\": placeholder_count,\n",
    "                    \"Placeholder %\": \"N/A\"\n",
    "                })\n",
    "\n",
    "    if null_placeholder_results:\n",
    "            display(pl.DataFrame(null_placeholder_results))\n",
    "    else:\n",
    "            print(\"  Could not retrieve NULL/Placeholder counts summary.\")\n",
    "            \n",
    "    print(\"\\n  Examples of Records with Placeholders (Limit 5 per placeholder type per column):\")\n",
    "    placeholders_found_overall_for_table = False\n",
    "    for col in columns:\n",
    "        id_col_name = 'NEW_EVENT_NUMBER' if table_name == RAW_NEW_TABLE_NAME else 'reference_number'\n",
    "        \n",
    "        if id_col_name not in columns or 'source_filename' not in columns:\n",
    "            # print(f\"    Skipping detailed placeholder check for column '{col}' in table '{table_name}': Identifier or source_filename not in columns.\")\n",
    "            continue\n",
    "\n",
    "        placeholders_found_for_this_col_overall = False\n",
    "        for placeholder_value in PLACEHOLDERS_LOWER:\n",
    "            sql_placeholder_value = placeholder_value.replace(\"'\", \"''\")\n",
    "            \n",
    "            if placeholder_value == \"\": \n",
    "                placeholder_condition = f\"trim(\\\"{col}\\\") = ''\"\n",
    "            else:\n",
    "                placeholder_condition = f\"lower(trim(\\\"{col}\\\")) = '{sql_placeholder_value}'\"\n",
    "\n",
    "            details_query = f'''\n",
    "                SELECT \"{id_col_name}\" AS identifier, \"source_filename\", \"{col}\" AS problematic_value\n",
    "                FROM \"{table_name}\"\n",
    "                WHERE {placeholder_condition} AND \"{col}\" IS NOT NULL\n",
    "                LIMIT 5\n",
    "            '''\n",
    "            details_df = run_query(con, details_query)\n",
    "\n",
    "            if details_df is not None and not details_df.is_empty():\n",
    "                if not placeholders_found_for_this_col_overall:\n",
    "                    print(f\"\\n    --- Column: '{col}' ---\")\n",
    "                    placeholders_found_for_this_col_overall = True\n",
    "                    placeholders_found_overall_for_table = True\n",
    "                \n",
    "                display_placeholder_name = f\"'{placeholder_value}'\" if placeholder_value != \"\" else \"(empty string)\"\n",
    "                print(f\"      Records with placeholder {display_placeholder_name}:\")\n",
    "                display(details_df)\n",
    "    \n",
    "    if not placeholders_found_overall_for_table and total_rows > 0 :\n",
    "        print(f\"    No specific placeholder examples to show for any column in {table_name}.\")\n",
    "print(\"--- Check 1: NULL and Placeholder Counts Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74a1442",
   "metadata": {},
   "source": [
    "#### String length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d103a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Check 2: String Length Analysis ---\n",
      "\n",
      "--- Analyzing Table for String Lengths: raw_new_roadworks ---\n",
      "\n",
      "2. String Length Analysis:\n",
      "  Shortest string example for 'source_filename' (length 25): ['nh_roadworks_2023_3_6.xml']\n",
      "  Longest string example for 'source_filename' (length 27): ['he_roadworks_2018_02_26.xml', 'he_roadworks_2020_05_25.xml', 'he_roadworks_2021_03_01.xml']\n",
      "  Shortest string example for 'NEW_EVENT_NUMBER' (length 12): ['00028139-001', '00032007-002', '00033568-004']\n",
      "  Longest string example for 'NEW_EVENT_NUMBER' (length 12): ['00028402-003', '00005530-003', '00031914-001']\n",
      "  Shortest string example for 'OLD_REFERENCE_NUMBER' (length 4): ['7592', '7383', '5662']\n",
      "  Longest string example for 'OLD_REFERENCE_NUMBER' (length 8): ['12018198']\n",
      "  Shortest string example for 'SDATE' (length 17): ['10-FEB-2018 22:00', '05-SEP-2017 21:00', '12-MAR-2018 09:00']\n",
      "  Longest string example for 'SDATE' (length 17): ['10-FEB-2018 22:00', '05-SEP-2017 21:00', '12-MAR-2018 09:00']\n",
      "  Shortest string example for 'EDATE' (length 17): ['05-MAR-2018 05:00', '29-MAR-2018 06:00', '06-MAR-2018 06:00']\n",
      "  Longest string example for 'EDATE' (length 17): ['02-MAR-2018 06:00', '23-MAR-2018 05:00', '09-MAR-2018 06:00']\n",
      "  Shortest string example for 'EXPDEL' (length 23): ['Moderate (10 - 30 mins)']\n",
      "  Longest string example for 'EXPDEL' (length 26): ['Slight (less than 10 mins)', 'Severe (more than 30 mins)']\n",
      "  Shortest string example for 'DESCRIPTION' (length 14): ['abnormal load ']\n",
      "  Longest string example for 'DESCRIPTION' (length 384): ['M20 Eastbound and Westbound between junctions 9 and 11, M20 J10a Scheme- Major Network Scheme Improvements - New Roundabouts and Realignment of A2070  including new structures and demolition of redundant structures - Nights - Carriageway Closures  Slip Road Closures  Narrow Lanes, lane closures,   50mph Speed Restrictions  Width Restrictions and Verge Works for Junction 10a project']\n",
      "  Shortest string example for 'CLOSURE_TYPE' (length 7): ['Embargo']\n",
      "  Longest string example for 'CLOSURE_TYPE' (length 38): ['Emergency and urgent Street/Road Works']\n",
      "  Shortest string example for 'STATUS' (length 6): ['Shared']\n",
      "  Longest string example for 'STATUS' (length 9): ['Published']\n",
      "  Shortest string example for 'PUBLISHED_DATE' (length 19): ['2018-02-23T14:16:41', '2018-02-16T15:45:44', '2018-01-17T09:38:21']\n",
      "  Longest string example for 'PUBLISHED_DATE' (length 19): ['2018-02-15T14:38:05', '2017-12-20T11:55:04', '2017-12-21T11:29:55']\n",
      "  Shortest string example for 'CENTRE_EASTING' (length 6): ['466461', '472582', '475665']\n",
      "  Longest string example for 'CENTRE_EASTING' (length 6): ['652209', '413039', '419534']\n",
      "  Shortest string example for 'CENTRE_NORTHING' (length 5): ['65963', '98886', '92172']\n",
      "  Longest string example for 'CENTRE_NORTHING' (length 6): ['422243', '451658', '201298']\n",
      "  Shortest string example for 'ROAD_NUMBERS' (length 2): ['M4', 'A2', 'M6']\n",
      "  Longest string example for 'ROAD_NUMBERS' (length 60): ['A2; A20; A2070; A21; A23; A249; A259; A26; A27; M2; M20; M23']\n",
      "\n",
      "  String Length Statistics Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (13, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Column</th><th>Min Length</th><th>Max Length</th><th>Avg Length</th><th>StdDev Length</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;source_filename&quot;</td><td>25</td><td>27</td><td>&quot;26.37&quot;</td><td>&quot;0.69&quot;</td></tr><tr><td>&quot;NEW_EVENT_NUMBER&quot;</td><td>12</td><td>12</td><td>&quot;12.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;OLD_REFERENCE_NUMBER&quot;</td><td>4</td><td>8</td><td>&quot;6.09&quot;</td><td>&quot;0.96&quot;</td></tr><tr><td>&quot;SDATE&quot;</td><td>17</td><td>17</td><td>&quot;17.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;EDATE&quot;</td><td>17</td><td>17</td><td>&quot;17.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;EXPDEL&quot;</td><td>23</td><td>26</td><td>&quot;25.24&quot;</td><td>&quot;1.30&quot;</td></tr><tr><td>&quot;DESCRIPTION&quot;</td><td>14</td><td>384</td><td>&quot;101.89&quot;</td><td>&quot;39.12&quot;</td></tr><tr><td>&quot;CLOSURE_TYPE&quot;</td><td>7</td><td>38</td><td>&quot;19.52&quot;</td><td>&quot;6.00&quot;</td></tr><tr><td>&quot;STATUS&quot;</td><td>6</td><td>9</td><td>&quot;9.00&quot;</td><td>&quot;0.05&quot;</td></tr><tr><td>&quot;PUBLISHED_DATE&quot;</td><td>19</td><td>19</td><td>&quot;19.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;CENTRE_EASTING&quot;</td><td>6</td><td>6</td><td>&quot;6.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;CENTRE_NORTHING&quot;</td><td>5</td><td>6</td><td>&quot;5.96&quot;</td><td>&quot;0.19&quot;</td></tr><tr><td>&quot;ROAD_NUMBERS&quot;</td><td>2</td><td>60</td><td>&quot;4.29&quot;</td><td>&quot;3.82&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (13, 5)\n",
       "\n",
       " Column                Min Length  Max Length  Avg Length  StdDev Length \n",
       " ---                   ---         ---         ---         ---           \n",
       " str                   i64         i64         str         str           \n",
       "\n",
       " source_filename       25          27          26.37       0.69          \n",
       " NEW_EVENT_NUMBER      12          12          12.00       0.00          \n",
       " OLD_REFERENCE_NUMBER  4           8           6.09        0.96          \n",
       " SDATE                 17          17          17.00       0.00          \n",
       " EDATE                 17          17          17.00       0.00          \n",
       " EXPDEL                23          26          25.24       1.30          \n",
       " DESCRIPTION           14          384         101.89      39.12         \n",
       " CLOSURE_TYPE          7           38          19.52       6.00          \n",
       " STATUS                6           9           9.00        0.05          \n",
       " PUBLISHED_DATE        19          19          19.00       0.00          \n",
       " CENTRE_EASTING        6           6           6.00        0.00          \n",
       " CENTRE_NORTHING       5           6           5.96        0.19          \n",
       " ROAD_NUMBERS          2           60          4.29        3.82          \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Table for String Lengths: raw_old_roadworks ---\n",
      "\n",
      "2. String Length Analysis:\n",
      "  Shortest string example for 'source_filename' (length 27): ['ha_roadworks_2015_03_16.xml', 'ha-roadworks_2013_05_06.xml', 'ha-roadworks_2011_10_10.xml']\n",
      "  Longest string example for 'source_filename' (length 27): ['ha-roadworks_2014_03_31.xml', 'ha_roadworks_2015_03_16.xml', 'ha-roadworks_2013_05_06.xml']\n",
      "  Shortest string example for 'reference_number' (length 6): ['738602', '625027', '265721']\n",
      "  Longest string example for 'reference_number' (length 7): ['1705796', '1851621', '1859353']\n",
      "  Shortest string example for 'start_date' (length 19): ['2011-08-09T22:00:00', '2011-10-18T09:30:00', '2011-10-14T08:00:00']\n",
      "  Longest string example for 'start_date' (length 19): ['2011-08-01T21:49:00', '2011-10-17T09:30:00', '2011-10-15T22:00:00']\n",
      "  Shortest string example for 'end_date' (length 19): ['2012-04-30T17:00:00', '2011-10-18T15:30:00', '2011-10-14T17:00:00']\n",
      "  Longest string example for 'end_date' (length 19): ['2011-10-25T06:00:00', '2011-10-20T06:00:00', '2011-10-22T05:00:00']\n",
      "  Shortest string example for 'expected_delay' (length 8): ['No Delay']\n",
      "  Longest string example for 'expected_delay' (length 26): ['Severe (more than 30 mins)', 'Slight (less than 10 mins)']\n",
      "  Shortest string example for 'description' (length 3): ['RTC', 'VCS', 'TBC']\n",
      "  Longest string example for 'description' (length 1988): ['A57 Hattersley Roundabout Pavement Patching (4221) with full closures on slips.  Diversion route in place Closure of A57 EB exit slip: Continue on roundabout and exit on A560 southbound. Turn left onto Ashworth Lane to the junction with Market Street. Turn left onto Market Street and continue NB to the junction with A57 at  Jollies Corner where the diversion ends.  2. Closure of A57 WB entry slip: Diversion starts at A57/ B6174 junction (Jollies Corner). Follow Market Street SB then turn right onto Ashworth Lane and continue to the junction with the A560. Turn right onto the A560 NB to the roundabout where the diversion ends.  3. Closure of A560 SB exit slip: Continue on roundabout and exit on A57 EB. Turn right at Jollies Corner and follow Market Street SB. Turn right onto Ashworth Lane to the junction with the A560 where the diversion ends.   4. Closure of A560 NB entry slip: Diversion starts at junction of A560 and Ashworth Lane. Follow Ashworth Lane EB to the junction with Market Street. Turn left onto Market Street and continue to the junction with A57 (Jollies Corner). Turn right onto A57 WB and continue to the roundabout where the diversion ends.   5. Closure of A57 EB exit slip: Continue on roundabout and exit on A560 SB. Turn right onto Underwood Road and continue WB to the junction with Hattersley Road West. Turn right onto Hattersley Road West and continue NB to junction with A57 Mottram Road where the diversion ends.   6. Closure of A57 WB entry slip: Diversion starts at the junction of the A57 Mottram Road and Hattersley Road West. Follow Hattersley Road West to the junction with Underwood Road. Turn left onto Underwood Road and continue EB to the junction with the A560. Turn left onto the A560 and continue NB to the roundabout where the diversion ends.   7. Closure of M67 WB exit slip: (Strategic Diversion Route) Continue on roundabout and exit on A57 WB. Continue to the junction with Clark Way. Turn right here and continue to junction with']\n",
      "  Shortest string example for 'closure_type' (length 13): ['Planned Works']\n",
      "  Longest string example for 'closure_type' (length 15): ['Emergency Works']\n",
      "  Shortest string example for 'status' (length 4): ['Firm']\n",
      "  Longest string example for 'status' (length 11): ['Provisional']\n",
      "  Shortest string example for 'published_date' (length 19): ['2011-06-07T14:37:41', '2011-08-02T10:43:19', '2011-09-23T08:44:48']\n",
      "  Longest string example for 'published_date' (length 19): ['2011-10-09T21:08:32', '2011-08-22T16:47:52', '2011-10-05T14:41:44']\n",
      "  Shortest string example for 'centre_easting' (length 1): ['0']\n",
      "  Longest string example for 'centre_easting' (length 6): ['443281', '511802', '446170']\n",
      "  Shortest string example for 'centre_northing' (length 1): ['0']\n",
      "  Longest string example for 'centre_northing' (length 6): ['223315', '429819', '202069']\n",
      "  Shortest string example for 'road' (length 2): ['A3', 'A5', 'A4']\n",
      "  Longest string example for 'road' (length 5): ['A194M', 'A6055', 'UETON']\n",
      "  Shortest string example for 'location' (length 2): ['J6', '43', 'J9']\n",
      "  Longest string example for 'location' (length 70): ['M4 WB Jct 20-19 includes D&K loop, J19 WB entry slip (taper on M32 NB)', 'In grass verge in lay-by. Approximately 215 metres from entrance to...', 'N/B & S/B closure B1111 Interchange Larling to Spooner Row Interchange']\n",
      "  Shortest string example for 'local_authority' (length 4): ['Avon', 'Kent', 'Bury']\n",
      "  Longest string example for 'local_authority' (length 100): ['Buckinghamshire / Slough / Swindon / West Berkshire / Wiltshire / Windsor and Maidenhead / Wokingham', 'Durham / Gateshead / Newcastle upon Tyne / North Tyneside / North Yorkshire / Northumberland / South', 'Calderdale / Humberside / Kirklees / Leeds / North Yorkshire / South Yorkshire / Wakefield / West Yo']\n",
      "  Shortest string example for 'traffic_management' (length 4): ['None']\n",
      "  Longest string example for 'traffic_management' (length 27): ['Lane Closure with Switching']\n",
      "\n",
      "  String Length Statistics Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (15, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Column</th><th>Min Length</th><th>Max Length</th><th>Avg Length</th><th>StdDev Length</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;source_filename&quot;</td><td>27</td><td>27</td><td>&quot;27.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;reference_number&quot;</td><td>6</td><td>7</td><td>&quot;6.99&quot;</td><td>&quot;0.07&quot;</td></tr><tr><td>&quot;start_date&quot;</td><td>19</td><td>19</td><td>&quot;19.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;end_date&quot;</td><td>19</td><td>19</td><td>&quot;19.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;expected_delay&quot;</td><td>8</td><td>26</td><td>&quot;22.39&quot;</td><td>&quot;6.66&quot;</td></tr><tr><td>&quot;description&quot;</td><td>3</td><td>1988</td><td>&quot;80.12&quot;</td><td>&quot;56.20&quot;</td></tr><tr><td>&quot;closure_type&quot;</td><td>13</td><td>15</td><td>&quot;13.14&quot;</td><td>&quot;0.51&quot;</td></tr><tr><td>&quot;status&quot;</td><td>4</td><td>11</td><td>&quot;4.70&quot;</td><td>&quot;2.11&quot;</td></tr><tr><td>&quot;published_date&quot;</td><td>19</td><td>19</td><td>&quot;19.00&quot;</td><td>&quot;0.00&quot;</td></tr><tr><td>&quot;centre_easting&quot;</td><td>1</td><td>6</td><td>&quot;5.99&quot;</td><td>&quot;0.19&quot;</td></tr><tr><td>&quot;centre_northing&quot;</td><td>1</td><td>6</td><td>&quot;5.96&quot;</td><td>&quot;0.26&quot;</td></tr><tr><td>&quot;road&quot;</td><td>2</td><td>5</td><td>&quot;2.79&quot;</td><td>&quot;0.66&quot;</td></tr><tr><td>&quot;location&quot;</td><td>2</td><td>70</td><td>&quot;27.55&quot;</td><td>&quot;14.98&quot;</td></tr><tr><td>&quot;local_authority&quot;</td><td>4</td><td>100</td><td>&quot;12.51&quot;</td><td>&quot;8.81&quot;</td></tr><tr><td>&quot;traffic_management&quot;</td><td>4</td><td>27</td><td>&quot;14.14&quot;</td><td>&quot;4.21&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (15, 5)\n",
       "\n",
       " Column              Min Length  Max Length  Avg Length  StdDev Length \n",
       " ---                 ---         ---         ---         ---           \n",
       " str                 i64         i64         str         str           \n",
       "\n",
       " source_filename     27          27          27.00       0.00          \n",
       " reference_number    6           7           6.99        0.07          \n",
       " start_date          19          19          19.00       0.00          \n",
       " end_date            19          19          19.00       0.00          \n",
       " expected_delay      8           26          22.39       6.66          \n",
       " description         3           1988        80.12       56.20         \n",
       " closure_type        13          15          13.14       0.51          \n",
       " status              4           11          4.70        2.11          \n",
       " published_date      19          19          19.00       0.00          \n",
       " centre_easting      1           6           5.99        0.19          \n",
       " centre_northing     1           6           5.96        0.26          \n",
       " road                2           5           2.79        0.66          \n",
       " location            2           70          27.55       14.98         \n",
       " local_authority     4           100         12.51       8.81          \n",
       " traffic_management  4           27          14.14       4.21          \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Check 2: String Length Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Check 2: String Length Analysis ---\n",
    "print(\"--- Running Check 2: String Length Analysis ---\")\n",
    "for table_name, columns in TABLES_INFO.items():\n",
    "    print(f\"\\n--- Analyzing Table for String Lengths: {table_name} ---\")\n",
    "    \n",
    "    count_df = run_query(con, f'SELECT COUNT(*) as total_rows FROM \"{table_name}\"')\n",
    "    total_rows = 0\n",
    "    if count_df is not None and not count_df.is_empty():\n",
    "        total_rows = count_df[0, \"total_rows\"]\n",
    "    # print(f\"Total Rows: {total_rows}\") # Optional context\n",
    "\n",
    "    if total_rows == 0:\n",
    "        print(\"Table is empty. Skipping string length checks for this table.\")\n",
    "        continue\n",
    "\n",
    "    print(\"\\n2. String Length Analysis:\")\n",
    "    length_results = []\n",
    "    for col in columns:\n",
    "        length_stats_query = f'''\n",
    "            SELECT MIN(LENGTH(\"{col}\")) as min_len,\n",
    "                    MAX(LENGTH(\"{col}\")) as max_len,\n",
    "                    AVG(LENGTH(\"{col}\")) as avg_len,\n",
    "                    STDDEV_POP(LENGTH(\"{col}\")) as stddev_len\n",
    "            FROM \"{table_name}\"\n",
    "            WHERE \"{col}\" IS NOT NULL AND trim(\"{col}\") != ''\n",
    "        '''\n",
    "        length_df = run_query(con, length_stats_query)\n",
    "\n",
    "        min_len, max_len, avg_len, stddev_len = \"Error\", \"Error\", \"Error\", \"Error\"\n",
    "        if length_df is not None and not length_df.is_empty():\n",
    "            min_len = length_df[0, \"min_len\"]\n",
    "            max_len = length_df[0, \"max_len\"]\n",
    "            avg_len_val = length_df[0, \"avg_len\"]\n",
    "            stddev_len_val = length_df[0, \"stddev_len\"]\n",
    "            \n",
    "            avg_len = f\"{avg_len_val:.2f}\" if avg_len_val is not None else \"N/A\"\n",
    "            stddev_len = f\"{stddev_len_val:.2f}\" if stddev_len_val is not None else \"N/A\"\n",
    "\n",
    "        length_results.append({\n",
    "            \"Column\": col,\n",
    "            \"Min Length\": min_len,\n",
    "            \"Max Length\": max_len,\n",
    "            \"Avg Length\": avg_len,\n",
    "            \"StdDev Length\": stddev_len\n",
    "        })\n",
    "        \n",
    "        if min_len != \"Error\" and min_len is not None:\n",
    "            shortest_strings_query = f'''\n",
    "                SELECT DISTINCT \"{col}\" as val\n",
    "                FROM \"{table_name}\"\n",
    "                WHERE \"{col}\" IS NOT NULL AND LENGTH(\"{col}\") = {min_len}\n",
    "                LIMIT 3\n",
    "            '''\n",
    "            shortest_df = run_query(con, shortest_strings_query)\n",
    "            if shortest_df is not None and not shortest_df.is_empty():\n",
    "                print(f\"  Shortest string example for '{col}' (length {min_len}): {shortest_df['val'].to_list()}\")\n",
    "\n",
    "        if max_len != \"Error\" and max_len is not None:\n",
    "            longest_strings_query = f'''\n",
    "                SELECT DISTINCT \"{col}\" as val\n",
    "                FROM \"{table_name}\"\n",
    "                WHERE \"{col}\" IS NOT NULL AND LENGTH(\"{col}\") = {max_len}\n",
    "                LIMIT 3\n",
    "            '''\n",
    "            longest_df = run_query(con, longest_strings_query)\n",
    "            if longest_df is not None and not longest_df.is_empty():\n",
    "                print(f\"  Longest string example for '{col}' (length {max_len}): {longest_df['val'].to_list()}\")\n",
    "\n",
    "    if length_results:\n",
    "        print(\"\\n  String Length Statistics Summary:\")\n",
    "        display(pl.DataFrame(length_results))\n",
    "    else:\n",
    "        print(\"  Could not retrieve string length statistics.\")\n",
    "print(\"--- Check 2: String Length Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765dbdae",
   "metadata": {},
   "source": [
    "#### Categorical variable check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4492b146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Check 3: Categorical Value Counts ---\n",
      "\n",
      "--- Analyzing Table for Categorical Values: raw_new_roadworks ---\n",
      "\n",
      "3. Categorical Value Counts:\n",
      "\n",
      "  Distinct values for 'STATUS':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>STATUS</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Published&quot;</td><td>11350</td></tr><tr><td>&quot;Shared&quot;</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "\n",
       " STATUS     count \n",
       " ---        ---   \n",
       " str        i64   \n",
       "\n",
       " Published  11350 \n",
       " Shared     3     \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Distinct values for 'EXPDEL':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>EXPDEL</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Slight (less than 10 mins)&quot;</td><td>8417</td></tr><tr><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>2877</td></tr><tr><td>&quot;Severe (more than 30 mins)&quot;</td><td>59</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "\n",
       " EXPDEL                      count \n",
       " ---                         ---   \n",
       " str                         i64   \n",
       "\n",
       " Slight (less than 10 mins)  8417  \n",
       " Moderate (10 - 30 mins)     2877  \n",
       " Severe (more than 30 mins)  59    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Distinct values for 'CLOSURE_TYPE':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (22, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>CLOSURE_TYPE</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Programmed Routine Works&quot;</td><td>3675</td></tr><tr><td>&quot;Area Schemes&quot;</td><td>1850</td></tr><tr><td>&quot;Major Schemes&quot;</td><td>1242</td></tr><tr><td>&quot;Area Renewals&quot;</td><td>956</td></tr><tr><td>&quot;Emergency Routine Works&quot;</td><td>768</td></tr><tr><td>&quot;Ad-hoc Routine Works&quot;</td><td>666</td></tr><tr><td>&quot;Regional Technology Works&quot;</td><td>368</td></tr><tr><td>&quot;Diversion/Alternate Route&quot;</td><td>336</td></tr><tr><td>&quot;Ad-hoc Street/Road Works&quot;</td><td>302</td></tr><tr><td>&quot;Programmed Street/Road Works&quot;</td><td>292</td></tr><tr><td>&quot;Developer Works&quot;</td><td>259</td></tr><tr><td>&quot;Off Network&quot;</td><td>136</td></tr><tr><td>&quot;Emergency Regional Technology </td><td>121</td></tr><tr><td>&quot;Abnormal Load Movements&quot;</td><td>82</td></tr><tr><td>&quot;Regional Technology Schemes&quot;</td><td>81</td></tr><tr><td>&quot;National Technology Works&quot;</td><td>53</td></tr><tr><td>&quot;Licensee Works&quot;</td><td>49</td></tr><tr><td>&quot;Emergency and urgent Street/Ro</td><td>42</td></tr><tr><td>&quot;Embargo&quot;</td><td>39</td></tr><tr><td>&quot;Emergency National Technology </td><td>25</td></tr><tr><td>&quot;Traffic Incidents&quot;</td><td>6</td></tr><tr><td>&quot;Short Stop Activities&quot;</td><td>5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (22, 2)\n",
       "\n",
       " CLOSURE_TYPE                     count \n",
       " ---                              ---   \n",
       " str                              i64   \n",
       "\n",
       " Programmed Routine Works         3675  \n",
       " Area Schemes                     1850  \n",
       " Major Schemes                    1242  \n",
       " Area Renewals                    956   \n",
       " Emergency Routine Works          768   \n",
       " Ad-hoc Routine Works             666   \n",
       " Regional Technology Works        368   \n",
       " Diversion/Alternate Route        336   \n",
       " Ad-hoc Street/Road Works         302   \n",
       " Programmed Street/Road Works     292   \n",
       " Developer Works                  259   \n",
       " Off Network                      136   \n",
       " Emergency Regional Technology   121   \n",
       " Abnormal Load Movements          82    \n",
       " Regional Technology Schemes      81    \n",
       " National Technology Works        53    \n",
       " Licensee Works                   49    \n",
       " Emergency and urgent Street/Ro  42    \n",
       " Embargo                          39    \n",
       " Emergency National Technology   25    \n",
       " Traffic Incidents                6     \n",
       " Short Stop Activities            5     \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Table for Categorical Values: raw_old_roadworks ---\n",
      "\n",
      "3. Categorical Value Counts:\n",
      "\n",
      "  Distinct values for 'status':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>status</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Firm&quot;</td><td>10853</td></tr><tr><td>&quot;Provisional&quot;</td><td>1215</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "\n",
       " status       count \n",
       " ---          ---   \n",
       " str          i64   \n",
       "\n",
       " Firm         10853 \n",
       " Provisional  1215  \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Distinct values for 'expected_delay':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>expected_delay</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Slight (less than 10 mins)&quot;</td><td>7894</td></tr><tr><td>&quot;No Delay&quot;</td><td>2078</td></tr><tr><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>2049</td></tr><tr><td>&quot;Severe (more than 30 mins)&quot;</td><td>47</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 2)\n",
       "\n",
       " expected_delay              count \n",
       " ---                         ---   \n",
       " str                         i64   \n",
       "\n",
       " Slight (less than 10 mins)  7894  \n",
       " No Delay                    2078  \n",
       " Moderate (10 - 30 mins)     2049  \n",
       " Severe (more than 30 mins)  47    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Distinct values for 'closure_type':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>closure_type</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Planned Works&quot;</td><td>11228</td></tr><tr><td>&quot;Emergency Works&quot;</td><td>840</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "\n",
       " closure_type     count \n",
       " ---              ---   \n",
       " str              i64   \n",
       "\n",
       " Planned Works    11228 \n",
       " Emergency Works  840   \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Distinct values for 'local_authority':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (50, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>local_authority</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Hampshire&quot;</td><td>705</td></tr><tr><td>&quot;Kent&quot;</td><td>635</td></tr><tr><td>&quot;Surrey&quot;</td><td>512</td></tr><tr><td>&quot;Essex&quot;</td><td>447</td></tr><tr><td>&quot;Warwickshire&quot;</td><td>417</td></tr><tr><td>&quot;Hertfordshire&quot;</td><td>347</td></tr><tr><td>&quot;Humberside&quot;</td><td>330</td></tr><tr><td>&quot;Oxfordshire&quot;</td><td>326</td></tr><tr><td>&quot;Cheshire&quot;</td><td>313</td></tr><tr><td>&quot;Avon&quot;</td><td>306</td></tr><tr><td>&quot;Staffordshire&quot;</td><td>292</td></tr><tr><td>&quot;Cambridgeshire&quot;</td><td>276</td></tr><tr><td>&quot;Wiltshire&quot;</td><td>257</td></tr><tr><td>&quot;Devon&quot;</td><td>245</td></tr><tr><td>&quot;Lancashire&quot;</td><td>228</td></tr><tr><td>&quot;Cumbria&quot;</td><td>223</td></tr><tr><td>&quot;Buckinghamshire&quot;</td><td>223</td></tr><tr><td>&quot;Northamptonshire&quot;</td><td>210</td></tr><tr><td>&quot;Gloucestershire&quot;</td><td>195</td></tr><tr><td>&quot;North Yorkshire&quot;</td><td>192</td></tr><tr><td>&quot;Doncaster&quot;</td><td>163</td></tr><tr><td>&quot;Bedfordshire&quot;</td><td>162</td></tr><tr><td>&quot;Worcestershire&quot;</td><td>152</td></tr><tr><td>&quot;Leicestershire&quot;</td><td>149</td></tr><tr><td>&quot;Cornwall&quot;</td><td>137</td></tr><tr><td>&quot;Shropshire&quot;</td><td>136</td></tr><tr><td>&quot;Leeds&quot;</td><td>128</td></tr><tr><td>&quot;East Sussex&quot;</td><td>121</td></tr><tr><td>&quot;West Berkshire&quot;</td><td>119</td></tr><tr><td>&quot;Suffolk&quot;</td><td>119</td></tr><tr><td>&quot;Derbyshire&quot;</td><td>118</td></tr><tr><td>&quot;Berkshire&quot;</td><td>112</td></tr><tr><td>&quot;S. Bucks&quot;</td><td>104</td></tr><tr><td>&quot;Somerset&quot;</td><td>103</td></tr><tr><td>&quot;Greater Manchester Motorways&quot;</td><td>99</td></tr><tr><td>&quot;Nottinghamshire&quot;</td><td>94</td></tr><tr><td>&quot;Dorset&quot;</td><td>91</td></tr><tr><td>&quot;West Sussex&quot;</td><td>91</td></tr><tr><td>&quot;Northumberland&quot;</td><td>81</td></tr><tr><td>&quot;Windsor and Maidenhead&quot;</td><td>76</td></tr><tr><td>&quot;Norfolk&quot;</td><td>66</td></tr><tr><td>&quot;West Yorkshire&quot;</td><td>66</td></tr><tr><td>&quot;Durham&quot;</td><td>62</td></tr><tr><td>&quot;City of Portsmouth&quot;</td><td>57</td></tr><tr><td>&quot;Kirklees&quot;</td><td>56</td></tr><tr><td>&quot;Cleveland&quot;</td><td>56</td></tr><tr><td>&quot;Barnet&quot;</td><td>53</td></tr><tr><td>&quot;Essex / Kent&quot;</td><td>52</td></tr><tr><td>&quot;Sandwell&quot;</td><td>49</td></tr><tr><td>&quot;Barnsley&quot;</td><td>47</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (50, 2)\n",
       "\n",
       " local_authority               count \n",
       " ---                           ---   \n",
       " str                           i64   \n",
       "\n",
       " Hampshire                     705   \n",
       " Kent                          635   \n",
       " Surrey                        512   \n",
       " Essex                         447   \n",
       " Warwickshire                  417   \n",
       " Hertfordshire                 347   \n",
       " Humberside                    330   \n",
       " Oxfordshire                   326   \n",
       " Cheshire                      313   \n",
       " Avon                          306   \n",
       " Staffordshire                 292   \n",
       " Cambridgeshire                276   \n",
       " Wiltshire                     257   \n",
       " Devon                         245   \n",
       " Lancashire                    228   \n",
       " Cumbria                       223   \n",
       " Buckinghamshire               223   \n",
       " Northamptonshire              210   \n",
       " Gloucestershire               195   \n",
       " North Yorkshire               192   \n",
       " Doncaster                     163   \n",
       " Bedfordshire                  162   \n",
       " Worcestershire                152   \n",
       " Leicestershire                149   \n",
       " Cornwall                      137   \n",
       " Shropshire                    136   \n",
       " Leeds                         128   \n",
       " East Sussex                   121   \n",
       " West Berkshire                119   \n",
       " Suffolk                       119   \n",
       " Derbyshire                    118   \n",
       " Berkshire                     112   \n",
       " S. Bucks                      104   \n",
       " Somerset                      103   \n",
       " Greater Manchester Motorways  99    \n",
       " Nottinghamshire               94    \n",
       " Dorset                        91    \n",
       " West Sussex                   91    \n",
       " Northumberland                81    \n",
       " Windsor and Maidenhead        76    \n",
       " Norfolk                       66    \n",
       " West Yorkshire                66    \n",
       " Durham                        62    \n",
       " City of Portsmouth            57    \n",
       " Kirklees                      56    \n",
       " Cleveland                     56    \n",
       " Barnet                        53    \n",
       " Essex / Kent                  52    \n",
       " Sandwell                      49    \n",
       " Barnsley                      47    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Distinct values for 'traffic_management':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (14, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>traffic_management</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Lane Closure&quot;</td><td>7093</td></tr><tr><td>&quot;Carriageway Closure&quot;</td><td>2662</td></tr><tr><td>&quot;Traffic Signals&quot;</td><td>549</td></tr><tr><td>&quot;Mobile Lane Closure&quot;</td><td>500</td></tr><tr><td>&quot;Lane Closure with Switching&quot;</td><td>327</td></tr><tr><td>&quot;None&quot;</td><td>290</td></tr><tr><td>&quot;Other&quot;</td><td>234</td></tr><tr><td>&quot;Width Restriction&quot;</td><td>130</td></tr><tr><td>&quot;Convoy Working&quot;</td><td>91</td></tr><tr><td>&quot;Contraflow&quot;</td><td>73</td></tr><tr><td>&quot;Speed Restriction&quot;</td><td>68</td></tr><tr><td>&quot;Stop/Go Boards&quot;</td><td>30</td></tr><tr><td>&quot;To Be Advised&quot;</td><td>18</td></tr><tr><td>&quot;Weight Restriction&quot;</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (14, 2)\n",
       "\n",
       " traffic_management           count \n",
       " ---                          ---   \n",
       " str                          i64   \n",
       "\n",
       " Lane Closure                 7093  \n",
       " Carriageway Closure          2662  \n",
       " Traffic Signals              549   \n",
       " Mobile Lane Closure          500   \n",
       " Lane Closure with Switching  327   \n",
       " None                         290   \n",
       " Other                        234   \n",
       " Width Restriction            130   \n",
       " Convoy Working               91    \n",
       " Contraflow                   73    \n",
       " Speed Restriction            68    \n",
       " Stop/Go Boards               30    \n",
       " To Be Advised                18    \n",
       " Weight Restriction           3     \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Check 3: Categorical Value Counts Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Check 3: Categorical Value Counts ---\n",
    "print(\"--- Running Check 3: Categorical Value Counts ---\")\n",
    "if 'con' not in globals() or con is None:\n",
    "    print(\"Error: Database connection 'con' is not established. Please run the setup cell first.\")\n",
    "elif 'TABLES_INFO' not in globals():\n",
    "    print(\"Error: TABLES_INFO is not defined. Please run the setup cell first.\")\n",
    "else:\n",
    "    for table_name, columns in TABLES_INFO.items():\n",
    "        print(f\"\\n--- Analyzing Table for Categorical Values: {table_name} ---\")\n",
    "        \n",
    "        count_df = run_query(con, f'SELECT COUNT(*) as total_rows FROM \"{table_name}\"')\n",
    "        total_rows = 0\n",
    "        if count_df is not None and not count_df.is_empty():\n",
    "            total_rows = count_df[0, \"total_rows\"]\n",
    "        # print(f\"Total Rows: {total_rows}\") # Optional context\n",
    "\n",
    "        if total_rows == 0:\n",
    "            print(\"Table is empty. Skipping categorical value checks for this table.\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\n3. Categorical Value Counts:\")\n",
    "        if table_name == RAW_NEW_TABLE_NAME:\n",
    "            categorical_cols = ['STATUS', 'EXPDEL', 'CLOSURE_TYPE']\n",
    "        elif table_name == RAW_OLD_TABLE_NAME:\n",
    "            categorical_cols = ['status', 'expected_delay', 'closure_type', 'local_authority', 'traffic_management']\n",
    "        else:\n",
    "            categorical_cols = []\n",
    "\n",
    "        if not categorical_cols:\n",
    "            print(\"  No categorical columns defined for this table.\")\n",
    "            continue\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            if col in columns:\n",
    "                print(f\"\\n  Distinct values for '{col}':\")\n",
    "                distinct_query = f'''\n",
    "                    SELECT \"{col}\", COUNT(*) as count\n",
    "                    FROM \"{table_name}\"\n",
    "                    GROUP BY \"{col}\"\n",
    "                    ORDER BY count DESC\n",
    "                    LIMIT 50\n",
    "                '''\n",
    "                distinct_df = run_query(con, distinct_query)\n",
    "                if distinct_df is not None and not distinct_df.is_empty():\n",
    "                    display(distinct_df)\n",
    "                elif distinct_df is not None and distinct_df.is_empty():\n",
    "                        print(f\"    No distinct values found for '{col}' (column might be all NULL).\")\n",
    "                else:\n",
    "                    print(f\"    Could not retrieve distinct values for '{col}'.\")\n",
    "            else:\n",
    "                    print(f\"  Configured categorical column '{col}' not found in table columns for {table_name}.\")\n",
    "    print(\"--- Check 3: Categorical Value Counts Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca3133",
   "metadata": {},
   "source": [
    "#### ID uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d593190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_id(con, table_name, id_col, id_value):\n",
    "    \"\"\"\n",
    "    Searches for a specific ID in the given table and returns the result as a Polars DataFrame.\n",
    "    \"\"\"\n",
    "    query = f'SELECT * FROM \"{table_name}\" WHERE \"{id_col}\" = {id_value}'\n",
    "    return run_query(con, query)\n",
    "\n",
    "\n",
    "# duplicate_examples_query = f'''\n",
    "#     SELECT *\n",
    "#     FROM \"{RAW_OLD_TABLE_NAME}\"\n",
    "#     WHERE \"reference_number\" = '1479020'\n",
    "# '''\n",
    "# details_df = run_query(con, duplicate_examples_query)\n",
    "# print(f\"\\n  Example of duplicate '1479020' value:\")\n",
    "# display(details_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0d330",
   "metadata": {},
   "source": [
    "##### Within tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c9cca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.1 Identifier Uniqueness Checks (within each table):\n",
      "\n",
      "  Checking for duplicate 'NEW_EVENT_NUMBER' in 'raw_new_roadworks':\n",
      "  Found 98 duplicate 'NEW_EVENT_NUMBER' values. Sample duplicates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>NEW_EVENT_NUMBER</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;00044016-002&quot;</td><td>4</td></tr><tr><td>&quot;00076857-001&quot;</td><td>4</td></tr><tr><td>&quot;00067347-003&quot;</td><td>3</td></tr><tr><td>&quot;00146456-002&quot;</td><td>3</td></tr><tr><td>&quot;00253822-003&quot;</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "\n",
       " NEW_EVENT_NUMBER  count \n",
       " ---               ---   \n",
       " str               i64   \n",
       "\n",
       " 00044016-002      4     \n",
       " 00076857-001      4     \n",
       " 00067347-003      3     \n",
       " 00146456-002      3     \n",
       " 00253822-003      3     \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Checking for duplicate 'OLD_REFERENCE_NUMBER' in 'raw_new_roadworks':\n",
      "  Found 78 duplicate 'OLD_REFERENCE_NUMBER' values. Sample duplicates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>OLD_REFERENCE_NUMBER</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;31779&quot;</td><td>14</td></tr><tr><td>&quot;14132&quot;</td><td>14</td></tr><tr><td>&quot;35658&quot;</td><td>9</td></tr><tr><td>&quot;48356&quot;</td><td>7</td></tr><tr><td>&quot;113713&quot;</td><td>7</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "\n",
       " OLD_REFERENCE_NUMBER  count \n",
       " ---                   ---   \n",
       " str                   i64   \n",
       "\n",
       " 31779                 14    \n",
       " 14132                 14    \n",
       " 35658                 9     \n",
       " 48356                 7     \n",
       " 113713                7     \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Checking for duplicate 'reference_number' in 'raw_old_roadworks':\n",
      "  Found 211 duplicate 'reference_number' values. Sample duplicates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reference_number</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;1479020&quot;</td><td>6</td></tr><tr><td>&quot;1512545&quot;</td><td>5</td></tr><tr><td>&quot;783303&quot;</td><td>4</td></tr><tr><td>&quot;213110&quot;</td><td>4</td></tr><tr><td>&quot;977371&quot;</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "\n",
       " reference_number  count \n",
       " ---               ---   \n",
       " str               i64   \n",
       "\n",
       " 1479020           6     \n",
       " 1512545           5     \n",
       " 783303            4     \n",
       " 213110            4     \n",
       " 977371            4     \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Check 4: Identifier Uniqueness and Table Overlap ---\n",
    "\n",
    "pl.Config.set_tbl_rows(10)\n",
    "\n",
    "print(\"\\n4.1 Identifier Uniqueness Checks (within each table):\")\n",
    "\n",
    "# Check New Format Identifier\n",
    "new_id_cols = ['NEW_EVENT_NUMBER','OLD_REFERENCE_NUMBER']\n",
    "for id_col in new_id_cols:\n",
    "    print(f\"\\n  Checking for duplicate '{id_col}' in '{RAW_NEW_TABLE_NAME}':\")\n",
    "    dupe_new_query = f'''\n",
    "        SELECT \"{id_col}\", COUNT(*) as count\n",
    "        FROM \"{RAW_NEW_TABLE_NAME}\"\n",
    "        WHERE \"{id_col}\" IS NOT NULL AND trim(\"{id_col}\") != ''\n",
    "        GROUP BY \"{id_col}\"\n",
    "        HAVING COUNT(*) > 1\n",
    "        ORDER BY count DESC\n",
    "    '''\n",
    "    dupe_new_df = run_query(con, dupe_new_query)\n",
    "    if dupe_new_df is not None and not dupe_new_df.is_empty():\n",
    "        print(f\"  Found {dupe_new_df.height} duplicate '{id_col}' values. Sample duplicates:\")\n",
    "        display(dupe_new_df.head(5))\n",
    "    elif dupe_new_df is not None and dupe_new_df.is_empty():\n",
    "        print(f\"  OK: '{id_col}' values are unique (excluding NULLs and empty strings).\")\n",
    "    else:\n",
    "        print(f\"  Could not perform duplicate check for '{id_col}'.\")\n",
    "\n",
    "# Check Old Format Identifier\n",
    "id_col = 'reference_number'\n",
    "print(f\"\\n  Checking for duplicate '{id_col}' in '{RAW_OLD_TABLE_NAME}':\")\n",
    "dupe_old_query = f'''\n",
    "    SELECT \"{id_col}\", COUNT(*) as count\n",
    "    FROM \"{RAW_OLD_TABLE_NAME}\"\n",
    "    WHERE \"{id_col}\" IS NOT NULL AND trim(\"{id_col}\") != ''\n",
    "    GROUP BY \"{id_col}\"\n",
    "    HAVING COUNT(*) > 1\n",
    "    ORDER BY count DESC\n",
    "'''\n",
    "dupe_old_df = run_query(con, dupe_old_query)\n",
    "if dupe_old_df is not None and not dupe_old_df.is_empty():\n",
    "    print(f\"  Found {dupe_old_df.height} duplicate '{id_col}' values. Sample duplicates:\")\n",
    "    display(dupe_old_df.head(5))\n",
    "elif dupe_old_df is not None and dupe_old_df.is_empty():\n",
    "    print(f\"  OK: '{id_col}' values are unique (excluding NULLs and empty strings).\")\n",
    "else:\n",
    "    print(f\"  Could not perform duplicate check for '{id_col}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748b245",
   "metadata": {},
   "source": [
    "##### Full-row duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41b2667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Check 4.3: Full Row Duplicates (within each table) ---\n",
      "\n",
      "--- Checking for Full Row Duplicates in Table: raw_new_roadworks ---\n",
      "  OK: No full row duplicates found in 'raw_new_roadworks'.\n",
      "\n",
      "--- Checking for Full Row Duplicates in Table: raw_old_roadworks ---\n",
      "  OK: No full row duplicates found in 'raw_old_roadworks'.\n"
     ]
    }
   ],
   "source": [
    "# --- Check 4.3: Full Row Duplicates (within each table) ---\n",
    "print(\"\\n--- Running Check 4.3: Full Row Duplicates (within each table) ---\")\n",
    "for table_name, columns in TABLES_INFO.items():\n",
    "    print(f\"\\n--- Checking for Full Row Duplicates in Table: {table_name} ---\")\n",
    "\n",
    "    if not columns:\n",
    "        print(\"  No columns defined for this table. Skipping full row duplicate check.\")\n",
    "        continue\n",
    "\n",
    "    # Construct the list of columns for the GROUP BY clause\n",
    "    # Ensure column names are quoted if they contain special characters or are keywords\n",
    "    group_by_columns_str = \", \".join([f'\"{col}\"' for col in columns])\n",
    "    select_columns_str = \", \".join([f'\"{col}\"' for col in columns])\n",
    "\n",
    "    # Query to find full row duplicates\n",
    "    # We select all columns and count occurrences, then filter for counts > 1\n",
    "    # To show the actual duplicate rows, we'd need a more complex query or join back.\n",
    "    # This query will show which combinations of values are duplicated and how many times.\n",
    "    duplicate_row_query = f'''\n",
    "        SELECT\n",
    "            {select_columns_str},\n",
    "            COUNT(*) as duplicate_count\n",
    "        FROM \"{table_name}\"\n",
    "        GROUP BY {group_by_columns_str}\n",
    "        HAVING COUNT(*) > 1\n",
    "        ORDER BY duplicate_count DESC\n",
    "    '''\n",
    "\n",
    "    duplicate_rows_df = run_query(con, duplicate_row_query)\n",
    "\n",
    "    if duplicate_rows_df is not None and not duplicate_rows_df.is_empty():\n",
    "        print(f\"  Found {duplicate_rows_df.height} sets of full row duplicates in '{table_name}'.\")\n",
    "        print(f\"  Showing up to 5 sets of duplicate rows (only the first instance of each set is shown below, with its count):\")\n",
    "        display(duplicate_rows_df.head(5))\n",
    "\n",
    "        # If you want to see all instances of a specific duplicated row, you would need another query.\n",
    "        # For example, to see all rows matching the first duplicated set:\n",
    "        if duplicate_rows_df.height > 0:\n",
    "            first_duplicate_set = duplicate_rows_df.row(0, named=True)\n",
    "            conditions = []\n",
    "            for col in columns:\n",
    "                value = first_duplicate_set[col]\n",
    "                if value is None:\n",
    "                    conditions.append(f'\"{col}\" IS NULL')\n",
    "                elif isinstance(value, (int, float)):\n",
    "                        conditions.append(f'\"{col}\" = {value}')\n",
    "                else: # string or other\n",
    "                    # Escape single quotes in string values for SQL\n",
    "                    escaped_value = str(value).replace(\"'\", \"''\")\n",
    "                    conditions.append(f'\"{col}\" = \\'{escaped_value}\\'')\n",
    "            \n",
    "            # Only attempt to show details if all columns were found in the first duplicate set\n",
    "            if len(conditions) == len(columns):\n",
    "                condition_str = \" AND \".join(conditions)\n",
    "                # print(f\"\\n  Example of all instances for one duplicated set (first one found):\")\n",
    "                # example_detail_query = f'SELECT * FROM \"{table_name}\" WHERE {condition_str}'\n",
    "                # example_detail_df = run_query(con, example_detail_query)\n",
    "                # if example_detail_df is not None:\n",
    "                #     display(example_detail_df)\n",
    "                # else:\n",
    "                #     print(f\"    Could not retrieve detailed example for the first duplicate set.\")\n",
    "            else:\n",
    "                print(\"    Could not construct detailed query for the first duplicate set due to missing column values in the sample.\")\n",
    "    else:\n",
    "        print(f\"  OK: No full row duplicates found in '{table_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb3ec9",
   "metadata": {},
   "source": [
    "##### Across tables\n",
    "\n",
    "###### Event nr. (new table) vs reference_number (old table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9c41ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2 Identifier Overlap Checks (between tables):\n",
      "Overlapping values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>overlapping_value</th><th>new_table_filename</th><th>old_table_filename</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 3)\n",
       "\n",
       " overlapping_value  new_table_filename  old_table_filename \n",
       " ---                ---                 ---                \n",
       " str                str                 str                \n",
       "\n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check overlap: NEW_EVENT_NUMBER (new) vs reference_number (old)\n",
    "\n",
    "print(\"4.2 Identifier Overlap Checks (between tables):\")\n",
    "\n",
    "new_event_col = 'NEW_EVENT_NUMBER'\n",
    "old_ref_col = 'reference_number'\n",
    "\n",
    "# Show overlapping values\n",
    "#if overlap_count_1 > 0:\n",
    "examples_query_1 = f'''\n",
    "    SELECT DISTINCT t1.\"{new_event_col}\" AS overlapping_value,  \n",
    "            t1.\"source_filename\" AS new_table_filename,\n",
    "            t2.\"source_filename\" AS old_table_filename\n",
    "    FROM \"{RAW_NEW_TABLE_NAME}\" t1\n",
    "    INNER JOIN \"{RAW_OLD_TABLE_NAME}\" t2 ON trim(t1.\"{new_event_col}\") = trim(t2.\"{old_ref_col}\")\n",
    "    WHERE t1.\"{new_event_col}\" IS NOT NULL AND trim(t1.\"{new_event_col}\") != ''\n",
    "        AND t2.\"{old_ref_col}\" IS NOT NULL AND trim(t2.\"{old_ref_col}\") != '';\n",
    "'''\n",
    "examples_df_1 = run_query(con, examples_query_1)\n",
    "print(\"Overlapping values:\")\n",
    "display(examples_df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1dff",
   "metadata": {},
   "source": [
    "###### Count Reference duplicates across tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61ad2c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 357 distinct reference IDs (new:'OLD_REFERENCE_NUMBER', old:'reference_number') appearing more than once in the combined dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>common_reference_id</th><th>total_combined_count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;31779&quot;</td><td>14</td></tr><tr><td>&quot;14132&quot;</td><td>14</td></tr><tr><td>&quot;35658&quot;</td><td>9</td></tr><tr><td>&quot;113713&quot;</td><td>7</td></tr><tr><td>&quot;48356&quot;</td><td>7</td></tr><tr><td>&quot;63555&quot;</td><td>7</td></tr><tr><td>&quot;1479020&quot;</td><td>6</td></tr><tr><td>&quot;1512545&quot;</td><td>5</td></tr><tr><td>&quot;19468&quot;</td><td>5</td></tr><tr><td>&quot;39752&quot;</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 2)\n",
       "\n",
       " common_reference_id  total_combined_count \n",
       " ---                  ---                  \n",
       " str                  i64                  \n",
       "\n",
       " 31779                14                   \n",
       " 14132                14                   \n",
       " 35658                9                    \n",
       " 113713               7                    \n",
       " 48356                7                    \n",
       " 63555                7                    \n",
       " 1479020              6                    \n",
       " 1512545              5                    \n",
       " 19468                5                    \n",
       " 39752                4                    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check combined total occurrences of reference IDs in both tables\n",
    "\n",
    "new_ref_col = 'OLD_REFERENCE_NUMBER'\n",
    "\n",
    "combined_total_occurrences_query = f'''\n",
    "WITH AllReferences AS (\n",
    "    -- Select relevant reference numbers from the new format table\n",
    "    SELECT trim(\"{new_ref_col}\") AS common_reference_id\n",
    "    FROM \"{RAW_NEW_TABLE_NAME}\"\n",
    "    WHERE \"{new_ref_col}\" IS NOT NULL AND trim(\"{new_ref_col}\") != ''\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Select relevant reference numbers from the old format table\n",
    "    SELECT trim(\"{old_ref_col}\") AS common_reference_id\n",
    "    FROM \"{RAW_OLD_TABLE_NAME}\"\n",
    "    WHERE \"{old_ref_col}\" IS NOT NULL AND trim(\"{old_ref_col}\") != ''\n",
    ")\n",
    "SELECT\n",
    "    common_reference_id,\n",
    "    COUNT(*) as total_combined_count\n",
    "FROM AllReferences\n",
    "WHERE common_reference_id IS NOT NULL AND trim(common_reference_id) != '' -- Final check on the combined IDs\n",
    "GROUP BY common_reference_id\n",
    "HAVING COUNT(*) > 1 -- Only show if the reference number appears more than once in the combined set\n",
    "ORDER BY total_combined_count DESC;\n",
    "'''\n",
    "\n",
    "combined_total_df = run_query(con, combined_total_occurrences_query)\n",
    "\n",
    "if not combined_total_df.is_empty():\n",
    "    print(f\"  Found {combined_total_df.height} distinct reference IDs (new:'{new_ref_col}', old:'{old_ref_col}') appearing more than once in the combined dataset.\")\n",
    "    display(combined_total_df.head(10))\n",
    "else:\n",
    "    print(f\"  No reference IDs (new:'{new_ref_col}', old:'{old_ref_col}') appear more than once in the combined dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a362b6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (82, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reference_id</th><th>new_format_filename</th><th>old_format_filename</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;2183499&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td></tr><tr><td>&quot;2294207&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td></tr><tr><td>&quot;2961781&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td></tr><tr><td>&quot;2961781&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;ha_roadworks_2015_03_16.xml&quot;</td></tr><tr><td>&quot;2961781&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;he_roadworks_2016_02_29.xml&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;4118637&quot;</td><td>&quot;he_roadworks_2020_05_25.xml&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td></tr><tr><td>&quot;4118637&quot;</td><td>&quot;he_roadworks_2019_04_15.xml&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td></tr><tr><td>&quot;4118637&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td></tr><tr><td>&quot;4131931&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td></tr><tr><td>&quot;4140886&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (82, 3)\n",
       "\n",
       " reference_id  new_format_filename          old_format_filename         \n",
       " ---           ---                          ---                         \n",
       " str           str                          str                         \n",
       "\n",
       " 2183499       he_roadworks_2018_02_26.xml  he_roadworks_2017_06_05.xml \n",
       " 2294207       he_roadworks_2018_02_26.xml  he_roadworks_2017_06_05.xml \n",
       " 2961781       he_roadworks_2018_02_26.xml  he_roadworks_2017_06_05.xml \n",
       " 2961781       he_roadworks_2018_02_26.xml  ha_roadworks_2015_03_16.xml \n",
       " 2961781       he_roadworks_2018_02_26.xml  he_roadworks_2016_02_29.xml \n",
       "                                                                     \n",
       " 4118637       he_roadworks_2020_05_25.xml  he_roadworks_2017_06_05.xml \n",
       " 4118637       he_roadworks_2019_04_15.xml  he_roadworks_2017_06_05.xml \n",
       " 4118637       he_roadworks_2018_02_26.xml  he_roadworks_2017_06_05.xml \n",
       " 4131931       he_roadworks_2018_02_26.xml  he_roadworks_2017_06_05.xml \n",
       " 4140886       he_roadworks_2018_02_26.xml  he_roadworks_2017_06_05.xml \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SQL query to find overlapping reference numbers and their associated filenames\n",
    "overlapping_references_query = f'''\n",
    "    SELECT \n",
    "        t1.\"OLD_REFERENCE_NUMBER\" AS reference_id,\n",
    "        t1.\"source_filename\" AS new_format_filename,\n",
    "        t2.\"source_filename\" AS old_format_filename\n",
    "    FROM \"{RAW_NEW_TABLE_NAME}\" t1\n",
    "    INNER JOIN \"{RAW_OLD_TABLE_NAME}\" t2\n",
    "        ON TRIM(t1.\"OLD_REFERENCE_NUMBER\") = TRIM(t2.\"reference_number\")\n",
    "    WHERE \n",
    "        t1.\"OLD_REFERENCE_NUMBER\" IS NOT NULL \n",
    "        AND TRIM(t1.\"OLD_REFERENCE_NUMBER\") != ''\n",
    "        AND t2.\"reference_number\" IS NOT NULL \n",
    "        AND TRIM(t2.\"reference_number\") != ''\n",
    "    ORDER BY reference_id;\n",
    "'''\n",
    "\n",
    "# Execute the query and display the results\n",
    "overlapping_references_df = run_query(con, overlapping_references_query)\n",
    "display(overlapping_references_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a22803",
   "metadata": {},
   "source": [
    "###### Inspect across-table duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0edd029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for ID '3295646' in NEW table, column 'OLD_REFERENCE_NUMBER':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reference_id</th><th>source_filename</th><th>count_in_file</th></tr><tr><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;3295646&quot;</td><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 3)\n",
       "\n",
       " reference_id  source_filename              count_in_file \n",
       " ---           ---                          ---           \n",
       " str           str                          i64           \n",
       "\n",
       " 3295646       he_roadworks_2018_02_26.xml  1             \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total occurrences of '3295646' in 'raw_new_roadworks': 1\n",
      "\n",
      "Checking for ID '3295646' in OLD table, column 'reference_number':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reference_id</th><th>source_filename</th><th>count_in_file</th></tr><tr><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;3295646&quot;</td><td>&quot;he_roadworks_2017_06_05.xml&quot;</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 3)\n",
       "\n",
       " reference_id  source_filename              count_in_file \n",
       " ---           ---                          ---           \n",
       " str           str                          i64           \n",
       "\n",
       " 3295646       he_roadworks_2017_06_05.xml  1             \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total occurrences of '3295646' in 'raw_old_roadworks': 1\n"
     ]
    }
   ],
   "source": [
    "find_id = '3295646'\n",
    "\n",
    "# Show in which XML files the ID appears, and how many times\n",
    "\n",
    "print(f\"Checking for ID '{find_id}' in NEW table, column '{new_ref_col}':\")\n",
    "query_debug_new = f'''\n",
    "    SELECT \"{new_ref_col}\" AS reference_id, \"source_filename\", COUNT(*) as count_in_file\n",
    "    FROM \"{RAW_NEW_TABLE_NAME}\"\n",
    "    WHERE trim(\"{new_ref_col}\") = '{find_id}'\n",
    "    GROUP BY \"{new_ref_col}\", \"source_filename\"\n",
    "    ORDER BY \"source_filename\";\n",
    "'''\n",
    "debug_new_df = run_query(con, query_debug_new)\n",
    "display(debug_new_df)\n",
    "total_new_occurrences = debug_new_df['count_in_file'].sum()\n",
    "print(f\"  Total occurrences of '{find_id}' in '{RAW_NEW_TABLE_NAME}': {total_new_occurrences}\")\n",
    "\n",
    "\n",
    "print(f\"\\nChecking for ID '{find_id}' in OLD table, column '{old_ref_col}':\")\n",
    "query_debug_old = f'''\n",
    "    SELECT \"{old_ref_col}\" AS reference_id, \"source_filename\", COUNT(*) as count_in_file\n",
    "    FROM \"{RAW_OLD_TABLE_NAME}\"\n",
    "    WHERE trim(\"{old_ref_col}\") = '{find_id}'\n",
    "    GROUP BY \"{old_ref_col}\", \"source_filename\"\n",
    "    ORDER BY \"source_filename\";\n",
    "'''\n",
    "debug_old_df = run_query(con, query_debug_old)\n",
    "display(debug_old_df)\n",
    "total_old_occurrences = debug_old_df['count_in_file'].sum()\n",
    "print(f\"  Total occurrences of '{find_id}' in '{RAW_OLD_TABLE_NAME}': {total_old_occurrences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f12a7718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for ref. '3295646' in both tables:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>NEW_EVENT_NUMBER</th><th>OLD_REFERENCE_NUMBER</th><th>SDATE</th><th>EDATE</th><th>EXPDEL</th><th>DESCRIPTION</th><th>CLOSURE_TYPE</th><th>STATUS</th><th>PUBLISHED_DATE</th><th>CENTRE_EASTING</th><th>CENTRE_NORTHING</th><th>ROAD_NUMBERS</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00001278-026&quot;</td><td>&quot;3295646&quot;</td><td>&quot;09-JAN-2017 11:48&quot;</td><td>&quot;06-MAY-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;M1 northbound and southbound J</td><td>&quot;Major Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-16T13:23:26&quot;</td><td>&quot;447946&quot;</td><td>&quot;322305&quot;</td><td>&quot;M1&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 13)\n",
       "\n",
       " source_fi  NEW_EVENT  OLD_REFER  SDATE        PUBLISHED  CENTRE_EA  CENTRE_NO  ROAD_NUM \n",
       " lename     _NUMBER    ENCE_NUMB  ---           _DATE      STING      RTHING     BERS     \n",
       " ---        ---        ER         str           ---        ---        ---        ---      \n",
       " str        str        ---                      str        str        str        str      \n",
       "                       str                                                                \n",
       "\n",
       " he_roadwo  00001278-  3295646    09-JAN-20    2018-02-1  447946     322305     M1       \n",
       " rks_2018_  026                   17 11:48      6T13:23:2                                 \n",
       " 02_26.xml                                      6                                         \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>reference_number</th><th>start_date</th><th>end_date</th><th>expected_delay</th><th>description</th><th>closure_type</th><th>status</th><th>published_date</th><th>centre_easting</th><th>centre_northing</th><th>road</th><th>location</th><th>local_authority</th><th>traffic_management</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;he_roadworks_2017_06_05.xml&quot;</td><td>&quot;3295646&quot;</td><td>&quot;2017-01-09T11:48:00&quot;</td><td>&quot;2018-05-06T06:00:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;24/7 Narrow lanes and 50 mph s</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2017-06-01T10:55:08&quot;</td><td>&quot;447781&quot;</td><td>&quot;322120&quot;</td><td>&quot;M1&quot;</td><td>&quot;Jct 23 to Jct 23a&quot;</td><td>&quot;Leicestershire&quot;</td><td>&quot;Carriageway Closure&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 15)\n",
       "\n",
       " source_fil  reference_  start_date  end_date      road  location   local_aut  traffic_m \n",
       " ename       number      ---         ---            ---   ---        hority     anagement \n",
       " ---         ---         str         str            str   str        ---        ---       \n",
       " str         str                                                     str        str       \n",
       "\n",
       " he_roadwor  3295646     2017-01-09  2018-05-06    M1    Jct 23 to  Leicester  Carriagew \n",
       " ks_2017_06              T11:48:00   T06:00:00            Jct 23a    shire      ay        \n",
       " _05.xml                                                                        Closure   \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for ref. '14132' in both tables:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (14, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>NEW_EVENT_NUMBER</th><th>OLD_REFERENCE_NUMBER</th><th>SDATE</th><th>EDATE</th><th>EXPDEL</th><th>DESCRIPTION</th><th>CLOSURE_TYPE</th><th>STATUS</th><th>PUBLISHED_DATE</th><th>CENTRE_EASTING</th><th>CENTRE_NORTHING</th><th>ROAD_NUMBERS</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259838-002&quot;</td><td>&quot;14132&quot;</td><td>&quot;19-MAR-2022 20:00&quot;</td><td>&quot;20-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;M4 eastbound Jct 16 - 15 mobil</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T09:17:50&quot;</td><td>&quot;415319&quot;</td><td>&quot;181771&quot;</td><td>&quot;M4&quot;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259889-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;22-MAR-2022 20:00&quot;</td><td>&quot;23-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A30 westbound Carminow, Bodmin</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T10:13:24&quot;</td><td>&quot;208931&quot;</td><td>&quot;65660&quot;</td><td>&quot;A30&quot;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259873-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;22-MAR-2022 20:00&quot;</td><td>&quot;23-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A30 westbound Innis Downs to I</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T10:02:35&quot;</td><td>&quot;198318&quot;</td><td>&quot;61508&quot;</td><td>&quot;A30&quot;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259829-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;16-MAR-2022 20:00&quot;</td><td>&quot;17-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;M5 southbound Jct 23 mobile la</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T07:32:53&quot;</td><td>&quot;331463&quot;</td><td>&quot;140689&quot;</td><td>&quot;M5&quot;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259864-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;21-MAR-2022 20:00&quot;</td><td>&quot;22-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A38 westbound Goodstone to Ash</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T09:22:32&quot;</td><td>&quot;275580&quot;</td><td>&quot;69326&quot;</td><td>&quot;A38&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259893-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;22-MAR-2022 19:00&quot;</td><td>&quot;23-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A38 Carminow to Turfdown, Bodm</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T11:33:48&quot;</td><td>&quot;209093&quot;</td><td>&quot;65613&quot;</td><td>&quot;A38&quot;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259837-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;18-MAR-2022 20:00&quot;</td><td>&quot;19-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A30 eastbound Stowford to Sour</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T08:00:17&quot;</td><td>&quot;248219&quot;</td><td>&quot;89993&quot;</td><td>&quot;A30&quot;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259871-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;21-MAR-2022 20:00&quot;</td><td>&quot;22-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A38 eastbound Lee Mill to Ivyb</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T09:30:04&quot;</td><td>&quot;259231&quot;</td><td>&quot;55537&quot;</td><td>&quot;A38&quot;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259867-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;21-MAR-2022 20:00&quot;</td><td>&quot;22-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A38 westbound Lower Dean to So</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T09:26:27&quot;</td><td>&quot;269040&quot;</td><td>&quot;58681&quot;</td><td>&quot;A38&quot;</td></tr><tr><td>&quot;nh_roadworks_2022_3_14.xml&quot;</td><td>&quot;00259835-001&quot;</td><td>&quot;14132&quot;</td><td>&quot;17-MAR-2022 20:00&quot;</td><td>&quot;18-MAR-2022 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A30 westbound Trebursye to Two</td><td>&quot;Ad-hoc Routine Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2022-03-09T07:55:51&quot;</td><td>&quot;230206&quot;</td><td>&quot;83701&quot;</td><td>&quot;A30&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (14, 13)\n",
       "\n",
       " source_fi  NEW_EVENT  OLD_REFER  SDATE        PUBLISHED  CENTRE_EA  CENTRE_NO  ROAD_NUM \n",
       " lename     _NUMBER    ENCE_NUMB  ---           _DATE      STING      RTHING     BERS     \n",
       " ---        ---        ER         str           ---        ---        ---        ---      \n",
       " str        str        ---                      str        str        str        str      \n",
       "                       str                                                                \n",
       "\n",
       " nh_roadwo  00259838-  14132      19-MAR-20    2022-03-0  415319     181771     M4       \n",
       " rks_2022_  002                   22 20:00      9T09:17:5                                 \n",
       " 3_14.xml                                       0                                         \n",
       " nh_roadwo  00259889-  14132      22-MAR-20    2022-03-0  208931     65660      A30      \n",
       " rks_2022_  001                   22 20:00      9T10:13:2                                 \n",
       " 3_14.xml                                       4                                         \n",
       " nh_roadwo  00259873-  14132      22-MAR-20    2022-03-0  198318     61508      A30      \n",
       " rks_2022_  001                   22 20:00      9T10:02:3                                 \n",
       " 3_14.xml                                       5                                         \n",
       " nh_roadwo  00259829-  14132      16-MAR-20    2022-03-0  331463     140689     M5       \n",
       " rks_2022_  001                   22 20:00      9T07:32:5                                 \n",
       " 3_14.xml                                       3                                         \n",
       " nh_roadwo  00259864-  14132      21-MAR-20    2022-03-0  275580     69326      A38      \n",
       " rks_2022_  001                   22 20:00      9T09:22:3                                 \n",
       " 3_14.xml                                       2                                         \n",
       "                                                                                 \n",
       " nh_roadwo  00259893-  14132      22-MAR-20    2022-03-0  209093     65613      A38      \n",
       " rks_2022_  001                   22 19:00      9T11:33:4                                 \n",
       " 3_14.xml                                       8                                         \n",
       " nh_roadwo  00259837-  14132      18-MAR-20    2022-03-0  248219     89993      A30      \n",
       " rks_2022_  001                   22 20:00      9T08:00:1                                 \n",
       " 3_14.xml                                       7                                         \n",
       " nh_roadwo  00259871-  14132      21-MAR-20    2022-03-0  259231     55537      A38      \n",
       " rks_2022_  001                   22 20:00      9T09:30:0                                 \n",
       " 3_14.xml                                       4                                         \n",
       " nh_roadwo  00259867-  14132      21-MAR-20    2022-03-0  269040     58681      A38      \n",
       " rks_2022_  001                   22 20:00      9T09:26:2                                 \n",
       " 3_14.xml                                       7                                         \n",
       " nh_roadwo  00259835-  14132      17-MAR-20    2022-03-0  230206     83701      A30      \n",
       " rks_2022_  001                   22 20:00      9T07:55:5                                 \n",
       " 3_14.xml                                       1                                         \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>reference_number</th><th>start_date</th><th>end_date</th><th>expected_delay</th><th>description</th><th>closure_type</th><th>status</th><th>published_date</th><th>centre_easting</th><th>centre_northing</th><th>road</th><th>location</th><th>local_authority</th><th>traffic_management</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 15)\n",
       "\n",
       " source_file  reference_  start_date  end_date    road  location  local_auth  traffic_ma \n",
       " name         number      ---         ---          ---   ---       ority       nagement   \n",
       " ---          ---         str         str          str   str       ---         ---        \n",
       " str          str                                                  str         str        \n",
       "\n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show all records where the ID matches\n",
    "find_id = '3295646'\n",
    "print(f\"Checking for ref. '{find_id}' in both tables:\")\n",
    "display(search_by_id(con, RAW_NEW_TABLE_NAME, 'OLD_REFERENCE_NUMBER', find_id))\n",
    "display(search_by_id(con, RAW_OLD_TABLE_NAME, 'reference_number', find_id))\n",
    "\n",
    "# Show all records where the ID matches\n",
    "find_id = '14132'\n",
    "print(f\"Checking for ref. '{find_id}' in both tables:\")\n",
    "display(search_by_id(con, RAW_NEW_TABLE_NAME, 'OLD_REFERENCE_NUMBER', find_id))\n",
    "display(search_by_id(con, RAW_OLD_TABLE_NAME, 'reference_number', find_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64226952",
   "metadata": {},
   "source": [
    "### Convert data types\n",
    "1. Numeric conversion (coordinates, reference number)\n",
    "1. Convert dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31d0e7",
   "metadata": {},
   "source": [
    "#### 1. Numeric conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "574c070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Adding Numeric Columns and Converting Data in roadworks_sample_data.duckdb ---\n",
      "\n",
      "--- Processing table: raw_new_roadworks ---\n",
      "  Adding column 'OLD_REFERENCE_NUMBER_NUMERIC' as BIGINT and attempting conversion from 'OLD_REFERENCE_NUMBER'.\n",
      "  Adding column 'CENTRE_EASTING_NUMERIC' as INTEGER and attempting conversion from 'CENTRE_EASTING'.\n",
      "  Adding column 'CENTRE_NORTHING_NUMERIC' as INTEGER and attempting conversion from 'CENTRE_NORTHING'.\n",
      "  Numeric columns added and data conversion attempted for raw_new_roadworks.\n",
      "\n",
      "--- Processing table: raw_old_roadworks ---\n",
      "  Adding column 'reference_number_numeric' as BIGINT and attempting conversion from 'reference_number'.\n",
      "  Adding column 'centre_easting_numeric' as INTEGER and attempting conversion from 'centre_easting'.\n",
      "  Adding column 'centre_northing_numeric' as INTEGER and attempting conversion from 'centre_northing'.\n",
      "  Numeric columns added and data conversion attempted for raw_old_roadworks.\n",
      "\n",
      "Changes committed to the database.\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Adding Numeric Columns and Converting Data in {DUCKDB_FILE} ---\")\n",
    "\n",
    "# Re-open the connection in write mode\n",
    "if con:\n",
    "        con.close()\n",
    "con = duckdb.connect(database=DUCKDB_FILE, read_only=False)\n",
    "\n",
    "# --- Process RAW_NEW_TABLE_NAME ---\n",
    "print(f\"\\n--- Processing table: {RAW_NEW_TABLE_NAME} ---\")\n",
    "\n",
    "# NEW_EVENT_NUMBER often contains non-numeric characters like '-' so it's excluded here.\n",
    "cols_to_convert_new = {\n",
    "    \"OLD_REFERENCE_NUMBER\": \"BIGINT\",\n",
    "    \"CENTRE_EASTING\": \"INTEGER\",\n",
    "    \"CENTRE_NORTHING\": \"INTEGER\"\n",
    "}\n",
    "\n",
    "for original_col, numeric_type in cols_to_convert_new.items():\n",
    "    new_col_name = f\"{original_col}_NUMERIC\"\n",
    "    print(f\"  Adding column '{new_col_name}' as {numeric_type} and attempting conversion from '{original_col}'.\")\n",
    "    \n",
    "    # Add the new column\n",
    "    alter_sql = f'ALTER TABLE \"{RAW_NEW_TABLE_NAME}\" ADD COLUMN \"{new_col_name}\" {numeric_type};'\n",
    "    con.execute(alter_sql)\n",
    "    \n",
    "    # Update the new column with converted values\n",
    "    update_sql = f'''\n",
    "    UPDATE \"{RAW_NEW_TABLE_NAME}\"\n",
    "    SET \"{new_col_name}\" = TRY_CAST(\"{original_col}\" AS {numeric_type});\n",
    "    '''\n",
    "    con.execute(update_sql)\n",
    "print(f\"  Numeric columns added and data conversion attempted for {RAW_NEW_TABLE_NAME}.\")\n",
    "\n",
    "# --- Process RAW_OLD_TABLE_NAME ---\n",
    "print(f\"\\n--- Processing table: {RAW_OLD_TABLE_NAME} ---\")\n",
    "\n",
    "# Columns to convert in the old table format\n",
    "# reference_number, centre_easting, centre_northing\n",
    "\n",
    "cols_to_convert_old = {\n",
    "    \"reference_number\": \"BIGINT\",\n",
    "    \"centre_easting\": \"INTEGER\",\n",
    "    \"centre_northing\": \"INTEGER\"\n",
    "}\n",
    "\n",
    "for original_col, numeric_type in cols_to_convert_old.items():\n",
    "    new_col_name = f\"{original_col}_numeric\" # Using lowercase to match original old column style\n",
    "    print(f\"  Adding column '{new_col_name}' as {numeric_type} and attempting conversion from '{original_col}'.\")\n",
    "    \n",
    "    # Add the new column\n",
    "    alter_sql = f'ALTER TABLE \"{RAW_OLD_TABLE_NAME}\" ADD COLUMN \"{new_col_name}\" {numeric_type};'\n",
    "    con.execute(alter_sql)\n",
    "    \n",
    "    # Update the new column with converted values\n",
    "    update_sql = f'''\n",
    "    UPDATE \"{RAW_OLD_TABLE_NAME}\"\n",
    "    SET \"{new_col_name}\" = TRY_CAST(\"{original_col}\" AS {numeric_type});\n",
    "    '''\n",
    "    con.execute(update_sql)\n",
    "print(f\"  Numeric columns added and data conversion attempted for {RAW_OLD_TABLE_NAME}.\")\n",
    "\n",
    "con.commit()\n",
    "print(\"\\nChanges committed to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02c43cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample data from 'raw_new_roadworks' with new numeric columns (first 5 rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>NEW_EVENT_NUMBER</th><th>OLD_REFERENCE_NUMBER</th><th>SDATE</th><th>EDATE</th><th>EXPDEL</th><th>DESCRIPTION</th><th>CLOSURE_TYPE</th><th>STATUS</th><th>PUBLISHED_DATE</th><th>CENTRE_EASTING</th><th>CENTRE_NORTHING</th><th>ROAD_NUMBERS</th><th>OLD_REFERENCE_NUMBER_NUMERIC</th><th>CENTRE_EASTING_NUMERIC</th><th>CENTRE_NORTHING_NUMERIC</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00026976-005&quot;</td><td>null</td><td>&quot;26-FEB-2018 21:00&quot;</td><td>&quot;28-FEB-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A3 northbound Sheet Link entry</td><td>&quot;Area Renewals&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-22T16:49:17&quot;</td><td>&quot;475209&quot;</td><td>&quot;124975&quot;</td><td>&quot;A3&quot;</td><td>null</td><td>475209</td><td>124975</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00004020-008&quot;</td><td>&quot;4188720&quot;</td><td>&quot;08-JAN-2018 20:00&quot;</td><td>&quot;10-MAR-2018 06:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;A14 Westbound\n",
       "Jct 58 to Jct 57</td><td>&quot;Area Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-22T10:13:27&quot;</td><td>&quot;614569&quot;</td><td>&quot;241115&quot;</td><td>&quot;A14&quot;</td><td>4188720</td><td>614569</td><td>241115</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00001459-026&quot;</td><td>&quot;4215713&quot;</td><td>&quot;31-JUL-2017 14:47&quot;</td><td>&quot;01-APR-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;M1 northbound and southbound T</td><td>&quot;Major Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-15T14:38:05&quot;</td><td>&quot;445124&quot;</td><td>&quot;364308&quot;</td><td>&quot;M1&quot;</td><td>4215713</td><td>445124</td><td>364308</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 16)\n",
       "\n",
       " source_fi  NEW_EVENT  OLD_REFER  SDATE        ROAD_NUMB  OLD_REFER  CENTRE_EA  CENTRE_N \n",
       " lename     _NUMBER    ENCE_NUMB  ---           ERS        ENCE_NUMB  STING_NUM  ORTHING_ \n",
       " ---        ---        ER         str           ---        ER_NUMERI  ERIC       NUMERIC  \n",
       " str        str        ---                      str        C          ---        ---      \n",
       "                       str                                 ---        i32        i32      \n",
       "                                                           i64                            \n",
       "\n",
       " he_roadwo  00026976-  null       26-FEB-20    A3         null       475209     124975   \n",
       " rks_2018_  005                   18 21:00                                                \n",
       " 02_26.xml                                                                                \n",
       " he_roadwo  00004020-  4188720    08-JAN-20    A14        4188720    614569     241115   \n",
       " rks_2018_  008                   18 20:00                                                \n",
       " 02_26.xml                                                                                \n",
       " he_roadwo  00001459-  4215713    31-JUL-20    M1         4215713    445124     364308   \n",
       " rks_2018_  026                   17 14:47                                                \n",
       " 02_26.xml                                                                                \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample data from 'raw_old_roadworks' with new numeric columns (first 5 rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 18)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>reference_number</th><th>start_date</th><th>end_date</th><th>expected_delay</th><th>description</th><th>closure_type</th><th>status</th><th>published_date</th><th>centre_easting</th><th>centre_northing</th><th>road</th><th>location</th><th>local_authority</th><th>traffic_management</th><th>reference_number_numeric</th><th>centre_easting_numeric</th><th>centre_northing_numeric</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;972963&quot;</td><td>&quot;2010-07-12T07:00:00&quot;</td><td>&quot;2013-03-23T06:00:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;Major junction works will incl</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2011-10-09T21:08:32&quot;</td><td>&quot;456252&quot;</td><td>&quot;278173&quot;</td><td>&quot;M1&quot;</td><td>&quot;Catthorpe&quot;</td><td>&quot;Leicestershire / Northamptonsh</td><td>&quot;Other&quot;</td><td>972963</td><td>456252</td><td>278173</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;978905&quot;</td><td>&quot;2011-04-01T22:00:00&quot;</td><td>&quot;2011-12-31T05:00:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;Contraflow with speed restrict</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2010-04-23T10:18:30&quot;</td><td>&quot;499082&quot;</td><td>&quot;235992&quot;</td><td>&quot;M1&quot;</td><td>&quot;Jct 13 to Jct 12&quot;</td><td>&quot;Bedfordshire / Buckinghamshire&quot;</td><td>&quot;Contraflow&quot;</td><td>978905</td><td>499082</td><td>235992</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;998294&quot;</td><td>&quot;2009-09-24T06:00:00&quot;</td><td>&quot;2013-09-24T05:00:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;Lane 1 closure and 24/7 Hardsh</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2010-06-19T05:03:50&quot;</td><td>&quot;465924&quot;</td><td>&quot;260154&quot;</td><td>&quot;M1&quot;</td><td>&quot;Approach to Junction 16 (21011</td><td>&quot;Northamptonshire&quot;</td><td>&quot;Lane Closure&quot;</td><td>998294</td><td>465924</td><td>260154</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 18)\n",
       "\n",
       " source_fi  reference  start_dat  end_date     traffic_m  reference  centre_ea  centre_n \n",
       " lename     _number    e          ---           anagement  _number_n  sting_num  orthing_ \n",
       " ---        ---        ---        str           ---        umeric     eric       numeric  \n",
       " str        str        str                      str        ---        ---        ---      \n",
       "                                                           i64        i32        i32      \n",
       "\n",
       " ha-roadwo  972963     2010-07-1  2013-03-2    Other      972963     456252     278173   \n",
       " rks_2011_             2T07:00:0  3T06:00:0                                               \n",
       " 10_10.xml             0          0                                                       \n",
       " ha-roadwo  978905     2011-04-0  2011-12-3    Contraflo  978905     499082     235992   \n",
       " rks_2011_             1T22:00:0  1T05:00:0     w                                         \n",
       " 10_10.xml             0          0                                                       \n",
       " ha-roadwo  998294     2009-09-2  2013-09-2    Lane       998294     465924     260154   \n",
       " rks_2011_             4T06:00:0  4T05:00:0     Closure                                   \n",
       " 10_10.xml             0          0                                                       \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Display sample data with new columns ---\n",
    "print(f\"\\n--- Sample data from '{RAW_NEW_TABLE_NAME}' with new numeric columns (first 5 rows) ---\")\n",
    "sample_new_df = con.execute(f'SELECT * FROM \"{RAW_NEW_TABLE_NAME}\" LIMIT 3').pl()\n",
    "display(sample_new_df)\n",
    "\n",
    "print(f\"\\n--- Sample data from '{RAW_OLD_TABLE_NAME}' with new numeric columns (first 5 rows) ---\")\n",
    "sample_old_df = con.execute(f'SELECT * FROM \"{RAW_OLD_TABLE_NAME}\" LIMIT 3').pl()\n",
    "display(sample_old_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5e8cb",
   "metadata": {},
   "source": [
    "##### Check for failed conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08d2f2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking failed conversions in: raw_new_roadworks ---\n",
      "No failed numeric conversions found (where original was non-empty and not NULL).\n",
      "\n",
      "--- Checking failed conversions in: raw_old_roadworks ---\n",
      "No failed numeric conversions found (where original was non-empty and not NULL).\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Checking failed conversions in: {RAW_NEW_TABLE_NAME} ---\")\n",
    "query_failed_new = f'''\n",
    "SELECT\n",
    "    \"source_filename\",\n",
    "    \"NEW_EVENT_NUMBER\",\n",
    "    \"OLD_REFERENCE_NUMBER\",\n",
    "    \"OLD_REFERENCE_NUMBER_NUMERIC\",\n",
    "    \"CENTRE_EASTING\",\n",
    "    \"CENTRE_EASTING_NUMERIC\",\n",
    "    \"CENTRE_NORTHING\",\n",
    "    \"CENTRE_NORTHING_NUMERIC\"\n",
    "FROM \"{RAW_NEW_TABLE_NAME}\"\n",
    "WHERE\n",
    "    (\"OLD_REFERENCE_NUMBER_NUMERIC\" IS NULL AND \"OLD_REFERENCE_NUMBER\" IS NOT NULL AND trim(\"OLD_REFERENCE_NUMBER\") != '') OR\n",
    "    (\"CENTRE_EASTING_NUMERIC\" IS NULL AND \"CENTRE_EASTING\" IS NOT NULL AND trim(\"CENTRE_EASTING\") != '') OR\n",
    "    (\"CENTRE_NORTHING_NUMERIC\" IS NULL AND \"CENTRE_NORTHING\" IS NOT NULL AND trim(\"CENTRE_NORTHING\") != '')\n",
    "LIMIT 20;\n",
    "'''\n",
    "failed_new_df = run_query(con, query_failed_new)\n",
    "if failed_new_df is not None and not failed_new_df.is_empty():\n",
    "    print(f\"Found {failed_new_df.height} potential failed conversions (showing up to 20):\")\n",
    "    display(failed_new_df)\n",
    "elif failed_new_df is not None:\n",
    "    print(\"No failed numeric conversions found (where original was non-empty and not NULL).\")\n",
    "else:\n",
    "    print(\"Could not execute check for failed conversions.\")\n",
    "\n",
    "# --- Check failed conversions for RAW_OLD_TABLE_NAME ---\n",
    "print(f\"\\n--- Checking failed conversions in: {RAW_OLD_TABLE_NAME} ---\")\n",
    "query_failed_old = f'''\n",
    "SELECT\n",
    "    \"source_filename\",\n",
    "    \"reference_number\",\n",
    "    \"reference_number_numeric\",\n",
    "    \"centre_easting\",\n",
    "    \"centre_easting_numeric\",\n",
    "    \"centre_northing\",\n",
    "    \"centre_northing_numeric\"\n",
    "FROM \"{RAW_OLD_TABLE_NAME}\"\n",
    "WHERE\n",
    "    (\"reference_number_numeric\" IS NULL AND \"reference_number\" IS NOT NULL AND trim(\"reference_number\") != '') OR\n",
    "    (\"centre_easting_numeric\" IS NULL AND \"centre_easting\" IS NOT NULL AND trim(\"centre_easting\") != '') OR\n",
    "    (\"centre_northing_numeric\" IS NULL AND \"centre_northing\" IS NOT NULL AND trim(\"centre_northing\") != '')\n",
    "LIMIT 20;\n",
    "'''\n",
    "failed_old_df = run_query(con, query_failed_old)\n",
    "if failed_old_df is not None and not failed_old_df.is_empty():\n",
    "    print(f\"Found {failed_old_df.height} potential failed conversions (showing up to 20):\")\n",
    "    display(failed_old_df)\n",
    "elif failed_old_df is not None:\n",
    "    print(\"No failed numeric conversions found (where original was non-empty and not NULL).\")\n",
    "else:\n",
    "    print(\"Could not execute check for failed conversions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b24bd",
   "metadata": {},
   "source": [
    "#### 2. Datetime conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af1bf671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing table: raw_new_roadworks for date/time conversion ---\n",
      "  Adding column 'SDATE_DT' as TIMESTAMP and attempting conversion from 'SDATE'.\n",
      "  Adding column 'EDATE_DT' as TIMESTAMP and attempting conversion from 'EDATE'.\n",
      "  Adding column 'PUBLISHED_DATE_DT' as TIMESTAMP and attempting conversion from 'PUBLISHED_DATE'.\n",
      "  Timestamp columns added and data conversion attempted for raw_new_roadworks.\n",
      "\n",
      "--- Processing table: raw_old_roadworks for date/time conversion ---\n",
      "  Adding column 'start_date_dt' as TIMESTAMP and attempting conversion from 'start_date'.\n",
      "  Adding column 'end_date_dt' as TIMESTAMP and attempting conversion from 'end_date'.\n",
      "  Adding column 'published_date_dt' as TIMESTAMP and attempting conversion from 'published_date'.\n",
      "  Timestamp columns added and data conversion attempted for raw_old_roadworks.\n",
      "\n",
      "Changes committed to the database.\n"
     ]
    }
   ],
   "source": [
    "# --- Process RAW_NEW_TABLE_NAME ---\n",
    "table_name_new = RAW_NEW_TABLE_NAME\n",
    "print(f\"\\n--- Processing table: {table_name_new} for date/time conversion ---\")\n",
    "\n",
    "# Columns to convert in the new table format and their original formats\n",
    "# SDATE: \"26-FEB-2018 21:00\" -> '%d-%b-%Y %H:%M'\n",
    "# EDATE: \"28-FEB-2018 06:00\" -> '%d-%b-%Y %H:%M'\n",
    "# PUBLISHED_DATE: \"2018-02-22T16:49:17\" -> ISO 8601\n",
    "cols_to_convert_new_dt = {\n",
    "    \"SDATE\": {\"new_col\": \"SDATE_DT\", \"format\": \"%d-%b-%Y %H:%M\"},\n",
    "    \"EDATE\": {\"new_col\": \"EDATE_DT\", \"format\": \"%d-%b-%Y %H:%M\"},\n",
    "    \"PUBLISHED_DATE\": {\"new_col\": \"PUBLISHED_DATE_DT\", \"format\": \"ISO\"} # ISO 8601\n",
    "}\n",
    "\n",
    "for original_col, details in cols_to_convert_new_dt.items():\n",
    "    new_col_name = details[\"new_col\"]\n",
    "    original_format = details[\"format\"]\n",
    "    print(f\"  Adding column '{new_col_name}' as TIMESTAMP and attempting conversion from '{original_col}'.\")\n",
    "\n",
    "    alter_sql = f'ALTER TABLE \"{table_name_new}\" ADD COLUMN \"{new_col_name}\" TIMESTAMP;'\n",
    "    con.execute(alter_sql)\n",
    "\n",
    "    if original_format == \"ISO\":\n",
    "        update_sql = f'''\n",
    "        UPDATE \"{table_name_new}\"\n",
    "        SET \"{new_col_name}\" = TRY_CAST(\"{original_col}\" AS TIMESTAMP);\n",
    "        '''\n",
    "    else:\n",
    "        update_sql = f'''\n",
    "        UPDATE \"{table_name_new}\"\n",
    "        SET \"{new_col_name}\" = TRY_STRPTIME(trim(\"{original_col}\"), '{original_format}');\n",
    "        '''\n",
    "    con.execute(update_sql)\n",
    "print(f\"  Timestamp columns added and data conversion attempted for {table_name_new}.\")\n",
    "\n",
    "# --- Process RAW_OLD_TABLE_NAME ---\n",
    "table_name_old = RAW_OLD_TABLE_NAME\n",
    "print(f\"\\n--- Processing table: {table_name_old} for date/time conversion ---\")\n",
    "\n",
    "# Columns to convert in the old table format (all appear to be ISO 8601 like \"2010-07-12T07:00:00\")\n",
    "# start_date, end_date, published_date\n",
    "cols_to_convert_old_dt = {\n",
    "    \"start_date\": {\"new_col\": \"start_date_dt\", \"format\": \"ISO\"},\n",
    "    \"end_date\": {\"new_col\": \"end_date_dt\", \"format\": \"ISO\"},\n",
    "    \"published_date\": {\"new_col\": \"published_date_dt\", \"format\": \"ISO\"}\n",
    "}\n",
    "\n",
    "for original_col, details in cols_to_convert_old_dt.items():\n",
    "    new_col_name = details[\"new_col\"] # Using lowercase to match original old column style\n",
    "    print(f\"  Adding column '{new_col_name}' as TIMESTAMP and attempting conversion from '{original_col}'.\")\n",
    "\n",
    "    alter_sql = f'ALTER TABLE \"{table_name_old}\" ADD COLUMN \"{new_col_name}\" TIMESTAMP;'\n",
    "    con.execute(alter_sql)\n",
    "\n",
    "    # All old format dates are ISO 8601 like, so TRY_CAST should work\n",
    "    update_sql = f'''\n",
    "    UPDATE \"{table_name_old}\"\n",
    "    SET \"{new_col_name}\" = TRY_CAST(\"{original_col}\" AS TIMESTAMP);\n",
    "    '''\n",
    "    con.execute(update_sql)\n",
    "print(f\"  Timestamp columns added and data conversion attempted for {table_name_old}.\")\n",
    "\n",
    "con.commit()\n",
    "print(\"\\nChanges committed to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfb7cf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample data from 'raw_new_roadworks' with new timestamp columns (first 5 rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>NEW_EVENT_NUMBER</th><th>SDATE</th><th>SDATE_DT</th><th>EDATE</th><th>EDATE_DT</th><th>PUBLISHED_DATE</th><th>PUBLISHED_DATE_DT</th></tr><tr><td>str</td><td>str</td><td>datetime[s]</td><td>str</td><td>datetime[s]</td><td>str</td><td>datetime[s]</td></tr></thead><tbody><tr><td>&quot;00026976-005&quot;</td><td>&quot;26-FEB-2018 21:00&quot;</td><td>2018-02-26 21:00:00</td><td>&quot;28-FEB-2018 06:00&quot;</td><td>2018-02-28 06:00:00</td><td>&quot;2018-02-22T16:49:17&quot;</td><td>2018-02-22 16:49:17</td></tr><tr><td>&quot;00004020-008&quot;</td><td>&quot;08-JAN-2018 20:00&quot;</td><td>2018-01-08 20:00:00</td><td>&quot;10-MAR-2018 06:00&quot;</td><td>2018-03-10 06:00:00</td><td>&quot;2018-02-22T10:13:27&quot;</td><td>2018-02-22 10:13:27</td></tr><tr><td>&quot;00001459-026&quot;</td><td>&quot;31-JUL-2017 14:47&quot;</td><td>2017-07-31 14:47:00</td><td>&quot;01-APR-2018 06:00&quot;</td><td>2018-04-01 06:00:00</td><td>&quot;2018-02-15T14:38:05&quot;</td><td>2018-02-15 14:38:05</td></tr><tr><td>&quot;00027883-003&quot;</td><td>&quot;12-FEB-2018 20:00&quot;</td><td>2018-02-12 20:00:00</td><td>&quot;17-MAR-2018 06:00&quot;</td><td>2018-03-17 06:00:00</td><td>&quot;2018-02-21T10:36:47&quot;</td><td>2018-02-21 10:36:47</td></tr><tr><td>&quot;00026799-002&quot;</td><td>&quot;10-FEB-2018 22:00&quot;</td><td>2018-02-10 22:00:00</td><td>&quot;22-MAR-2018 06:00&quot;</td><td>2018-03-22 06:00:00</td><td>&quot;2018-02-22T14:08:43&quot;</td><td>2018-02-22 14:08:43</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "\n",
       " NEW_EVENT_NU  SDATE        SDATE_DT     EDATE        EDATE_DT     PUBLISHED_D  PUBLISHED_D \n",
       " MBER          ---          ---          ---          ---          ATE          ATE_DT      \n",
       " ---           str          datetime[s  str          datetime[s  ---          ---         \n",
       " str                        ]                         ]            str          datetime[s \n",
       "                                                                                ]           \n",
       "\n",
       " 00026976-005  26-FEB-2018  2018-02-26   28-FEB-2018  2018-02-28   2018-02-22T  2018-02-22  \n",
       "               21:00        21:00:00     06:00        06:00:00     16:49:17     16:49:17    \n",
       " 00004020-008  08-JAN-2018  2018-01-08   10-MAR-2018  2018-03-10   2018-02-22T  2018-02-22  \n",
       "               20:00        20:00:00     06:00        06:00:00     10:13:27     10:13:27    \n",
       " 00001459-026  31-JUL-2017  2017-07-31   01-APR-2018  2018-04-01   2018-02-15T  2018-02-15  \n",
       "               14:47        14:47:00     06:00        06:00:00     14:38:05     14:38:05    \n",
       " 00027883-003  12-FEB-2018  2018-02-12   17-MAR-2018  2018-03-17   2018-02-21T  2018-02-21  \n",
       "               20:00        20:00:00     06:00        06:00:00     10:36:47     10:36:47    \n",
       " 00026799-002  10-FEB-2018  2018-02-10   22-MAR-2018  2018-03-22   2018-02-22T  2018-02-22  \n",
       "               22:00        22:00:00     06:00        06:00:00     14:08:43     14:08:43    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample data from 'raw_old_roadworks' with new timestamp columns (first 5 rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reference_number</th><th>start_date</th><th>start_date_dt</th><th>end_date</th><th>end_date_dt</th><th>published_date</th><th>published_date_dt</th></tr><tr><td>str</td><td>str</td><td>datetime[s]</td><td>str</td><td>datetime[s]</td><td>str</td><td>datetime[s]</td></tr></thead><tbody><tr><td>&quot;972963&quot;</td><td>&quot;2010-07-12T07:00:00&quot;</td><td>2010-07-12 07:00:00</td><td>&quot;2013-03-23T06:00:00&quot;</td><td>2013-03-23 06:00:00</td><td>&quot;2011-10-09T21:08:32&quot;</td><td>2011-10-09 21:08:32</td></tr><tr><td>&quot;978905&quot;</td><td>&quot;2011-04-01T22:00:00&quot;</td><td>2011-04-01 22:00:00</td><td>&quot;2011-12-31T05:00:00&quot;</td><td>2011-12-31 05:00:00</td><td>&quot;2010-04-23T10:18:30&quot;</td><td>2010-04-23 10:18:30</td></tr><tr><td>&quot;998294&quot;</td><td>&quot;2009-09-24T06:00:00&quot;</td><td>2009-09-24 06:00:00</td><td>&quot;2013-09-24T05:00:00&quot;</td><td>2013-09-24 05:00:00</td><td>&quot;2010-06-19T05:03:50&quot;</td><td>2010-06-19 05:03:50</td></tr><tr><td>&quot;1172899&quot;</td><td>&quot;2011-10-10T22:00:00&quot;</td><td>2011-10-10 22:00:00</td><td>&quot;2011-12-03T06:00:00&quot;</td><td>2011-12-03 06:00:00</td><td>&quot;2011-09-28T15:40:36&quot;</td><td>2011-09-28 15:40:36</td></tr><tr><td>&quot;1306529&quot;</td><td>&quot;2010-08-04T00:00:00&quot;</td><td>2010-08-04 00:00:00</td><td>&quot;2012-07-05T00:00:00&quot;</td><td>2012-07-05 00:00:00</td><td>&quot;2011-08-22T16:47:52&quot;</td><td>2011-08-22 16:47:52</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "\n",
       " reference_nu  start_date   start_date_  end_date     end_date_dt  published_d  published_d \n",
       " mber          ---          dt           ---          ---          ate          ate_dt      \n",
       " ---           str          ---          str          datetime[s  ---          ---         \n",
       " str                        datetime[s               ]            str          datetime[s \n",
       "                            ]                                                   ]           \n",
       "\n",
       " 972963        2010-07-12T  2010-07-12   2013-03-23T  2013-03-23   2011-10-09T  2011-10-09  \n",
       "               07:00:00     07:00:00     06:00:00     06:00:00     21:08:32     21:08:32    \n",
       " 978905        2011-04-01T  2011-04-01   2011-12-31T  2011-12-31   2010-04-23T  2010-04-23  \n",
       "               22:00:00     22:00:00     05:00:00     05:00:00     10:18:30     10:18:30    \n",
       " 998294        2009-09-24T  2009-09-24   2013-09-24T  2013-09-24   2010-06-19T  2010-06-19  \n",
       "               06:00:00     06:00:00     05:00:00     05:00:00     05:03:50     05:03:50    \n",
       " 1172899       2011-10-10T  2011-10-10   2011-12-03T  2011-12-03   2011-09-28T  2011-09-28  \n",
       "               22:00:00     22:00:00     06:00:00     06:00:00     15:40:36     15:40:36    \n",
       " 1306529       2010-08-04T  2010-08-04   2012-07-05T  2012-07-05   2011-08-22T  2011-08-22  \n",
       "               00:00:00     00:00:00     00:00:00     00:00:00     16:47:52     16:47:52    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Display sample data with new timestamp columns ---\n",
    "print(f\"\\n--- Sample data from '{table_name_new}' with new timestamp columns (first 5 rows) ---\")\n",
    "cols_to_select_new = ['NEW_EVENT_NUMBER', 'SDATE', 'SDATE_DT', 'EDATE', 'EDATE_DT', 'PUBLISHED_DATE', 'PUBLISHED_DATE_DT']\n",
    "selected_cols_str_new = \", \".join([f'\"{c}\"' for c in cols_to_select_new])\n",
    "sample_new_dt_df = con.execute(f'SELECT {selected_cols_str_new} FROM \"{table_name_new}\" LIMIT 5').pl()\n",
    "display(sample_new_dt_df)\n",
    "\n",
    "print(f\"\\n--- Sample data from '{table_name_old}' with new timestamp columns (first 5 rows) ---\")\n",
    "cols_to_select_old = ['reference_number', 'start_date', 'start_date_dt', 'end_date', 'end_date_dt', 'published_date', 'published_date_dt']\n",
    "selected_cols_str_old = \", \".join([f'\"{c}\"' for c in cols_to_select_old])\n",
    "sample_old_dt_df = con.execute(f'SELECT {selected_cols_str_old} FROM \"{table_name_old}\" LIMIT 5').pl()\n",
    "display(sample_old_dt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ddae4",
   "metadata": {},
   "source": [
    "##### Check for failed conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b794c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking failed date/time conversions in: raw_new_roadworks ---\n",
      "No failed date/time conversions found (where original was non-empty and not NULL).\n",
      "\n",
      "--- Checking failed date/time conversions in: raw_old_roadworks ---\n",
      "No failed date/time conversions found (where original was non-empty and not NULL).\n"
     ]
    }
   ],
   "source": [
    "table_name_new = RAW_NEW_TABLE_NAME\n",
    "print(f\"--- Checking failed date/time conversions in: {table_name_new} ---\")\n",
    "\n",
    "# Original and new timestamp columns for the 'new' table\n",
    "dt_cols_check_new = {\n",
    "    \"SDATE\": \"SDATE_DT\",\n",
    "    \"EDATE\": \"EDATE_DT\",\n",
    "    \"PUBLISHED_DATE\": \"PUBLISHED_DATE_DT\"\n",
    "}\n",
    "conditions_new = []\n",
    "select_cols_new = ['\"source_filename\"', '\"NEW_EVENT_NUMBER\"']\n",
    "for orig_col, new_dt_col in dt_cols_check_new.items():\n",
    "    select_cols_new.extend([f'\"{orig_col}\"', f'\"{new_dt_col}\"'])\n",
    "    conditions_new.append(f'(\"{new_dt_col}\" IS NULL AND \"{orig_col}\" IS NOT NULL AND trim(\"{orig_col}\") != \\'\\')')\n",
    "\n",
    "query_failed_dt_new = f'''\n",
    "SELECT\n",
    "    {', '.join(select_cols_new)}\n",
    "FROM \"{table_name_new}\"\n",
    "WHERE\n",
    "    {' OR '.join(conditions_new)}\n",
    "LIMIT 20;\n",
    "'''\n",
    "failed_dt_new_df = run_query(con, query_failed_dt_new)\n",
    "if failed_dt_new_df is not None and not failed_dt_new_df.is_empty():\n",
    "    print(f\"Found {failed_dt_new_df.height} potential failed date/time conversions (showing up to 20):\")\n",
    "    display(failed_dt_new_df)\n",
    "elif failed_dt_new_df is not None:\n",
    "    print(\"No failed date/time conversions found (where original was non-empty and not NULL).\")\n",
    "else:\n",
    "    print(\"Could not execute check for failed date/time conversions.\")\n",
    "\n",
    "# --- Check failed conversions for RAW_OLD_TABLE_NAME ---\n",
    "table_name_old = RAW_OLD_TABLE_NAME\n",
    "print(f\"\\n--- Checking failed date/time conversions in: {table_name_old} ---\")\n",
    "\n",
    "# Original and new timestamp columns for the 'old' table\n",
    "dt_cols_check_old = {\n",
    "    \"start_date\": \"start_date_dt\",\n",
    "    \"end_date\": \"end_date_dt\",\n",
    "    \"published_date\": \"published_date_dt\"\n",
    "}\n",
    "conditions_old = []\n",
    "select_cols_old = ['\"source_filename\"', '\"reference_number\"']\n",
    "for orig_col, new_dt_col in dt_cols_check_old.items():\n",
    "    select_cols_old.extend([f'\"{orig_col}\"', f'\"{new_dt_col}\"'])\n",
    "    conditions_old.append(f'(\"{new_dt_col}\" IS NULL AND \"{orig_col}\" IS NOT NULL AND trim(\"{orig_col}\") != \\'\\')')\n",
    "\n",
    "query_failed_dt_old = f'''\n",
    "SELECT\n",
    "    {', '.join(select_cols_old)}\n",
    "FROM \"{table_name_old}\"\n",
    "WHERE\n",
    "    {' OR '.join(conditions_old)}\n",
    "LIMIT 20;\n",
    "'''\n",
    "failed_dt_old_df = run_query(con, query_failed_dt_old)\n",
    "if failed_dt_old_df is not None and not failed_dt_old_df.is_empty():\n",
    "    print(f\"Found {failed_dt_old_df.height} potential failed date/time conversions (showing up to 20):\")\n",
    "    display(failed_dt_old_df)\n",
    "elif failed_dt_old_df is not None:\n",
    "    print(\"No failed date/time conversions found (where original was non-empty and not NULL).\")\n",
    "else:\n",
    "    print(\"Could not execute check for failed date/time conversions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee2d9d",
   "metadata": {},
   "source": [
    "### Coordinate conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ff1436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1: Transforming a known OSGB36 coordinate using 'EPSG:27700' ---\n",
      "Test 1 Result (EPSG:27700):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>test_lon</th><th>test_lat</th></tr><tr><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>-0.141588</td><td>51.501009</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 2)\n",
       "\n",
       " test_lon   test_lat  \n",
       " ---        ---       \n",
       " f64        f64       \n",
       "\n",
       " -0.141588  51.501009 \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Test 1 with 'EPSG:27700' was successful.\n",
      "\n",
      "--- Test 2: Transforming a known OSGB36 coordinate using '+nadgrids' method ---\n",
      "Test 2 Result (+nadgrids):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>test_lon</th><th>test_lat</th></tr><tr><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>inf</td><td>inf</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 2)\n",
       "\n",
       " test_lon  test_lat \n",
       " ---       ---      \n",
       " f64       f64      \n",
       "\n",
       " inf       inf      \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Test 2 with '+nadgrids' resulted in None or Infinity. Will fall back to 'EPSG:27700'.\n",
      "\n",
      "--- Proceeding with table updates. Using 'EPSG:27700' (less accurate, +nadgrids test pending/failed). ---\n",
      "\n",
      "--- Processing table: raw_new_roadworks ---\n",
      "  Coordinate transformation complete for raw_new_roadworks.\n",
      "\n",
      "--- Processing table: raw_old_roadworks ---\n",
      "  Coordinate transformation complete for raw_old_roadworks.\n",
      "\n",
      "Coordinate transformation changes committed to the database.\n",
      "\n",
      "--- Sample data from 'raw_new_roadworks' with WGS84 coordinates (first 5 valid) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>NEW_EVENT_NUMBER</th><th>CENTRE_EASTING_NUMERIC</th><th>CENTRE_NORTHING_NUMERIC</th><th>longitude_wgs84</th><th>latitude_wgs84</th></tr><tr><td>str</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;00027345-004&quot;</td><td>337590</td><td>155117</td><td>-2.896408</td><td>51.291749</td></tr><tr><td>&quot;00001327-015&quot;</td><td>455025</td><td>283940</td><td>-1.191766</td><td>52.450655</td></tr><tr><td>&quot;00027254-003&quot;</td><td>447294</td><td>172888</td><td>-1.320762</td><td>51.453007</td></tr><tr><td>&quot;00006120-003&quot;</td><td>573003</td><td>126110</td><td>0.464652</td><td>51.008358</td></tr><tr><td>&quot;00000400-032&quot;</td><td>453103</td><td>422243</td><td>-1.19726</td><td>53.69394</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "\n",
       " NEW_EVENT_NUMBER  CENTRE_EASTING_NUMER  CENTRE_NORTHING_NUM  longitude_wgs84  latitude_wgs84 \n",
       " ---               IC                    ERIC                 ---              ---            \n",
       " str               ---                   ---                  f64              f64            \n",
       "                   i32                   i32                                                  \n",
       "\n",
       " 00027345-004      337590                155117               -2.896408        51.291749      \n",
       " 00001327-015      455025                283940               -1.191766        52.450655      \n",
       " 00027254-003      447294                172888               -1.320762        51.453007      \n",
       " 00006120-003      573003                126110               0.464652         51.008358      \n",
       " 00000400-032      453103                422243               -1.19726         53.69394       \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample data from 'raw_old_roadworks' with WGS84 coordinates (first 5 valid) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reference_number</th><th>centre_easting_numeric</th><th>centre_northing_numeric</th><th>longitude_wgs84</th><th>latitude_wgs84</th></tr><tr><td>str</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1683376&quot;</td><td>441977</td><td>389133</td><td>-1.370176</td><td>53.397364</td></tr><tr><td>&quot;1690626&quot;</td><td>443281</td><td>389200</td><td>-1.350558</td><td>53.397861</td></tr><tr><td>&quot;1690628&quot;</td><td>443412</td><td>389335</td><td>-1.348569</td><td>53.399064</td></tr><tr><td>&quot;1705796&quot;</td><td>447205</td><td>335640</td><td>-1.299428</td><td>52.916116</td></tr><tr><td>&quot;1705838&quot;</td><td>445196</td><td>356606</td><td>-1.326376</td><td>53.104741</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "\n",
       " reference_number  centre_easting_numer  centre_northing_num  longitude_wgs84  latitude_wgs84 \n",
       " ---               ic                    eric                 ---              ---            \n",
       " str               ---                   ---                  f64              f64            \n",
       "                   i32                   i32                                                  \n",
       "\n",
       " 1683376           441977                389133               -1.370176        53.397364      \n",
       " 1690626           443281                389200               -1.350558        53.397861      \n",
       " 1690628           443412                389335               -1.348569        53.399064      \n",
       " 1705796           447205                335640               -1.299428        52.916116      \n",
       " 1705838           445196                356606               -1.326376        53.104741      \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking for 'inf' values after transformation ---\n",
      "Number of rows with 'inf' in WGS84 coordinates in 'raw_new_roadworks': 0\n",
      "Number of rows with 'inf' in WGS84 coordinates in 'raw_old_roadworks': 0\n"
     ]
    }
   ],
   "source": [
    "con.close()\n",
    "con = duckdb.connect(database=DUCKDB_FILE, read_only=False)\n",
    "\n",
    "# Define the GSB file for precise OSGB36 to ETRS89 coordinate transformation\n",
    "# Source: https://www.ordnancesurvey.co.uk/geodesy-positioning/coordinate-transformations/resources\n",
    "gsb_file_path = 'OSTN15_NTv2_OSGBtoETRS.gsb'\n",
    "\n",
    "# Define source CRS strings\n",
    "source_crs_epsg27700 = 'EPSG:27700' # OSGB36 British National Grid (less accurate, but good for testing)\n",
    "source_crs_nadgrids = f'+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +nadgrids={gsb_file_path} +type=crs'\n",
    "target_crs_epsg = 'EPSG:4326' # WGS84\n",
    "\n",
    "# Determine which source CRS to use\n",
    "chosen_source_crs = None\n",
    "crs_method_message = \"\"\n",
    "\n",
    "try:\n",
    "    con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "\n",
    "    # --- Test 1: Using EPSG:27700 (simpler, more likely to work if +nadgrids is the issue) ---\n",
    "    print(f\"\\n--- Test 1: Transforming a known OSGB36 coordinate using '{source_crs_epsg27700}' ---\")\n",
    "    test_easting = 529090  # Buckingham Palace\n",
    "    test_northing = 179645\n",
    "    \n",
    "    test_query_simple = f\"\"\"\n",
    "    SELECT\n",
    "        ST_X(ST_Transform(ST_Point({test_easting}, {test_northing}), '{source_crs_epsg27700}', '{target_crs_epsg}', always_xy := true)) AS test_lon,\n",
    "        ST_Y(ST_Transform(ST_Point({test_easting}, {test_northing}), '{source_crs_epsg27700}', '{target_crs_epsg}', always_xy := true)) AS test_lat;\n",
    "    \"\"\"\n",
    "    test_result_simple_df = run_query(con, test_query_simple)\n",
    "\n",
    "    simple_test_successful = False\n",
    "    if test_result_simple_df is not None and not test_result_simple_df.is_empty():\n",
    "        test_lon_simple = test_result_simple_df[0, \"test_lon\"]\n",
    "        test_lat_simple = test_result_simple_df[0, \"test_lat\"]\n",
    "        print(\"Test 1 Result (EPSG:27700):\")\n",
    "        display(test_result_simple_df)\n",
    "        if not (test_lon_simple is None or abs(test_lon_simple) == float('inf') or abs(test_lat_simple) == float('inf')):\n",
    "            simple_test_successful = True\n",
    "            chosen_source_crs = source_crs_epsg27700\n",
    "            crs_method_message = f\"Using '{source_crs_epsg27700}' (less accurate, +nadgrids test pending/failed).\"\n",
    "            print(f\"SUCCESS: Test 1 with '{source_crs_epsg27700}' was successful.\")\n",
    "        else:\n",
    "            print(f\"FAILURE: Test 1 with '{source_crs_epsg27700}' resulted in None or Infinity.\")\n",
    "    else:\n",
    "        print(f\"FAILURE: Test 1 query with '{source_crs_epsg27700}' failed or returned empty.\")\n",
    "\n",
    "    # --- Test 2: Using +nadgrids (more accurate, only if GSB file exists and simple test was okay) ---\n",
    "    if simple_test_successful and os.path.exists(gsb_file_path):\n",
    "        print(f\"\\n--- Test 2: Transforming a known OSGB36 coordinate using '+nadgrids' method ---\")\n",
    "        test_query_nadgrids = f\"\"\"\n",
    "        SELECT\n",
    "            ST_X(ST_Transform(ST_Point({test_easting}, {test_northing}), '{source_crs_nadgrids}', '{target_crs_epsg}', always_xy := true)) AS test_lon,\n",
    "            ST_Y(ST_Transform(ST_Point({test_easting}, {test_northing}), '{source_crs_nadgrids}', '{target_crs_epsg}', always_xy := true)) AS test_lat;\n",
    "        \"\"\"\n",
    "        test_result_nadgrids_df = run_query(con, test_query_nadgrids)\n",
    "        \n",
    "        if test_result_nadgrids_df is not None and not test_result_nadgrids_df.is_empty():\n",
    "            test_lon_nadgrids = test_result_nadgrids_df[0, \"test_lon\"]\n",
    "            test_lat_nadgrids = test_result_nadgrids_df[0, \"test_lat\"]\n",
    "            print(\"Test 2 Result (+nadgrids):\")\n",
    "            display(test_result_nadgrids_df)\n",
    "            if not (test_lon_nadgrids is None or abs(test_lon_nadgrids) == float('inf') or abs(test_lat_nadgrids) == float('inf')):\n",
    "                chosen_source_crs = source_crs_nadgrids\n",
    "                crs_method_message = f\"Using '{source_crs_nadgrids}' (more accurate, with GSB file).\"\n",
    "                print(f\"SUCCESS: Test 2 with '+nadgrids' was successful.\")\n",
    "            else:\n",
    "                print(f\"WARNING: Test 2 with '+nadgrids' resulted in None or Infinity. Will fall back to '{source_crs_epsg27700}'.\")\n",
    "        else:\n",
    "            print(f\"WARNING: Test 2 query with '+nadgrids' failed or returned empty. Will fall back to '{source_crs_epsg27700}'.\")\n",
    "    elif not os.path.exists(gsb_file_path):\n",
    "        print(\"\\nINFO: GSB file not found, skipping Test 2 (+nadgrids method).\")\n",
    "\n",
    "\n",
    "    # --- Proceed with table updates if any CRS method was successful ---\n",
    "    if chosen_source_crs:\n",
    "        print(f\"\\n--- Proceeding with table updates. {crs_method_message} ---\")\n",
    "        \n",
    "        tables_to_transform = {\n",
    "            RAW_NEW_TABLE_NAME: {\"easting_col\": \"CENTRE_EASTING_NUMERIC\", \"northing_col\": \"CENTRE_NORTHING_NUMERIC\"},\n",
    "            RAW_OLD_TABLE_NAME: {\"easting_col\": \"centre_easting_numeric\", \"northing_col\": \"centre_northing_numeric\"}\n",
    "        }\n",
    "\n",
    "        for table_name, cols in tables_to_transform.items():\n",
    "            easting_col = cols[\"easting_col\"]\n",
    "            northing_col = cols[\"northing_col\"]\n",
    "            print(f\"\\n--- Processing table: {table_name} ---\")\n",
    "\n",
    "            table_schema = run_query(con, f\"PRAGMA table_info('{table_name}');\")\n",
    "            existing_columns = [row['name'] for row in table_schema.iter_rows(named=True)] if table_schema is not None else []\n",
    "\n",
    "            if 'longitude_wgs84' not in existing_columns:\n",
    "                con.execute(f'ALTER TABLE \"{table_name}\" ADD COLUMN longitude_wgs84 DOUBLE;')\n",
    "            if 'latitude_wgs84' not in existing_columns:\n",
    "                con.execute(f'ALTER TABLE \"{table_name}\" ADD COLUMN latitude_wgs84 DOUBLE;')\n",
    "            # if 'geom_wgs84' not in existing_columns:\n",
    "            #     con.execute(f'ALTER TABLE \"{table_name}\" ADD COLUMN geom_wgs84 GEOMETRY;')\n",
    "            \n",
    "            con.execute(f'UPDATE \"{table_name}\" SET longitude_wgs84 = NULL, latitude_wgs84 = NULL;') # Clear existing values\n",
    "            \n",
    "            # If geom_wgs84 is needed, add the following to the SET-clause below:\n",
    "            # geom_wgs84 = ST_Transform(ST_Point(\"{easting_col}\", \"{northing_col}\"), '{chosen_source_crs}', '{target_crs_epsg}', always_xy := true)\n",
    "            update_sql = f'''\n",
    "            UPDATE \"{table_name}\"\n",
    "            SET\n",
    "                longitude_wgs84 = ST_X(ST_Transform(ST_Point(\"{easting_col}\", \"{northing_col}\"), '{chosen_source_crs}', '{target_crs_epsg}', always_xy := true)),\n",
    "                latitude_wgs84 = ST_Y(ST_Transform(ST_Point(\"{easting_col}\", \"{northing_col}\"), '{chosen_source_crs}', '{target_crs_epsg}', always_xy := true))\n",
    "            WHERE \"{easting_col}\" IS NOT NULL AND \"{northing_col}\" IS NOT NULL AND \"{easting_col}\" != 0 AND \"{northing_col}\" != 0;\n",
    "            '''\n",
    "            \n",
    "            con.execute(update_sql)\n",
    "            print(f\"  Coordinate transformation complete for {table_name}.\")\n",
    "        con.commit()\n",
    "        print(\"\\nCoordinate transformation changes committed to the database.\")\n",
    "    else:\n",
    "        print(\"\\n--- ERROR: Both transformation tests failed. Aborting table updates. ---\")\n",
    "        print(\"Please check DuckDB spatial extension, PROJ library compatibility, or input data for extreme/invalid values.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during coordinate transformation: {e}\")\n",
    "    if con: con.rollback()\n",
    "\n",
    "# --- Display sample data and checks ---\n",
    "if chosen_source_crs: # Only show samples if transformation was attempted\n",
    "    print(f\"\\n--- Sample data from '{RAW_NEW_TABLE_NAME}' with WGS84 coordinates (first 5 valid) ---\")\n",
    "    # ... (rest of the sample display and inf check code from previous response) ...\n",
    "    cols_to_select_new = ['NEW_EVENT_NUMBER', 'CENTRE_EASTING_NUMERIC', 'CENTRE_NORTHING_NUMERIC', 'longitude_wgs84', 'latitude_wgs84']\n",
    "    selected_cols_str_new = \", \".join([f'\"{c}\"' for c in cols_to_select_new])\n",
    "    sample_new_wgs84_df = run_query(con, f'SELECT {selected_cols_str_new} FROM \"{RAW_NEW_TABLE_NAME}\" WHERE longitude_wgs84 IS NOT NULL AND longitude_wgs84 != \\'inf\\' AND longitude_wgs84 != \\'-inf\\' LIMIT 5 OFFSET 10')\n",
    "    if sample_new_wgs84_df is not None: display(sample_new_wgs84_df)\n",
    "    else: print(\"Could not retrieve sample data.\")\n",
    "\n",
    "    print(f\"\\n--- Sample data from '{RAW_OLD_TABLE_NAME}' with WGS84 coordinates (first 5 valid) ---\")\n",
    "    cols_to_select_old = ['reference_number', 'centre_easting_numeric', 'centre_northing_numeric', 'longitude_wgs84', 'latitude_wgs84']\n",
    "    selected_cols_str_old = \", \".join([f'\"{c}\"' for c in cols_to_select_old])\n",
    "    sample_old_wgs84_df = run_query(con, f'SELECT {selected_cols_str_old} FROM \"{RAW_OLD_TABLE_NAME}\" WHERE longitude_wgs84 IS NOT NULL AND longitude_wgs84 != \\'inf\\' AND longitude_wgs84 != \\'-inf\\' LIMIT 5 OFFSET 10')\n",
    "    if sample_old_wgs84_df is not None: display(sample_old_wgs84_df)\n",
    "    else: print(\"Could not retrieve sample data.\")\n",
    "\n",
    "    print(\"\\n--- Checking for 'inf' values after transformation ---\")\n",
    "    for table_name, _ in tables_to_transform.items():\n",
    "        inf_check_query = f\"SELECT COUNT(*) as inf_count FROM \\\"{table_name}\\\" WHERE longitude_wgs84 = 'inf' OR longitude_wgs84 = '-inf' OR latitude_wgs84 = 'inf' OR latitude_wgs84 = '-inf';\"\n",
    "        inf_df = run_query(con, inf_check_query)\n",
    "        if inf_df is not None and not inf_df.is_empty(): print(f\"Number of rows with 'inf' in WGS84 coordinates in '{table_name}': {inf_df[0, 'inf_count']}\")\n",
    "        else: print(f\"Could not check for 'inf' values in '{table_name}'.\")\n",
    "else:\n",
    "    print(\"\\n--- Skipping sample display and 'inf' checks as transformations were aborted. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9609d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample data from 'raw_new_roadworks' with WGS84 coordinates (first 5 rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>NEW_EVENT_NUMBER</th><th>DESCRIPTION</th><th>ROAD_NUMBERS</th><th>CENTRE_EASTING_NUMERIC</th><th>CENTRE_NORTHING_NUMERIC</th><th>longitude_wgs84</th><th>latitude_wgs84</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;00026976-005&quot;</td><td>&quot;A3 northbound Sheet Link entry</td><td>&quot;A3&quot;</td><td>475209</td><td>124975</td><td>-0.929132</td><td>51.019241</td></tr><tr><td>&quot;00004020-008&quot;</td><td>&quot;A14 Westbound\n",
       "Jct 58 to Jct 57</td><td>&quot;A14&quot;</td><td>614569</td><td>241115</td><td>1.126274</td><td>52.026925</td></tr><tr><td>&quot;00001459-026&quot;</td><td>&quot;M1 northbound and southbound T</td><td>&quot;M1&quot;</td><td>445124</td><td>364308</td><td>-1.32637</td><td>53.173976</td></tr><tr><td>&quot;00027883-003&quot;</td><td>&quot;A259, east and westbound betwe</td><td>&quot;A259&quot;</td><td>596442</td><td>123787</td><td>0.797101</td><td>50.979974</td></tr><tr><td>&quot;00026799-002&quot;</td><td>&quot;A3 northbound Compton to Denni</td><td>&quot;A3&quot;</td><td>498261</td><td>150727</td><td>-0.593562</td><td>51.247262</td></tr><tr><td>&quot;00004943-003&quot;</td><td>&quot;M50 from Jct 3 to 2 Eastbound </td><td>&quot;M50&quot;</td><td>373518</td><td>232863</td><td>-2.387095</td><td>51.993578</td></tr><tr><td>&quot;00024589-002&quot;</td><td>&quot;A120 Diversion Route for local</td><td>&quot;A120&quot;</td><td>583029</td><td>222574</td><td>0.657245</td><td>51.871704</td></tr><tr><td>&quot;00027233-002&quot;</td><td>&quot;M1 northbound Jct 39 lane clos</td><td>&quot;M1&quot;</td><td>430596</td><td>415827</td><td>-1.53873</td><td>53.638072</td></tr><tr><td>&quot;00001313-011&quot;</td><td>&quot;M1 northbound and southbound J</td><td>&quot;M1&quot;</td><td>472224</td><td>257362</td><td>-0.944434</td><td>52.209756</td></tr><tr><td>&quot;00028472-002&quot;</td><td>&quot;A14 westbound Jct 8 to 7\r\n",
       "Lane</td><td>&quot;A14&quot;</td><td>485666</td><td>277318</td><td>-0.742691</td><td>52.387199</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 7)\n",
       "\n",
       " NEW_EVENT_NU  DESCRIPTION  ROAD_NUMBER  CENTRE_EAST  CENTRE_NORT  longitude_w  latitude_wg \n",
       " MBER          ---          S            ING_NUMERIC  HING_NUMERI  gs84         s84         \n",
       " ---           str          ---          ---          C            ---          ---         \n",
       " str                        str          i32          ---          f64          f64         \n",
       "                                                      i32                                   \n",
       "\n",
       " 00026976-005  A3           A3           475209       124975       -0.929132    51.019241   \n",
       "               northbound                                                                   \n",
       "               Sheet Link                                                                   \n",
       "               entry                                                                       \n",
       " 00004020-008  A14          A14          614569       241115       1.126274     52.026925   \n",
       "               Westbound                                                                    \n",
       "               Jct 58 to                                                                    \n",
       "               Jct 57                                                                      \n",
       " 00001459-026  M1           M1           445124       364308       -1.32637     53.173976   \n",
       "               northbound                                                                   \n",
       "               and                                                                          \n",
       "               southbound                                                                   \n",
       "               T                                                                           \n",
       " 00027883-003  A259, east   A259         596442       123787       0.797101     50.979974   \n",
       "               and                                                                          \n",
       "               westbound                                                                    \n",
       "               betwe                                                                       \n",
       " 00026799-002  A3           A3           498261       150727       -0.593562    51.247262   \n",
       "               northbound                                                                   \n",
       "               Compton to                                                                   \n",
       "               Denni                                                                       \n",
       " 00004943-003  M50 from     M50          373518       232863       -2.387095    51.993578   \n",
       "               Jct 3 to 2                                                                   \n",
       "               Eastbound                                                                   \n",
       " 00024589-002  A120         A120         583029       222574       0.657245     51.871704   \n",
       "               Diversion                                                                    \n",
       "               Route for                                                                    \n",
       "               local                                                                       \n",
       " 00027233-002  M1           M1           430596       415827       -1.53873     53.638072   \n",
       "               northbound                                                                   \n",
       "               Jct 39 lane                                                                  \n",
       "               clos                                                                        \n",
       " 00001313-011  M1           M1           472224       257362       -0.944434    52.209756   \n",
       "               northbound                                                                   \n",
       "               and                                                                          \n",
       "               southbound                                                                   \n",
       "               J                                                                           \n",
       " 00028472-002  A14          A14          485666       277318       -0.742691    52.387199   \n",
       "               westbound                                                                    \n",
       "               Jct 8 to 7\n",
       "                                                                  \n",
       "               Lane                                                                        \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample data from 'raw_old_roadworks' with WGS84 coordinates (first 5 rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reference_number</th><th>centre_easting_numeric</th><th>centre_northing_numeric</th><th>longitude_wgs84</th><th>latitude_wgs84</th></tr><tr><td>str</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;972963&quot;</td><td>456252</td><td>278173</td><td>-1.174681</td><td>52.39869</td></tr><tr><td>&quot;978905&quot;</td><td>499082</td><td>235992</td><td>-0.557701</td><td>52.013521</td></tr><tr><td>&quot;998294&quot;</td><td>465924</td><td>260154</td><td>-1.036076</td><td>52.235641</td></tr><tr><td>&quot;1172899&quot;</td><td>446842</td><td>324130</td><td>-1.306477</td><td>52.812687</td></tr><tr><td>&quot;1306529&quot;</td><td>511897</td><td>202047</td><td>-0.382033</td><td>51.706014</td></tr><tr><td>&quot;1384320&quot;</td><td>447205</td><td>335640</td><td>-1.299428</td><td>52.916116</td></tr><tr><td>&quot;1384375&quot;</td><td>449297</td><td>351779</td><td>-1.265864</td><td>53.060993</td></tr><tr><td>&quot;1439508&quot;</td><td>462487</td><td>262887</td><td>-1.085893</td><td>52.260609</td></tr><tr><td>&quot;1528177&quot;</td><td>430564</td><td>418913</td><td>-1.538912</td><td>53.66581</td></tr><tr><td>&quot;1535479&quot;</td><td>452705</td><td>304813</td><td>-1.222593</td><td>52.638512</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 5)\n",
       "\n",
       " reference_number  centre_easting_numer  centre_northing_num  longitude_wgs84  latitude_wgs84 \n",
       " ---               ic                    eric                 ---              ---            \n",
       " str               ---                   ---                  f64              f64            \n",
       "                   i32                   i32                                                  \n",
       "\n",
       " 972963            456252                278173               -1.174681        52.39869       \n",
       " 978905            499082                235992               -0.557701        52.013521      \n",
       " 998294            465924                260154               -1.036076        52.235641      \n",
       " 1172899           446842                324130               -1.306477        52.812687      \n",
       " 1306529           511897                202047               -0.382033        51.706014      \n",
       " 1384320           447205                335640               -1.299428        52.916116      \n",
       " 1384375           449297                351779               -1.265864        53.060993      \n",
       " 1439508           462487                262887               -1.085893        52.260609      \n",
       " 1528177           430564                418913               -1.538912        53.66581       \n",
       " 1535479           452705                304813               -1.222593        52.638512      \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Display sample data with new WGS84 columns ---\n",
    "print(f\"\\n--- Sample data from '{RAW_NEW_TABLE_NAME}' with WGS84 coordinates (first 5 rows) ---\")\n",
    "cols_to_select_new = ['NEW_EVENT_NUMBER', 'DESCRIPTION', 'ROAD_NUMBERS', 'CENTRE_EASTING_NUMERIC', 'CENTRE_NORTHING_NUMERIC', 'longitude_wgs84', 'latitude_wgs84']\n",
    "selected_cols_str_new = \", \".join([f'\"{c}\"' for c in cols_to_select_new])\n",
    "sample_new_wgs84_df = run_query(con, f'SELECT {selected_cols_str_new} FROM \"{RAW_NEW_TABLE_NAME}\" WHERE longitude_wgs84 IS NOT NULL LIMIT 10')\n",
    "if sample_new_wgs84_df is not None:\n",
    "    display(sample_new_wgs84_df)\n",
    "else:\n",
    "    print(\"Could not retrieve sample data or no transformed data available.\")\n",
    "\n",
    "print(f\"\\n--- Sample data from '{RAW_OLD_TABLE_NAME}' with WGS84 coordinates (first 5 rows) ---\")\n",
    "cols_to_select_old = ['reference_number', 'centre_easting_numeric', 'centre_northing_numeric', 'longitude_wgs84', 'latitude_wgs84']\n",
    "selected_cols_str_old = \", \".join([f'\"{c}\"' for c in cols_to_select_old])\n",
    "sample_old_wgs84_df = run_query(con, f'SELECT {selected_cols_str_old} FROM \"{RAW_OLD_TABLE_NAME}\" WHERE longitude_wgs84 IS NOT NULL LIMIT 10')\n",
    "if sample_old_wgs84_df is not None:\n",
    "    display(sample_old_wgs84_df)\n",
    "else:\n",
    "    print(\"Could not retrieve sample data or no transformed data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ebf51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ea3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dde8324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
