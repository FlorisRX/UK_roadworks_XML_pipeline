{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa3ab55",
   "metadata": {},
   "source": [
    "# Explore XML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "614cee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import duckdb\n",
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9faecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "NEW_DATA_DIRECTORY = 'data/new_format'     # data from 2018 onwards\n",
    "OLD_DATA_DIRECTORY = 'data/old_format' # data from 2017 and earlier\n",
    "\n",
    "DUCKDB_FILE = 'roadworks_data.duckdb'  # Name for your DuckDB database file\n",
    "# Define separate table names for new and old formats\n",
    "RAW_NEW_TABLE_NAME = 'raw_new_roadworks'\n",
    "RAW_OLD_TABLE_NAME = 'raw_old_roadworks'\n",
    "\n",
    "# Define the namespace map\n",
    "NSMAP = {'d': 'WebTeam'}\n",
    "\n",
    "# XPath to find the repeating record element\n",
    "NEW_ROADWORK_RECORD_XPATH = './/d:HE_PLANNED_WORKS'\n",
    "OLD_ROADWORK_RECORD_XPATH = './/ha_planned_works' # XPath for the old format record\n",
    "\n",
    "# --- Define Raw Columns based on exploration ---\n",
    "\n",
    "# Columns for the 'new' format raw table\n",
    "# Includes source_filename and handles nested elements\n",
    "RAW_NEW_COLUMNS = [\n",
    "    'source_filename',\n",
    "    # Attributes from HE_PLANNED_WORKS\n",
    "    'NEW_EVENT_NUMBER',\n",
    "    'OLD_REFERENCE_NUMBER',\n",
    "    'SDATE',\n",
    "    'EDATE',\n",
    "    'EXPDEL',\n",
    "    'DESCRIPTION',\n",
    "    'CLOSURE_TYPE',\n",
    "    'STATUS',\n",
    "    'PUBLISHED_DATE',\n",
    "    # Nested attributes (will be extracted)\n",
    "    'CENTRE_EASTING',\n",
    "    'CENTRE_NORTHING',\n",
    "    'ROAD_NUMBERS' # Potentially multiple, joined by ';'\n",
    "]\n",
    "\n",
    "# Columns for the 'old' format raw table\n",
    "# Includes source_filename and direct child element tags\n",
    "RAW_OLD_COLUMNS = [\n",
    "    'source_filename',\n",
    "    # Child elements of ha_planned_works\n",
    "    'reference_number',\n",
    "    'start_date',\n",
    "    'end_date',\n",
    "    'expected_delay',\n",
    "    'description',\n",
    "    'closure_type',\n",
    "    'status',\n",
    "    'published_date',\n",
    "    'centre_easting',\n",
    "    'centre_northing',\n",
    "    'road',\n",
    "    'location',\n",
    "    'local_authority',\n",
    "    'traffic_management'\n",
    "]\n",
    "\n",
    "# Define XPaths for nested data relative to the NEW format HE_PLANNED_WORKS element\n",
    "NEW_COORD_XPATH = './d:EASTNORTH/d:Report/d:EASTINGNORTHING/d:EASTNORTH_Collection/d:EASTNORTH'\n",
    "NEW_ROAD_XPATH = './d:ROADS/d:Report/d:ROADS/d:ROAD_Collection/d:ROAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471dd6a",
   "metadata": {},
   "source": [
    "### OLD: Extract single XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record_data_as_dict(record_element, source_filename):\n",
    "    \"\"\"\n",
    "    Extracts data from a single <HE_PLANNED_WORKS> lxml element into a dictionary.\n",
    "    Handles nested structures.\n",
    "    Args:\n",
    "        record_element: The lxml element for the record.\n",
    "        source_filename: The name of the file this record came from.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted data for one record,\n",
    "        or None if essential data (like event number) is missing.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    data['source_filename'] = source_filename\n",
    "\n",
    "    # Extract direct attributes based on TARGET_COLUMNS (excluding derived ones)\n",
    "    direct_attrs = [col for col in TARGET_COLUMNS if col not in ['source_filename', 'centre_easting', 'centre_northing', 'road_numbers']]\n",
    "    for attr in direct_attrs:\n",
    "        data[attr] = record_element.get(attr)\n",
    "\n",
    "    # Basic check - skip if no event number\n",
    "    if data.get('NEW_EVENT_NUMBER') is None:\n",
    "        # print(f\"Warning: Record missing NEW_EVENT_NUMBER in {source_filename}. Skipping.\")\n",
    "        return None # Return None to indicate skipping this record\n",
    "\n",
    "    # Extract nested coordinates\n",
    "    coord_elements = record_element.xpath(NEW_COORD_XPATH, namespaces=NSMAP)\n",
    "    if coord_elements:\n",
    "        coord_element = coord_elements[0]\n",
    "        data['centre_easting'] = coord_element.get('CENTRE_EASTING')\n",
    "        data['centre_northing'] = coord_element.get('CENTRE_NORTHING')\n",
    "    else:\n",
    "        data['centre_easting'] = None\n",
    "        data['centre_northing'] = None\n",
    "\n",
    "    # Extract nested roads\n",
    "    road_elements = record_element.xpath(NEW_ROAD_XPATH, namespaces=NSMAP)\n",
    "    if road_elements:\n",
    "        road_numbers = [road.get('ROAD_NUMBER') for road in road_elements if road.get('ROAD_NUMBER')]\n",
    "        data['road_numbers'] = '; '.join(road_numbers) if road_numbers else None\n",
    "    else:\n",
    "        data['road_numbers'] = None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132af45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_files(data_dir, db_file, table_name):\n",
    "    \"\"\"\n",
    "    Processes all XML files in a directory and loads data into DuckDB\n",
    "    directly using executemany.\n",
    "    \"\"\"\n",
    "    all_records_data_dicts = [] # Keep collecting dictionaries first\n",
    "    xml_files = glob.glob(os.path.join(data_dir, '*.xml'))\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Error: No XML files found in directory: {data_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(xml_files)} XML files to process in '{data_dir}'.\")\n",
    "    parser = etree.XMLParser(recover=True, ns_clean=True)\n",
    "\n",
    "    total_processed_records = 0\n",
    "    skipped_records = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing file: {filename}...\")\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            records = root.xpath(NEW_ROADWORK_RECORD_XPATH, namespaces=NSMAP)\n",
    "\n",
    "            if not records:\n",
    "                print(f\"  Warning: No records found matching XPath in {filename}.\")\n",
    "                continue\n",
    "\n",
    "            file_record_count = 0\n",
    "            for record in records:\n",
    "                try:\n",
    "                    extracted_dict = extract_record_data_as_dict(record, filename)\n",
    "                    if extracted_dict:\n",
    "                        all_records_data_dicts.append(extracted_dict)\n",
    "                        file_record_count += 1\n",
    "                    else:\n",
    "                        skipped_records += 1 # Count records skipped due to missing ID\n",
    "                except Exception as e_rec:\n",
    "                    event_id = record.get('NEW_EVENT_NUMBER', 'UNKNOWN_ID')\n",
    "                    print(f\"  Error processing record {event_id} in {filename}: {e_rec}\")\n",
    "                    skipped_records += 1\n",
    "\n",
    "            print(f\"  Extracted {file_record_count} valid records from {filename}.\")\n",
    "            total_processed_records += file_record_count\n",
    "\n",
    "        except etree.XMLSyntaxError as e_xml:\n",
    "            print(f\"  Error parsing XML file {filename}: {e_xml}. Skipping file.\")\n",
    "        except Exception as e_file:\n",
    "            print(f\"  An unexpected error occurred processing file {filename}: {e_file}. Skipping file.\")\n",
    "\n",
    "    if not all_records_data_dicts:\n",
    "        print(\"No valid data extracted from any files. Database will not be updated.\")\n",
    "        if skipped_records > 0:\n",
    "             print(f\"Note: {skipped_records} records were skipped due to errors or missing IDs.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nTotal valid records extracted across all files: {total_processed_records}\")\n",
    "    if skipped_records > 0:\n",
    "        print(f\"Total records skipped due to errors or missing IDs: {skipped_records}\")\n",
    "\n",
    "    # --- Convert list of dictionaries to list of tuples/lists for insertion ---\n",
    "    print(\"Preparing data for insertion...\")\n",
    "    data_to_insert = []\n",
    "    t = 0 # DEBUG\n",
    "    for record_dict in all_records_data_dicts:\n",
    "        row_values = [record_dict.get(col_name) for col_name in TARGET_COLUMNS]\n",
    "        if t == 0: # DEBUG\n",
    "            print(f\"Row {t}: {row_values}\") # DEBUG\n",
    "        t += 1 # DEBUG\n",
    "        data_to_insert.append(row_values)\n",
    "\n",
    "    # --- Load data into DuckDB directly using executemany ---\n",
    "    print(f\"Connecting to DuckDB database: {db_file}\")\n",
    "    con = duckdb.connect(database=db_file, read_only=False)\n",
    "\n",
    "    try:\n",
    "        print(f\"Creating or replacing table: {table_name}\")\n",
    "        column_defs = [f'\"{col}\" VARCHAR' for col in TARGET_COLUMNS] # Quote names\n",
    "        create_table_sql = f\"CREATE OR REPLACE TABLE {table_name} ({', '.join(column_defs)})\"\n",
    "        con.execute(create_table_sql)\n",
    "\n",
    "        print(f\"Inserting {len(data_to_insert)} records into {table_name}...\")\n",
    "\n",
    "        # ***** CORRECTED INSERTION METHOD using executemany *****\n",
    "        # Create the SQL insert statement with placeholders\n",
    "        placeholders = ', '.join(['?'] * NUM_COLUMNS) # e.g., \"?, ?, ?, ...\"\n",
    "        insert_sql = f'INSERT INTO {table_name} VALUES ({placeholders})'\n",
    "\n",
    "        # Execute the insert statement for all rows in data_to_insert\n",
    "        con.executemany(insert_sql, data_to_insert)\n",
    "        # *******************************************************\n",
    "\n",
    "        con.commit() # Commit the transaction\n",
    "        print(\"Data insertion complete.\")\n",
    "\n",
    "        # Verify insertion (optional)\n",
    "        count_result = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()\n",
    "        print(f\"Verification: Table '{table_name}' now contains {count_result[0]} rows.\")\n",
    "\n",
    "    except duckdb.Error as e_db: # Catch specific DuckDB errors\n",
    "        print(f\"Database error: {e_db}\")\n",
    "        try:\n",
    "            print(\"Attempting to rollback transaction.\")\n",
    "            con.rollback()\n",
    "        except duckdb.TransactionException as e_tx:\n",
    "            print(f\"Rollback failed (likely no active transaction): {e_tx}\")\n",
    "    except Exception as e:\n",
    "         print(f\"An unexpected error occurred during DB operation: {e}\")\n",
    "         try:\n",
    "            con.rollback()\n",
    "         except duckdb.TransactionException as e_tx:\n",
    "            print(f\"Rollback failed (likely no active transaction): {e_tx}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "        print(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a148eef",
   "metadata": {},
   "source": [
    "### Find all unique attributes in many XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21064cb3",
   "metadata": {},
   "source": [
    "##### For 'new' format (attributes-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546d879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_record_attributes_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Parses all XML files (new format) in a directory and finds all unique attribute names\n",
    "    used across all elements matching the NEW_ROADWORK_RECORD_XPATH in any file.\n",
    "    \"\"\"\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Error: No XML files found in directory: {directory_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"--- Finding All Unique Attributes in Directory: {directory_path} ---\")\n",
    "    print(f\"Found {len(xml_files)} XML files to scan.\")\n",
    "\n",
    "    all_attribute_names = set() # Use a set to automatically store unique names across all files\n",
    "    parser = etree.XMLParser(recover=True, ns_clean=True) # Define parser once\n",
    "\n",
    "    processed_files = 0\n",
    "    files_with_errors = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        # print(f\"Scanning file: {filename}...\") # Optional: uncomment for more verbose output\n",
    "        try:\n",
    "            # Parse the XML file\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Use xpath with the namespace map to find all records in this file\n",
    "            records = root.xpath(NEW_ROADWORK_RECORD_XPATH, namespaces=NSMAP)\n",
    "\n",
    "            if not records:\n",
    "                # print(f\"  Warning: No records found matching XPath in {filename}.\") # Optional warning\n",
    "                continue # Move to the next file if no records found\n",
    "\n",
    "            # Iterate through ALL found records in the current file\n",
    "            for record in records:\n",
    "                # Get the keys (attribute names) from the current record's attributes\n",
    "                attribute_keys = record.attrib.keys()\n",
    "                all_attribute_names.update(attribute_keys)\n",
    "\n",
    "                # Additionally: find attributes in DESCENDANT elements\n",
    "                # Use iterdescendants() to visit every element below the current record\n",
    "                for descendant in record.iterdescendants():\n",
    "                    all_attribute_names.update(descendant.attrib.keys())\n",
    "\n",
    "            processed_files += 1\n",
    "\n",
    "        except etree.XMLSyntaxError as e:\n",
    "            print(f\"  Error parsing XML file {filename}: {e}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  An unexpected error occurred scanning file {filename}: {e}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "\n",
    "    print(f\"\\n--- Scan Complete ---\")\n",
    "    print(f\"Successfully scanned {processed_files} files.\")\n",
    "    if files_with_errors > 0:\n",
    "        print(f\"Skipped {files_with_errors} files due to errors.\")\n",
    "\n",
    "    if not all_attribute_names:\n",
    "        print(\"No attributes found in any successfully processed files.\")\n",
    "        return None\n",
    "\n",
    "    # Sort the results for readability\n",
    "    sorted_attributes = sorted(list(all_attribute_names))\n",
    "\n",
    "    print(f\"\\nFound {len(sorted_attributes)} unique attributes across all scanned files:\")\n",
    "    return sorted_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cfe7734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding All Unique Attributes in Directory: data/new_format ---\n",
      "Found 8 XML files to scan.\n",
      "\n",
      "--- Scan Complete ---\n",
      "Successfully scanned 8 files.\n",
      "\n",
      "Found 13 unique attributes across all scanned files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CENTRE_EASTING',\n",
       " 'CENTRE_NORTHING',\n",
       " 'CLOSURE_TYPE',\n",
       " 'DESCRIPTION',\n",
       " 'EDATE',\n",
       " 'EXPDEL',\n",
       " 'NEW_EVENT_NUMBER',\n",
       " 'Name',\n",
       " 'OLD_REFERENCE_NUMBER',\n",
       " 'PUBLISHED_DATE',\n",
       " 'ROAD_NUMBER',\n",
       " 'SDATE',\n",
       " 'STATUS']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_record_attributes_in_directory(NEW_DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f527b21",
   "metadata": {},
   "source": [
    "##### For 'old' format (child element-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de44832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_record_elements_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Parses all XML files (old format) in a directory and finds all\n",
    "    unique child element tag names used across all elements matching the\n",
    "    OLD_ROADWORK_RECORD_XPATH in any file.\n",
    "    \"\"\"\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Error: No XML files found in directory: {directory_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"--- Finding All Unique Child Element Tags in Directory: {directory_path} ---\")\n",
    "    print(f\"Found {len(xml_files)} XML files to scan.\")\n",
    "\n",
    "    all_element_tags = set() # Use a set to automatically store unique tag names\n",
    "    # Use a simpler parser if namespaces are not expected/needed for old format\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "\n",
    "    processed_files = 0\n",
    "    files_with_errors = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        # print(f\"Scanning file: {filename}...\") # Optional: uncomment for more verbose output\n",
    "        try:\n",
    "            # Parse the XML file\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Check if the root tag matches the expected old format root\n",
    "            if root.tag != 'ha_planned_roadworks':\n",
    "                # print(f\"  Skipping file {filename}: Root tag '{root.tag}' does not match expected 'ha_planned_roadworks'.\")\n",
    "                continue # Skip files that don't match the old root tag\n",
    "\n",
    "            # Use xpath to find all records in this file (no namespace needed)\n",
    "            records = root.xpath(OLD_ROADWORK_RECORD_XPATH) # Use the XPath for the old format\n",
    "\n",
    "            if not records:\n",
    "                # print(f\"  Warning: No records found matching XPath '{OLD_ROADWORK_RECORD_XPATH}' in {filename}.\")\n",
    "                continue # Move to the next file if no records found\n",
    "\n",
    "            # Iterate through ALL found records in the current file\n",
    "            for record in records:\n",
    "                # Iterate through the child elements of the record\n",
    "                for child_element in record:\n",
    "                    # Add the tag name of the child element to the set\n",
    "                    all_element_tags.add(child_element.tag)\n",
    "\n",
    "            processed_files += 1\n",
    "\n",
    "        except etree.XMLSyntaxError as e:\n",
    "            print(f\"  Error parsing XML file {filename}: {e}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  An unexpected error occurred scanning file {filename}: {e}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "\n",
    "    print(f\"\\n--- Scan Complete ---\")\n",
    "    print(f\"Successfully scanned {processed_files} files (matching root tag).\")\n",
    "    if files_with_errors > 0:\n",
    "        print(f\"Skipped {files_with_errors} files due to errors during parsing.\")\n",
    "    skipped_non_matching = len(xml_files) - processed_files - files_with_errors\n",
    "    if skipped_non_matching > 0:\n",
    "         print(f\"Skipped {skipped_non_matching} files because their root tag did not match 'ha_planned_roadworks'.\")\n",
    "\n",
    "\n",
    "    if not all_element_tags:\n",
    "        print(\"No child element tags found in any successfully processed files.\")\n",
    "        return None\n",
    "\n",
    "    # Sort the results for readability\n",
    "    sorted_tags = sorted(list(all_element_tags))\n",
    "\n",
    "    print(f\"\\nFound {len(sorted_tags)} unique child element tags across all scanned files:\")\n",
    "    return sorted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecf9d62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding All Unique Child Element Tags in Directory: data/old_format ---\n",
      "Found 7 XML files to scan.\n",
      "\n",
      "--- Scan Complete ---\n",
      "Successfully scanned 7 files (matching root tag).\n",
      "\n",
      "Found 14 unique child element tags across all scanned files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['centre_easting',\n",
       " 'centre_northing',\n",
       " 'closure_type',\n",
       " 'description',\n",
       " 'end_date',\n",
       " 'expected_delay',\n",
       " 'local_authority',\n",
       " 'location',\n",
       " 'published_date',\n",
       " 'reference_number',\n",
       " 'road',\n",
       " 'start_date',\n",
       " 'status',\n",
       " 'traffic_management']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_format_elements = find_all_record_elements_in_directory(OLD_DATA_DIRECTORY)\n",
    "old_format_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9948b",
   "metadata": {},
   "source": [
    "'New' format attributes:\n",
    "```python\n",
    "['CENTRE_EASTING',\n",
    " 'CENTRE_NORTHING',\n",
    " 'CLOSURE_TYPE',\n",
    " 'DESCRIPTION',\n",
    " 'EDATE',\n",
    " 'EXPDEL',\n",
    " 'NEW_EVENT_NUMBER',\n",
    " 'Name',\n",
    " 'OLD_REFERENCE_NUMBER',\n",
    " 'PUBLISHED_DATE',\n",
    " 'ROAD_NUMBER',\n",
    " 'SDATE',\n",
    " 'STATUS']\n",
    "```\n",
    "\n",
    "'Old' format attributes:\n",
    "```python\n",
    "['centre_easting',\n",
    " 'centre_northing',\n",
    " 'closure_type',\n",
    " 'description',\n",
    " 'end_date',\n",
    " 'expected_delay',\n",
    " 'local_authority',\n",
    " 'location',\n",
    " 'published_date',\n",
    " 'reference_number',\n",
    " 'road',\n",
    " 'start_date',\n",
    " 'status',\n",
    " 'traffic_management']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d7157",
   "metadata": {},
   "source": [
    "### Explore some records\n",
    "\n",
    "##### For 'new' format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7bd3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RECORDS_TO_INSPECT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd458644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exploring XML File (Updated): data/new_format/nh_roadworks_2025_14_4.xml ---\n",
      "\n",
      "1. Root Element Tag: {WebTeam}Report\n",
      "   Root Namespace Map: {'xsi': 'http://www.w3.org/2001/XMLSchema-instance', None: 'WebTeam'}\n",
      "\n",
      "2. Found 1429 records matching XPath './/d:HE_PLANNED_WORKS'.\n",
      "--- Inspecting first 3 records ---\n",
      "\n",
      "\n",
      "--- Record 1 ---\n",
      " Attributes of <HE_PLANNED_WORKS>:\n",
      "    NEW_EVENT_NUMBER: 00352573-001\n",
      "    SDATE: 31-DEC-2023 23:59\n",
      "    EDATE: 31-MAY-2025 23:59\n",
      "    EXPDEL: Moderate (10 - 30 mins)\n",
      "    DESCRIPTION: M25 Anticlockwise Jct 11 to Jct 9\n",
      "Narrow Lanes for Major Improvement Scheme \n",
      "    CLOSURE_TYPE: Major Schemes\n",
      "    STATUS: Published\n",
      "    PUBLISHED_DATE: 2023-12-21T14:45:07\n",
      "\n",
      " Extracted Key Attributes:\n",
      "- NEW_EVENT_NUMBER: 00352573-001\n",
      "- SDATE: 31-DEC-2023 23:59\n",
      "- EDATE: 31-MAY-2025 23:59\n",
      "- DESCRIPTION: M25 Anticlockwise Jct 11 to Jct 9\n",
      "Narrow Lanes for Major Improvement Scheme \n",
      "- CLOSURE_TYPE: Major Schemes\n",
      "- STATUS: Published\n",
      "- PUBLISHED_DATE: 2023-12-21T14:45:07\n",
      "- EXPDEL: Moderate (10 - 30 mins)\n",
      "\n",
      " Extracting Nested Coordinates:\n",
      "    CENTRE_EASTING: 507930\n",
      "    CENTRE_NORTHING: 159334\n",
      "\n",
      "Extracting Nested Roads:\n",
      "    ROAD_NUMBER(s): ['M25']\n",
      "\n",
      "\n",
      "--- Record 2 ---\n",
      " Attributes of <HE_PLANNED_WORKS>:\n",
      "    NEW_EVENT_NUMBER: 00380443-001\n",
      "    SDATE: 29-MAY-2024 21:00\n",
      "    EDATE: 29-MAY-2025 23:59\n",
      "    EXPDEL: Slight (less than 10 mins)\n",
      "    DESCRIPTION: M275 southbound M27 to Tipner \n",
      "Lane closure for Portsmouth City Council.\n",
      "\n",
      "    CLOSURE_TYPE: Emergency and urgent Street/Road Works\n",
      "    STATUS: Published\n",
      "    PUBLISHED_DATE: 2024-05-30T11:39:55\n",
      "\n",
      " Extracted Key Attributes:\n",
      "- NEW_EVENT_NUMBER: 00380443-001\n",
      "- SDATE: 29-MAY-2024 21:00\n",
      "- EDATE: 29-MAY-2025 23:59\n",
      "- DESCRIPTION: M275 southbound M27 to Tipner \n",
      "Lane closure for Portsmouth City Council.\n",
      "\n",
      "- CLOSURE_TYPE: Emergency and urgent Street/Road Works\n",
      "- STATUS: Published\n",
      "- PUBLISHED_DATE: 2024-05-30T11:39:55\n",
      "- EXPDEL: Slight (less than 10 mins)\n",
      "\n",
      " Extracting Nested Coordinates:\n",
      "    CENTRE_EASTING: 464494\n",
      "    CENTRE_NORTHING: 104095\n",
      "\n",
      "Extracting Nested Roads:\n",
      "    ROAD_NUMBER(s): ['M27']\n",
      "\n",
      "\n",
      "--- Record 3 ---\n",
      " Attributes of <HE_PLANNED_WORKS>:\n",
      "    NEW_EVENT_NUMBER: 00389408-001\n",
      "    SDATE: 24-JUL-2024 12:00\n",
      "    EDATE: 26-AUG-2027 05:00\n",
      "    EXPDEL: Slight (less than 10 mins)\n",
      "    DESCRIPTION: A38 both directions Streethay (Cappers Lane Jct) to Fradley.\n",
      "24/7 Fradley Park layby closure and narrow lanes with 40mph speed limit.\n",
      "\n",
      "    CLOSURE_TYPE: Developer Works\n",
      "    STATUS: Published\n",
      "    PUBLISHED_DATE: 2024-07-24T11:00:45\n",
      "\n",
      " Extracted Key Attributes:\n",
      "- NEW_EVENT_NUMBER: 00389408-001\n",
      "- SDATE: 24-JUL-2024 12:00\n",
      "- EDATE: 26-AUG-2027 05:00\n",
      "- DESCRIPTION: A38 both directions Streethay (Cappers Lane Jct) to Fradley.\n",
      "24/7 Fradley Park layby closure and narrow lanes with 40mph speed limit.\n",
      "\n",
      "- CLOSURE_TYPE: Developer Works\n",
      "- STATUS: Published\n",
      "- PUBLISHED_DATE: 2024-07-24T11:00:45\n",
      "- EXPDEL: Slight (less than 10 mins)\n",
      "\n",
      " Extracting Nested Coordinates:\n",
      "    CENTRE_EASTING: 413949\n",
      "    CENTRE_NORTHING: 309566\n",
      "\n",
      "Extracting Nested Roads:\n",
      "    ROAD_NUMBER(s): ['A38']\n",
      "\n",
      "--- End of Exploration for data/new_format/nh_roadworks_2025_14_4.xml ---\n"
     ]
    }
   ],
   "source": [
    "def explore_roadworks_xml_new(file_path):\n",
    "    \"\"\"Parses and explores the specific structure of the provided roadworks XML.\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Exploring XML File (Updated): {file_path} ---\")\n",
    "\n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        # Using recover=True can help skip over minor errors if files are slightly malformed\n",
    "        parser = etree.XMLParser(recover=True, ns_clean=True)\n",
    "        tree = etree.parse(file_path, parser)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        print(f\"\\n1. Root Element Tag: {root.tag}\") # Should be {WebTeam}Report\n",
    "        print(f\"   Root Namespace Map: {root.nsmap}\")\n",
    "\n",
    "        # Use xpath with the namespace map to find the records\n",
    "        records = root.xpath(NEW_ROADWORK_RECORD_XPATH, namespaces=NSMAP)\n",
    "\n",
    "        if not records:\n",
    "            print(f\"\\nError: Could not find any elements matching XPath '{NEW_ROADWORK_RECORD_XPATH}'.\")\n",
    "            print(\"Please double-check the NEW_ROADWORK_RECORD_XPATH and the XML structure.\")\n",
    "            # Print children with their full {namespace}tag names to help debug\n",
    "            print(\"\\nFirst few children of the root (with full tags):\")\n",
    "            for i, child in enumerate(root[:5]):\n",
    "                 print(f\"  Child {i+1}: {child.tag}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n2. Found {len(records)} records matching XPath '{NEW_ROADWORK_RECORD_XPATH}'.\")\n",
    "        print(f\"--- Inspecting first {min(NUM_RECORDS_TO_INSPECT, len(records))} records ---\")\n",
    "\n",
    "        for i, record in enumerate(records[:NUM_RECORDS_TO_INSPECT]):\n",
    "            print(f\"\\n\\n--- Record {i+1} ---\")\n",
    "\n",
    "            # --- Accessing Attributes of <HE_PLANNED_WORKS> ---\n",
    "            print(\" Attributes of <HE_PLANNED_WORKS>:\")\n",
    "            record_attrs = record.attrib\n",
    "            for key, value in record_attrs.items():\n",
    "                 print(f\"    {key}: {value}\")\n",
    "\n",
    "            # Extract specific attributes by name\n",
    "            event_id = record.get('NEW_EVENT_NUMBER')\n",
    "            start_date = record.get('SDATE')\n",
    "            end_date = record.get('EDATE')\n",
    "            description = record.get('DESCRIPTION')\n",
    "            closure_type = record.get('CLOSURE_TYPE')\n",
    "            status = record.get('STATUS')\n",
    "            published_date = record.get('PUBLISHED_DATE')\n",
    "            exp_del = record.get('EXPDEL')\n",
    "\n",
    "            print(\"\\n Extracted Key Attributes:\")\n",
    "            print(f\"- NEW_EVENT_NUMBER: {event_id}\")\n",
    "            print(f\"- SDATE: {start_date}\")\n",
    "            print(f\"- EDATE: {end_date}\")\n",
    "            print(f\"- DESCRIPTION: {description}\")\n",
    "            print(f\"- CLOSURE_TYPE: {closure_type}\")\n",
    "            print(f\"- STATUS: {status}\")\n",
    "            print(f\"- PUBLISHED_DATE: {published_date}\")\n",
    "            print(f\"- EXPDEL: {exp_del}\")\n",
    "\n",
    "\n",
    "            # --- Accessing Nested Coordinates ---\n",
    "            print(\"\\n Extracting Nested Coordinates:\")\n",
    "            # Define the precise XPath relative to the current 'record' element\n",
    "            coord_xpath = './d:EASTNORTH/d:Report/d:EASTINGNORTHING/d:EASTNORTH_Collection/d:EASTNORTH'\n",
    "            coord_elements = record.xpath(coord_xpath, namespaces=NSMAP)\n",
    "\n",
    "            if coord_elements:\n",
    "                # Usually expect only one coordinate block per record\n",
    "                coord_element = coord_elements[0]\n",
    "                easting = coord_element.get('CENTRE_EASTING')\n",
    "                northing = coord_element.get('CENTRE_NORTHING')\n",
    "                print(f\"    CENTRE_EASTING: {easting}\")\n",
    "                print(f\"    CENTRE_NORTHING: {northing}\")\n",
    "            else:\n",
    "                print(\"    Coordinate elements not found.\")\n",
    "\n",
    "            # --- Accessing Nested Roads ---\n",
    "            print(\"\\nExtracting Nested Roads:\")\n",
    "            # Define the precise XPath relative to the current 'record' element\n",
    "            road_xpath = './d:ROADS/d:Report/d:ROADS/d:ROAD_Collection/d:ROAD'\n",
    "            road_elements = record.xpath(road_xpath, namespaces=NSMAP)\n",
    "\n",
    "            if road_elements:\n",
    "                road_numbers = [road.get('ROAD_NUMBER') for road in road_elements]\n",
    "                print(f\"    ROAD_NUMBER(s): {road_numbers}\") # Might be multiple roads\n",
    "            else:\n",
    "                print(\"    Road elements not found.\")\n",
    "\n",
    "\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        print(f\"\\nError parsing XML file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "\n",
    "    print(f\"\\n--- End of Exploration for {file_path} ---\")\n",
    "\n",
    "\n",
    "explore_roadworks_xml_new(\"data/new_format/nh_roadworks_2025_14_4.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f7aca",
   "metadata": {},
   "source": [
    "##### For 'old' format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6e842fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example file 'data/old_format\\he_roadworks_2017_06_05' not found. Using first available file: data/old_format\\ha-roadworks_2011_10_10.xml\n",
      "--- Exploring XML File (Old Format): data/old_format\\ha-roadworks_2011_10_10.xml ---\n",
      "\n",
      "1. Root Element Tag: ha_planned_roadworks\n",
      "\n",
      "2. Found 1425 records matching XPath './/ha_planned_works'.\n",
      "--- Inspecting first 3 records ---\n",
      "\n",
      "\n",
      "--- Record 1 ---\n",
      " Record Element Tag: ha_planned_works\n",
      " Child Elements (Tag: Text Content):\n",
      "    reference_number: 972963\n",
      "    road: M1\n",
      "    local_authority: Leicestershire / Northamptonshire\n",
      "    location: Catthorpe\n",
      "    start_date: 2010-07-12T07:00:00\n",
      "    end_date: 2013-03-23T06:00:00\n",
      "    expected_delay: Moderate (10 - 30 mins)\n",
      "    description: Major junction works will include lane closures, contraflow, full closures and 50 MPH speed restrictions on the M1 and M6.\n",
      "    traffic_management: Other\n",
      "    closure_type: Planned Works\n",
      "    centre_easting: 456252\n",
      "    centre_northing: 278173\n",
      "    status: Firm\n",
      "    published_date: 2011-10-09T21:08:32\n",
      "\n",
      " Extracted Key Child Element Values:\n",
      "- reference_number: 972963\n",
      "- start_date: 2010-07-12T07:00:00\n",
      "- end_date: 2013-03-23T06:00:00\n",
      "- description: Major junction works will include lane closures, contraflow, full closures and 50 MPH speed restrictions on the M1 and M6.\n",
      "- road: M1\n",
      "- status: Firm\n",
      "- centre_easting: 456252\n",
      "- centre_northing: 278173\n",
      "- expected_delay: Moderate (10 - 30 mins)\n",
      "- closure_type: Planned Works\n",
      "\n",
      "\n",
      "--- Record 2 ---\n",
      " Record Element Tag: ha_planned_works\n",
      " Child Elements (Tag: Text Content):\n",
      "    reference_number: 978905\n",
      "    road: M1\n",
      "    local_authority: Bedfordshire / Buckinghamshire\n",
      "    location: Jct 13 to Jct 12\n",
      "    start_date: 2011-04-01T22:00:00\n",
      "    end_date: 2011-12-31T05:00:00\n",
      "    expected_delay: Moderate (10 - 30 mins)\n",
      "    description: Contraflow with speed restriction southbound 24 hrs due to improvement works.\n",
      "    traffic_management: Contraflow\n",
      "    closure_type: Planned Works\n",
      "    centre_easting: 499082\n",
      "    centre_northing: 235992\n",
      "    status: Firm\n",
      "    published_date: 2010-04-23T10:18:30\n",
      "\n",
      " Extracted Key Child Element Values:\n",
      "- reference_number: 978905\n",
      "- start_date: 2011-04-01T22:00:00\n",
      "- end_date: 2011-12-31T05:00:00\n",
      "- description: Contraflow with speed restriction southbound 24 hrs due to improvement works.\n",
      "- road: M1\n",
      "- status: Firm\n",
      "- centre_easting: 499082\n",
      "- centre_northing: 235992\n",
      "- expected_delay: Moderate (10 - 30 mins)\n",
      "- closure_type: Planned Works\n",
      "\n",
      "\n",
      "--- Record 3 ---\n",
      " Record Element Tag: ha_planned_works\n",
      " Child Elements (Tag: Text Content):\n",
      "    reference_number: 998294\n",
      "    road: M1\n",
      "    local_authority: Northamptonshire\n",
      "    location: Approach to Junction 16 (210113)\n",
      "    start_date: 2009-09-24T06:00:00\n",
      "    end_date: 2013-09-24T05:00:00\n",
      "    expected_delay: Slight (less than 10 mins)\n",
      "    description: Lane 1 closure and 24/7 Hardshoulder closure Southbound 21:00 to 06:00 hrs for surveys.\n",
      "    traffic_management: Lane Closure\n",
      "    closure_type: Planned Works\n",
      "    centre_easting: 465924\n",
      "    centre_northing: 260154\n",
      "    status: Firm\n",
      "    published_date: 2010-06-19T05:03:50\n",
      "\n",
      " Extracted Key Child Element Values:\n",
      "- reference_number: 998294\n",
      "- start_date: 2009-09-24T06:00:00\n",
      "- end_date: 2013-09-24T05:00:00\n",
      "- description: Lane 1 closure and 24/7 Hardshoulder closure Southbound 21:00 to 06:00 hrs for surveys.\n",
      "- road: M1\n",
      "- status: Firm\n",
      "- centre_easting: 465924\n",
      "- centre_northing: 260154\n",
      "- expected_delay: Slight (less than 10 mins)\n",
      "- closure_type: Planned Works\n",
      "\n",
      "--- End of Exploration for data/old_format\\ha-roadworks_2011_10_10.xml ---\n"
     ]
    }
   ],
   "source": [
    "def explore_roadworks_xml_old(file_path):\n",
    "    \"\"\"Parses and explores the structure of an old-format roadworks XML.\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Exploring XML File (Old Format): {file_path} ---\")\n",
    "\n",
    "    try:\n",
    "        # Use a simpler parser, potentially without namespace handling if not needed\n",
    "        parser = etree.XMLParser(recover=True)\n",
    "        tree = etree.parse(file_path, parser)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        print(f\"\\n1. Root Element Tag: {root.tag}\") # Should be ha_planned_roadworks\n",
    "\n",
    "        # Check if the root tag is as expected for the old format\n",
    "        if root.tag != 'ha_planned_roadworks':\n",
    "            print(f\"  Warning: Root tag '{root.tag}' does not match expected 'ha_planned_roadworks'.\")\n",
    "            # Optionally, still try to find records if the XPath might work\n",
    "            # return # Or uncomment to stop if root tag is wrong\n",
    "\n",
    "        # Use xpath to find the records (no namespace typically needed for old format)\n",
    "        records = root.xpath(OLD_ROADWORK_RECORD_XPATH)\n",
    "\n",
    "        if not records:\n",
    "            print(f\"\\nError: Could not find any elements matching XPath '{OLD_ROADWORK_RECORD_XPATH}'.\")\n",
    "            print(\"Please double-check the OLD_ROADWORK_RECORD_XPATH and the XML structure.\")\n",
    "            print(\"\\nFirst few children of the root:\")\n",
    "            for i, child in enumerate(root[:5]):\n",
    "                 print(f\"  Child {i+1}: {child.tag}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n2. Found {len(records)} records matching XPath '{OLD_ROADWORK_RECORD_XPATH}'.\")\n",
    "        print(f\"--- Inspecting first {min(NUM_RECORDS_TO_INSPECT, len(records))} records ---\")\n",
    "\n",
    "        for i, record in enumerate(records[:NUM_RECORDS_TO_INSPECT]):\n",
    "            print(f\"\\n\\n--- Record {i+1} ---\")\n",
    "            print(f\" Record Element Tag: {record.tag}\") # Should be ha_planned_works\n",
    "\n",
    "            # --- Accessing Child Elements ---\n",
    "            print(\" Child Elements (Tag: Text Content):\")\n",
    "            record_data = {}\n",
    "            for child in record:\n",
    "                # Clean up text content (strip whitespace, handle None)\n",
    "                text_content = (child.text or '').strip()\n",
    "                print(f\"    {child.tag}: {text_content}\")\n",
    "                record_data[child.tag] = text_content # Store for easier access later\n",
    "\n",
    "            # Extract specific child element text content by tag name\n",
    "            # Based on the output of find_all_record_elements_in_directory\n",
    "            ref_num = record_data.get('reference_number')\n",
    "            start_date = record_data.get('start_date')\n",
    "            end_date = record_data.get('end_date')\n",
    "            description = record_data.get('description')\n",
    "            road = record_data.get('road')\n",
    "            status = record_data.get('status')\n",
    "            easting = record_data.get('centre_easting')\n",
    "            northing = record_data.get('centre_northing')\n",
    "            delay = record_data.get('expected_delay')\n",
    "            closure = record_data.get('closure_type')\n",
    "\n",
    "            print(\"\\n Extracted Key Child Element Values:\")\n",
    "            print(f\"- reference_number: {ref_num}\")\n",
    "            print(f\"- start_date: {start_date}\")\n",
    "            print(f\"- end_date: {end_date}\")\n",
    "            print(f\"- description: {description}\")\n",
    "            print(f\"- road: {road}\")\n",
    "            print(f\"- status: {status}\")\n",
    "            print(f\"- centre_easting: {easting}\")\n",
    "            print(f\"- centre_northing: {northing}\")\n",
    "            print(f\"- expected_delay: {delay}\")\n",
    "            print(f\"- closure_type: {closure}\")\n",
    "\n",
    "\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        print(f\"\\nError parsing XML file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "\n",
    "    print(f\"\\n--- End of Exploration for {file_path} ---\")\n",
    "\n",
    "\n",
    "example_old_file = os.path.join(OLD_DATA_DIRECTORY, 'he_roadworks_2017_06_05')\n",
    "if os.path.exists(example_old_file):\n",
    "    explore_roadworks_xml_old(example_old_file)\n",
    "else:\n",
    "    # Find the first available XML file in the old data directory if the example doesn't exist\n",
    "    old_files = glob.glob(os.path.join(OLD_DATA_DIRECTORY, '*.xml'))\n",
    "    if old_files:\n",
    "        print(f\"Example file '{example_old_file}' not found. Using first available file: {old_files[0]}\")\n",
    "        explore_roadworks_xml_old(old_files[0])\n",
    "    else:\n",
    "        print(f\"Error: No XML files found in {OLD_DATA_DIRECTORY} to explore.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b732248",
   "metadata": {},
   "source": [
    "### Define XML-record extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b91a577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record_new_format(record_element, source_filename):\n",
    "    \"\"\"\n",
    "    Extracts raw data from a 'new' format <HE_PLANNED_WORKS> element\n",
    "    into a dictionary matching RAW_NEW_COLUMNS.\n",
    "    \"\"\"\n",
    "    data = {col: None for col in RAW_NEW_COLUMNS} # Initialize with None\n",
    "    data['source_filename'] = source_filename\n",
    "\n",
    "    # --- Extract direct attributes ---\n",
    "    data['NEW_EVENT_NUMBER'] = record_element.get('NEW_EVENT_NUMBER')\n",
    "    data['OLD_REFERENCE_NUMBER'] = record_element.get('OLD_REFERENCE_NUMBER')\n",
    "    data['SDATE'] = record_element.get('SDATE')\n",
    "    data['EDATE'] = record_element.get('EDATE')\n",
    "    data['EXPDEL'] = record_element.get('EXPDEL')\n",
    "    data['DESCRIPTION'] = record_element.get('DESCRIPTION')\n",
    "    data['CLOSURE_TYPE'] = record_element.get('CLOSURE_TYPE')\n",
    "    data['STATUS'] = record_element.get('STATUS')\n",
    "    data['PUBLISHED_DATE'] = record_element.get('PUBLISHED_DATE')\n",
    "\n",
    "    # Basic check - skip if no event number (essential identifier)\n",
    "    if data.get('NEW_EVENT_NUMBER') is None:\n",
    "        # print(f\"Warning: New format record missing NEW_EVENT_NUMBER in {source_filename}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # --- Extract nested coordinates ---\n",
    "    coord_elements = record_element.xpath(NEW_COORD_XPATH, namespaces=NSMAP)\n",
    "    if coord_elements:\n",
    "        coord_element = coord_elements[0]\n",
    "        data['CENTRE_EASTING'] = coord_element.get('CENTRE_EASTING')\n",
    "        data['CENTRE_NORTHING'] = coord_element.get('CENTRE_NORTHING')\n",
    "\n",
    "    # --- Extract nested roads ---\n",
    "    road_elements = record_element.xpath(NEW_ROAD_XPATH, namespaces=NSMAP)\n",
    "    if road_elements:\n",
    "        road_numbers_list = [road.get('ROAD_NUMBER') for road in road_elements if road.get('ROAD_NUMBER')]\n",
    "        # Join multiple roads with a separator\n",
    "        data['ROAD_NUMBERS'] = '; '.join(road_numbers_list) if road_numbers_list else None\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_record_old_format(record_element, source_filename):\n",
    "    \"\"\"\n",
    "    Extracts raw data from an 'old' format <ha_planned_works> element\n",
    "    into a dictionary matching RAW_OLD_COLUMNS.\n",
    "    \"\"\"\n",
    "    data = {col: None for col in RAW_OLD_COLUMNS} # Initialize with None\n",
    "    data['source_filename'] = source_filename\n",
    "\n",
    "    # Helper to get text content safely\n",
    "    def get_text(tag_name):\n",
    "        element = record_element.find(tag_name)\n",
    "        return element.text.strip() if element is not None and element.text else None\n",
    "\n",
    "    # --- Map child elements to raw columns ---\n",
    "    # Iterate through expected raw old columns (excluding source_filename)\n",
    "    for col_name in RAW_OLD_COLUMNS:\n",
    "        if col_name != 'source_filename':\n",
    "             data[col_name] = get_text(col_name)\n",
    "\n",
    "    # Basic check - skip if no reference number (essential identifier)\n",
    "    if data.get('reference_number') is None:\n",
    "        # print(f\"Warning: Old format record missing reference_number in {source_filename}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd130cd",
   "metadata": {},
   "source": [
    "### Generic directory processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "940c7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD METHOD: Returns a list (memory inefficient for large datasets)\n",
    "def process_directory(directory_path, record_xpath, extraction_func, nsmap=None):\n",
    "    \"\"\"\n",
    "    Processes all XML files in a directory using a specific XPath and extraction function.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing XML files.\n",
    "        record_xpath (str): XPath expression to find record elements.\n",
    "        extraction_func (callable): Function to call for each record element found.\n",
    "                                    It should accept (record_element, source_filename)\n",
    "                                    and return a dictionary or None.\n",
    "        nsmap (dict, optional): Namespace map for XPath evaluation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a processed record.\n",
    "    \"\"\"\n",
    "    all_records_data_dicts = []\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "    parser = etree.XMLParser(recover=True, ns_clean=True) # Use robust parser\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Warning: No XML files found in directory: {directory_path}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n--- Processing Directory: {directory_path} ---\")\n",
    "    print(f\"Found {len(xml_files)} XML files.\")\n",
    "\n",
    "    total_processed_records = 0\n",
    "    total_skipped_records = 0\n",
    "    files_with_errors = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        # print(f\"Processing file: {filename}...\") # Optional verbose output\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            # Find records using the provided XPath and namespace map\n",
    "            records = root.xpath(record_xpath, namespaces=nsmap)\n",
    "\n",
    "            if not records:\n",
    "                # print(f\"  Warning: No records found matching XPath in {filename}.\")\n",
    "                continue\n",
    "\n",
    "            file_record_count = 0\n",
    "            file_skipped_count = 0\n",
    "            for record in records:\n",
    "                try:\n",
    "                    extracted_dict = extraction_func(record, filename)\n",
    "                    if extracted_dict:\n",
    "                        all_records_data_dicts.append(extracted_dict)\n",
    "                        file_record_count += 1\n",
    "                    else:\n",
    "                        file_skipped_count += 1 # Count records skipped by extraction func\n",
    "                except Exception as e_rec:\n",
    "                    # Try to get an ID for logging, adapt based on potential extraction func errors\n",
    "                    event_id = \"UNKNOWN_ID\"\n",
    "                    try:\n",
    "                        if nsmap: # Likely new format\n",
    "                             event_id = record.get('NEW_EVENT_NUMBER', event_id)\n",
    "                        else: # Likely old format\n",
    "                             ref_num_el = record.find('reference_number')\n",
    "                             if ref_num_el is not None and ref_num_el.text:\n",
    "                                 event_id = ref_num_el.text.strip()\n",
    "                    except: pass # Ignore errors getting ID for logging\n",
    "                    print(f\"  Error processing record {event_id} in {filename}: {e_rec}\")\n",
    "                    file_skipped_count += 1\n",
    "\n",
    "            # if file_record_count > 0 or file_skipped_count > 0: # Only print if something happened\n",
    "            #    print(f\"  Extracted {file_record_count} valid records from {filename}. Skipped {file_skipped_count}.\")\n",
    "\n",
    "            total_processed_records += file_record_count\n",
    "            total_skipped_records += file_skipped_count\n",
    "\n",
    "        except etree.XMLSyntaxError as e_xml:\n",
    "            print(f\"  Error parsing XML file {filename}: {e_xml}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "        except Exception as e_file:\n",
    "            print(f\"  An unexpected error occurred processing file {filename}: {e_file}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "\n",
    "    print(f\"--- Directory Scan Complete: {directory_path} ---\")\n",
    "    print(f\"Successfully extracted {total_processed_records} records.\")\n",
    "    if total_skipped_records > 0:\n",
    "        print(f\"Skipped {total_skipped_records} records (missing ID or processing error).\")\n",
    "    if files_with_errors > 0:\n",
    "        print(f\"Skipped {files_with_errors} files due to parsing/file errors.\")\n",
    "\n",
    "    return all_records_data_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  UPDATED GENERATOR FUNCTION: Yields records one by one instead of returning a list\n",
    "def process_directory(directory_path, record_xpath, extraction_func, nsmap=None):\n",
    "    \"\"\"\n",
    "    Processes all XML files in a directory using a specific XPath and extraction function,\n",
    "    yielding each processed record as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing XML files.\n",
    "        record_xpath (str): XPath expression to find record elements.\n",
    "        extraction_func (callable): Function to call for each record element found.\n",
    "                                    It should accept (record_element, source_filename)\n",
    "                                    and return a dictionary or None.\n",
    "        nsmap (dict, optional): Namespace map for XPath evaluation. Defaults to None.\n",
    "\n",
    "    Yields:\n",
    "        dict: A dictionary representing a processed record, if valid.\n",
    "    \"\"\"\n",
    "    xml_files = glob.glob(os.path.join(directory_path, '*.xml'))\n",
    "    parser = etree.XMLParser(recover=True, ns_clean=True) # Use robust parser\n",
    "\n",
    "    if not xml_files:\n",
    "        print(f\"Warning: No XML files found in directory: {directory_path}\")\n",
    "        return # Return early if no files\n",
    "\n",
    "    print(f\"\\n--- Processing Directory: {directory_path} ---\")\n",
    "    print(f\"Found {len(xml_files)} XML files.\")\n",
    "\n",
    "    total_yielded_records = 0\n",
    "    total_skipped_records = 0\n",
    "    files_with_errors = 0\n",
    "\n",
    "    for file_path in xml_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            records = root.xpath(record_xpath, namespaces=nsmap)\n",
    "\n",
    "            if not records:\n",
    "                continue\n",
    "\n",
    "            file_yielded_count = 0\n",
    "            file_skipped_count = 0\n",
    "            for record in records:\n",
    "                try:\n",
    "                    extracted_dict = extraction_func(record, filename)\n",
    "                    if extracted_dict:\n",
    "                        yield extracted_dict\n",
    "                        file_yielded_count += 1\n",
    "                    else:\n",
    "                        file_skipped_count += 1\n",
    "                except Exception as e_rec:\n",
    "                    event_id = \"UNKNOWN_ID\"\n",
    "                    try: # Attempt to get ID for logging\n",
    "                        if nsmap: event_id = record.get('NEW_EVENT_NUMBER', event_id)\n",
    "                        else:\n",
    "                             ref_num_el = record.find('reference_number')\n",
    "                             if ref_num_el is not None and ref_num_el.text: event_id = ref_num_el.text.strip()\n",
    "                    except: pass\n",
    "                    print(f\"  Error processing record {event_id} in {filename}: {e_rec}\")\n",
    "                    file_skipped_count += 1\n",
    "\n",
    "            total_yielded_records += file_yielded_count\n",
    "            total_skipped_records += file_skipped_count\n",
    "\n",
    "        except etree.XMLSyntaxError as e_xml:\n",
    "            print(f\"  Error parsing XML file {filename}: {e_xml}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "        except Exception as e_file:\n",
    "            print(f\"  An unexpected error occurred processing file {filename}: {e_file}. Skipping file.\")\n",
    "            files_with_errors += 1\n",
    "\n",
    "    print(f\"--- Directory Scan Complete: {directory_path} ---\")\n",
    "    print(f\"Successfully yielded {total_yielded_records} records.\") \n",
    "    if total_skipped_records > 0:\n",
    "        print(f\"Skipped {total_skipped_records} records (missing ID or processing error).\")\n",
    "    if files_with_errors > 0:\n",
    "        print(f\"Skipped {files_with_errors} files due to parsing/file errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054af20",
   "metadata": {},
   "source": [
    "### Process data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_batches(con, table_name, target_columns, data_iterator, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Loads data from an iterator into a DuckDB table in batches.\n",
    "\n",
    "    Args:\n",
    "        con: Active DuckDB connection object.\n",
    "        table_name (str): Name of the target table.\n",
    "        target_columns (list): List of column names in the target table order.\n",
    "        data_iterator (iterator): An iterator yielding dictionaries of data.\n",
    "        batch_size (int): Number of records to insert per batch.\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    total_inserted = 0\n",
    "    num_columns = len(target_columns)\n",
    "    placeholders = ', '.join(['?'] * num_columns)\n",
    "    insert_sql = f'INSERT INTO \"{table_name}\" VALUES ({placeholders})'\n",
    "\n",
    "    print(f\"Starting batch insertion into '{table_name}' (batch size: {batch_size})...\")\n",
    "\n",
    "    for record_dict in data_iterator:\n",
    "        # Convert dict to list/tuple in the correct column order\n",
    "        row_values = [record_dict.get(col_name) for col_name in target_columns]\n",
    "        batch_data.append(row_values)\n",
    "\n",
    "        if len(batch_data) >= batch_size:\n",
    "            try:\n",
    "                con.executemany(insert_sql, batch_data)\n",
    "                total_inserted += len(batch_data)\n",
    "                print(f\"  Inserted batch of {len(batch_data)}. Total inserted: {total_inserted}\")\n",
    "                batch_data = [] # Clear the batch\n",
    "            except duckdb.Error as e:\n",
    "                print(f\"  Error inserting batch: {e}\")\n",
    "                # Decide how to handle batch errors (e.g., log, skip, stop)\n",
    "                # For now, just print and continue trying next batch\n",
    "                batch_data = [] # Clear potentially problematic batch\n",
    "\n",
    "    # Insert any remaining records in the last batch\n",
    "    if batch_data:\n",
    "        try:\n",
    "            con.executemany(insert_sql, batch_data)\n",
    "            total_inserted += len(batch_data)\n",
    "            print(f\"  Inserted final batch of {len(batch_data)}. Total inserted: {total_inserted}\")\n",
    "        except duckdb.Error as e:\n",
    "            print(f\"  Error inserting final batch: {e}\")\n",
    "\n",
    "    print(f\"Batch insertion complete. Total records inserted: {total_inserted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5efc5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to DuckDB database: roadworks_data.duckdb\n",
      "Creating or replacing table: raw_new_roadworks\n",
      "Table 'raw_new_roadworks' created/replaced successfully.\n",
      "\n",
      "Processing NEW format data...\n",
      "Starting batch insertion into 'raw_new_roadworks' (batch size: 1000)...\n",
      "\n",
      "--- Processing Directory: data/new_format ---\n",
      "Found 8 XML files.\n",
      "  Inserted batch of 1000. Total inserted: 1000\n",
      "  Inserted batch of 1000. Total inserted: 2000\n",
      "  Inserted batch of 1000. Total inserted: 3000\n",
      "  Inserted batch of 1000. Total inserted: 4000\n",
      "  Inserted batch of 1000. Total inserted: 5000\n",
      "  Inserted batch of 1000. Total inserted: 6000\n",
      "  Inserted batch of 1000. Total inserted: 7000\n",
      "  Inserted batch of 1000. Total inserted: 8000\n",
      "  Inserted batch of 1000. Total inserted: 9000\n",
      "  Inserted batch of 1000. Total inserted: 10000\n",
      "  Inserted batch of 1000. Total inserted: 11000\n",
      "--- Directory Scan Complete: data/new_format ---\n",
      "Successfully yielded 11353 records.\n",
      "  Inserted final batch of 353. Total inserted: 11353\n",
      "Batch insertion complete. Total records inserted: 11353\n",
      "\n",
      "Creating or replacing table: raw_old_roadworks\n",
      "Table 'raw_old_roadworks' created/replaced successfully.\n",
      "\n",
      "Processing OLD format data...\n",
      "Starting batch insertion into 'raw_old_roadworks' (batch size: 1000)...\n",
      "\n",
      "--- Processing Directory: data/old_format ---\n",
      "Found 7 XML files.\n",
      "  Inserted batch of 1000. Total inserted: 1000\n",
      "  Inserted batch of 1000. Total inserted: 2000\n",
      "  Inserted batch of 1000. Total inserted: 3000\n",
      "  Inserted batch of 1000. Total inserted: 4000\n",
      "  Inserted batch of 1000. Total inserted: 5000\n",
      "  Inserted batch of 1000. Total inserted: 6000\n",
      "  Inserted batch of 1000. Total inserted: 7000\n",
      "  Inserted batch of 1000. Total inserted: 8000\n",
      "  Inserted batch of 1000. Total inserted: 9000\n",
      "  Inserted batch of 1000. Total inserted: 10000\n",
      "  Inserted batch of 1000. Total inserted: 11000\n",
      "  Inserted batch of 1000. Total inserted: 12000\n",
      "--- Directory Scan Complete: data/old_format ---\n",
      "Successfully yielded 12068 records.\n",
      "  Inserted final batch of 68. Total inserted: 12068\n",
      "Batch insertion complete. Total records inserted: 12068\n",
      "\n",
      "Committing transaction...\n",
      "Transaction committed.\n",
      "\n",
      "Verification: Table 'raw_new_roadworks' now contains 11353 rows.\n",
      "Verification: Table 'raw_old_roadworks' now contains 12068 rows.\n",
      "Database connection closed.\n",
      "\n",
      "--- Raw Data Pipeline Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Main Data Processing and Loading (Batch Mode) ---\n",
    "\n",
    "print(f\"Connecting to DuckDB database: {DUCKDB_FILE}\")\n",
    "\n",
    "con = None # Initialize connection variable\n",
    "try:\n",
    "    con = duckdb.connect(database=DUCKDB_FILE, read_only=False)\n",
    "\n",
    "    # --- Create/Replace RAW NEW Table Structure ---\n",
    "    print(f\"Creating or replacing table: {RAW_NEW_TABLE_NAME}\")\n",
    "    # Quote column names\n",
    "    new_column_defs = [f'\"{col}\" VARCHAR' for col in RAW_NEW_COLUMNS]\n",
    "    create_new_table_sql = f'CREATE OR REPLACE TABLE \"{RAW_NEW_TABLE_NAME}\" ({\", \".join(new_column_defs)})'\n",
    "    con.execute(create_new_table_sql)\n",
    "    print(f\"Table '{RAW_NEW_TABLE_NAME}' created/replaced successfully.\")\n",
    "\n",
    "    # --- Process and Load New Format Raw Data ---\n",
    "    print(\"\\nProcessing NEW format data...\")\n",
    "    new_data_iterator = process_directory(\n",
    "        directory_path=NEW_DATA_DIRECTORY,\n",
    "        record_xpath=NEW_ROADWORK_RECORD_XPATH,\n",
    "        extraction_func=extract_record_new_format,\n",
    "        nsmap=NSMAP\n",
    "    )\n",
    "    # Load into the raw new table using the specific columns\n",
    "    load_data_in_batches(con, RAW_NEW_TABLE_NAME, RAW_NEW_COLUMNS, new_data_iterator)\n",
    "\n",
    "    # --- Create/Replace RAW OLD Table Structure ---\n",
    "    print(f\"\\nCreating or replacing table: {RAW_OLD_TABLE_NAME}\")\n",
    "    # Quote column names\n",
    "    old_column_defs = [f'\"{col}\" VARCHAR' for col in RAW_OLD_COLUMNS]\n",
    "    create_old_table_sql = f'CREATE OR REPLACE TABLE \"{RAW_OLD_TABLE_NAME}\" ({\", \".join(old_column_defs)})'\n",
    "    con.execute(create_old_table_sql)\n",
    "    print(f\"Table '{RAW_OLD_TABLE_NAME}' created/replaced successfully.\")\n",
    "\n",
    "    # --- Process and Load Old Format Raw Data ---\n",
    "    print(\"\\nProcessing OLD format data...\")\n",
    "    old_data_iterator = process_directory(\n",
    "        directory_path=OLD_DATA_DIRECTORY,\n",
    "        record_xpath=OLD_ROADWORK_RECORD_XPATH,\n",
    "        extraction_func=extract_record_old_format,\n",
    "        nsmap=None # No namespace needed for old format XPath\n",
    "    )\n",
    "    # Load into the raw old table using the specific columns\n",
    "    load_data_in_batches(con, RAW_OLD_TABLE_NAME, RAW_OLD_COLUMNS, old_data_iterator)\n",
    "\n",
    "    # --- Finalize ---\n",
    "    print(\"\\nCommitting transaction...\")\n",
    "    con.commit()\n",
    "    print(\"Transaction committed.\")\n",
    "\n",
    "    # Verify final counts\n",
    "    count_new = con.execute(f'SELECT COUNT(*) FROM \"{RAW_NEW_TABLE_NAME}\"').fetchone()\n",
    "    count_old = con.execute(f'SELECT COUNT(*) FROM \"{RAW_OLD_TABLE_NAME}\"').fetchone()\n",
    "    print(f\"\\nVerification: Table '{RAW_NEW_TABLE_NAME}' now contains {count_new[0]} rows.\")\n",
    "    print(f\"Verification: Table '{RAW_OLD_TABLE_NAME}' now contains {count_old[0]} rows.\")\n",
    "\n",
    "\n",
    "except duckdb.Error as e_db:\n",
    "    print(f\"\\nDatabase error occurred: {e_db}\")\n",
    "    if con:\n",
    "        try:\n",
    "            print(\"Attempting to rollback transaction.\")\n",
    "            con.rollback()\n",
    "        except duckdb.Error as e_tx: # More specific exception type if available\n",
    "            print(f\"Rollback failed: {e_tx}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "    if con:\n",
    "        try:\n",
    "            con.rollback()\n",
    "        except duckdb.Error as e_tx:\n",
    "            print(f\"Rollback failed: {e_tx}\")\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "        print(\"Database connection closed.\")\n",
    "\n",
    "print(\"\\n--- Raw Data Pipeline Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf80de",
   "metadata": {},
   "source": [
    "## Analyze data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2898f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to roadworks_data.duckdb for quality checks...\n",
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# --- Basic Quality Checks Setup ---\n",
    "\n",
    "con = None\n",
    "\n",
    "def run_query(connection, sql_query):\n",
    "    \"\"\"Helper function to run a query and return a Polars DataFrame.\"\"\"\n",
    "    if not connection:\n",
    "        print(\"Error: Database connection is not established.\")\n",
    "        return None\n",
    "    try:\n",
    "        # print(f\"Running query:\\n{sql_query}\") # Optional: print query being run\n",
    "        return connection.sql(sql_query).pl()\n",
    "    except duckdb.Error as e:\n",
    "        print(f\"Error running query:\\n{sql_query}\\nError: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Establish connection (read-only)\n",
    "try:\n",
    "    print(f\"Connecting to {DUCKDB_FILE} for quality checks...\")\n",
    "    con = duckdb.connect(database=DUCKDB_FILE, read_only=True)\n",
    "    print(\"Connection successful.\")\n",
    "except duckdb.Error as e:\n",
    "    print(f\"Error connecting to database: {e}\")\n",
    "    con = None # Ensure con_check is None if connection failed\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during connection: {e}\")\n",
    "    con = None\n",
    "\n",
    "# Define common placeholders to check\n",
    "PLACEHOLDERS = [\"''\", \"'none'\", \"'n/a'\", \"'null'\", \"'unknown'\"]\n",
    "#PLACEHOLDER_FILTER = \" OR \".join([f'lower(\"{col}\") = {p}' for p in PLACEHOLDERS])\n",
    "\n",
    "# Define tables and columns to iterate over\n",
    "TABLES_INFO = {\n",
    "    RAW_NEW_TABLE_NAME: RAW_NEW_COLUMNS,\n",
    "    RAW_OLD_TABLE_NAME: RAW_OLD_COLUMNS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e32d6ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting DuckDB Database: roadworks_data.duckdb ---\n",
      "--- Inspecting Table: raw_new_roadworks ---\n",
      "\n",
      "Schema for table 'raw_new_roadworks':\n",
      "Table 'raw_new_roadworks' found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (13, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_name</th><th>column_type</th><th>null</th><th>key</th><th>default</th><th>extra</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;source_filename&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;NEW_EVENT_NUMBER&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;OLD_REFERENCE_NUMBER&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;SDATE&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;EDATE&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;STATUS&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;PUBLISHED_DATE&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;CENTRE_EASTING&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;CENTRE_NORTHING&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;ROAD_NUMBERS&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (13, 6)\n",
       "\n",
       " column_name           column_type  null  key   default  extra \n",
       " ---                   ---          ---   ---   ---      ---   \n",
       " str                   str          str   str   str      str   \n",
       "\n",
       " source_filename       VARCHAR      YES   null  null     null  \n",
       " NEW_EVENT_NUMBER      VARCHAR      YES   null  null     null  \n",
       " OLD_REFERENCE_NUMBER  VARCHAR      YES   null  null     null  \n",
       " SDATE                 VARCHAR      YES   null  null     null  \n",
       " EDATE                 VARCHAR      YES   null  null     null  \n",
       "                                                         \n",
       " STATUS                VARCHAR      YES   null  null     null  \n",
       " PUBLISHED_DATE        VARCHAR      YES   null  null     null  \n",
       " CENTRE_EASTING        VARCHAR      YES   null  null     null  \n",
       " CENTRE_NORTHING       VARCHAR      YES   null  null     null  \n",
       " ROAD_NUMBERS          VARCHAR      YES   null  null     null  \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in 'raw_new_roadworks': 11353\n",
      "\n",
      "First 5 rows from 'raw_new_roadworks':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>NEW_EVENT_NUMBER</th><th>OLD_REFERENCE_NUMBER</th><th>SDATE</th><th>EDATE</th><th>EXPDEL</th><th>DESCRIPTION</th><th>CLOSURE_TYPE</th><th>STATUS</th><th>PUBLISHED_DATE</th><th>CENTRE_EASTING</th><th>CENTRE_NORTHING</th><th>ROAD_NUMBERS</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00026976-005&quot;</td><td>null</td><td>&quot;26-FEB-2018 21:00&quot;</td><td>&quot;28-FEB-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A3 northbound Sheet Link entry</td><td>&quot;Area Renewals&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-22T16:49:17&quot;</td><td>&quot;475209&quot;</td><td>&quot;124975&quot;</td><td>&quot;A3&quot;</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00004020-008&quot;</td><td>&quot;4188720&quot;</td><td>&quot;08-JAN-2018 20:00&quot;</td><td>&quot;10-MAR-2018 06:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;A14 Westbound\n",
       "Jct 58 to Jct 57</td><td>&quot;Area Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-22T10:13:27&quot;</td><td>&quot;614569&quot;</td><td>&quot;241115&quot;</td><td>&quot;A14&quot;</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00001459-026&quot;</td><td>&quot;4215713&quot;</td><td>&quot;31-JUL-2017 14:47&quot;</td><td>&quot;01-APR-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;M1 northbound and southbound T</td><td>&quot;Major Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-15T14:38:05&quot;</td><td>&quot;445124&quot;</td><td>&quot;364308&quot;</td><td>&quot;M1&quot;</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00027883-003&quot;</td><td>null</td><td>&quot;12-FEB-2018 20:00&quot;</td><td>&quot;17-MAR-2018 06:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;A259, east and westbound betwe</td><td>&quot;Area Schemes&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-21T10:36:47&quot;</td><td>&quot;596442&quot;</td><td>&quot;123787&quot;</td><td>&quot;A259&quot;</td></tr><tr><td>&quot;he_roadworks_2018_02_26.xml&quot;</td><td>&quot;00026799-002&quot;</td><td>null</td><td>&quot;10-FEB-2018 22:00&quot;</td><td>&quot;22-MAR-2018 06:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;A3 northbound Compton to Denni</td><td>&quot;Regional Technology Works&quot;</td><td>&quot;Published&quot;</td><td>&quot;2018-02-22T14:08:43&quot;</td><td>&quot;498261&quot;</td><td>&quot;150727&quot;</td><td>&quot;A3&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "\n",
       " source_fi  NEW_EVENT  OLD_REFER  SDATE        PUBLISHED  CENTRE_EA  CENTRE_NO  ROAD_NUM \n",
       " lename     _NUMBER    ENCE_NUMB  ---           _DATE      STING      RTHING     BERS     \n",
       " ---        ---        ER         str           ---        ---        ---        ---      \n",
       " str        str        ---                      str        str        str        str      \n",
       "                       str                                                                \n",
       "\n",
       " he_roadwo  00026976-  null       26-FEB-20    2018-02-2  475209     124975     A3       \n",
       " rks_2018_  005                   18 21:00      2T16:49:1                                 \n",
       " 02_26.xml                                      7                                         \n",
       " he_roadwo  00004020-  4188720    08-JAN-20    2018-02-2  614569     241115     A14      \n",
       " rks_2018_  008                   18 20:00      2T10:13:2                                 \n",
       " 02_26.xml                                      7                                         \n",
       " he_roadwo  00001459-  4215713    31-JUL-20    2018-02-1  445124     364308     M1       \n",
       " rks_2018_  026                   17 14:47      5T14:38:0                                 \n",
       " 02_26.xml                                      5                                         \n",
       " he_roadwo  00027883-  null       12-FEB-20    2018-02-2  596442     123787     A259     \n",
       " rks_2018_  003                   18 20:00      1T10:36:4                                 \n",
       " 02_26.xml                                      7                                         \n",
       " he_roadwo  00026799-  null       10-FEB-20    2018-02-2  498261     150727     A3       \n",
       " rks_2018_  002                   18 22:00      2T14:08:4                                 \n",
       " 02_26.xml                                      3                                         \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inspecting Table: raw_old_roadworks ---\n",
      "\n",
      "Schema for table 'raw_old_roadworks':\n",
      "Table 'raw_old_roadworks' found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (15, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_name</th><th>column_type</th><th>null</th><th>key</th><th>default</th><th>extra</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;source_filename&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;reference_number&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;start_date&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;end_date&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;expected_delay&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;centre_northing&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;road&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;location&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;local_authority&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;traffic_management&quot;</td><td>&quot;VARCHAR&quot;</td><td>&quot;YES&quot;</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (15, 6)\n",
       "\n",
       " column_name         column_type  null  key   default  extra \n",
       " ---                 ---          ---   ---   ---      ---   \n",
       " str                 str          str   str   str      str   \n",
       "\n",
       " source_filename     VARCHAR      YES   null  null     null  \n",
       " reference_number    VARCHAR      YES   null  null     null  \n",
       " start_date          VARCHAR      YES   null  null     null  \n",
       " end_date            VARCHAR      YES   null  null     null  \n",
       " expected_delay      VARCHAR      YES   null  null     null  \n",
       "                                                       \n",
       " centre_northing     VARCHAR      YES   null  null     null  \n",
       " road                VARCHAR      YES   null  null     null  \n",
       " location            VARCHAR      YES   null  null     null  \n",
       " local_authority     VARCHAR      YES   null  null     null  \n",
       " traffic_management  VARCHAR      YES   null  null     null  \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in 'raw_old_roadworks': 12068\n",
      "\n",
      "First 5 rows from 'raw_old_roadworks':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_filename</th><th>reference_number</th><th>start_date</th><th>end_date</th><th>expected_delay</th><th>description</th><th>closure_type</th><th>status</th><th>published_date</th><th>centre_easting</th><th>centre_northing</th><th>road</th><th>location</th><th>local_authority</th><th>traffic_management</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;972963&quot;</td><td>&quot;2010-07-12T07:00:00&quot;</td><td>&quot;2013-03-23T06:00:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;Major junction works will incl</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2011-10-09T21:08:32&quot;</td><td>&quot;456252&quot;</td><td>&quot;278173&quot;</td><td>&quot;M1&quot;</td><td>&quot;Catthorpe&quot;</td><td>&quot;Leicestershire / Northamptonsh</td><td>&quot;Other&quot;</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;978905&quot;</td><td>&quot;2011-04-01T22:00:00&quot;</td><td>&quot;2011-12-31T05:00:00&quot;</td><td>&quot;Moderate (10 - 30 mins)&quot;</td><td>&quot;Contraflow with speed restrict</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2010-04-23T10:18:30&quot;</td><td>&quot;499082&quot;</td><td>&quot;235992&quot;</td><td>&quot;M1&quot;</td><td>&quot;Jct 13 to Jct 12&quot;</td><td>&quot;Bedfordshire / Buckinghamshire&quot;</td><td>&quot;Contraflow&quot;</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;998294&quot;</td><td>&quot;2009-09-24T06:00:00&quot;</td><td>&quot;2013-09-24T05:00:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;Lane 1 closure and 24/7 Hardsh</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2010-06-19T05:03:50&quot;</td><td>&quot;465924&quot;</td><td>&quot;260154&quot;</td><td>&quot;M1&quot;</td><td>&quot;Approach to Junction 16 (21011</td><td>&quot;Northamptonshire&quot;</td><td>&quot;Lane Closure&quot;</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;1172899&quot;</td><td>&quot;2011-10-10T22:00:00&quot;</td><td>&quot;2011-12-03T06:00:00&quot;</td><td>&quot;Slight (less than 10 mins)&quot;</td><td>&quot;Lane closures during the day w</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2011-09-28T15:40:36&quot;</td><td>&quot;446842&quot;</td><td>&quot;324130&quot;</td><td>&quot;M1&quot;</td><td>&quot;Junction 23a (220116)&quot;</td><td>&quot;Leicestershire&quot;</td><td>&quot;Carriageway Closure&quot;</td></tr><tr><td>&quot;ha-roadworks_2011_10_10.xml&quot;</td><td>&quot;1306529&quot;</td><td>&quot;2010-08-04T00:00:00&quot;</td><td>&quot;2012-07-05T00:00:00&quot;</td><td>&quot;No Delay&quot;</td><td>&quot;24hrs, lane 1 closure, northbo</td><td>&quot;Planned Works&quot;</td><td>&quot;Firm&quot;</td><td>&quot;2011-08-22T16:47:52&quot;</td><td>&quot;511897&quot;</td><td>&quot;202047&quot;</td><td>&quot;M1&quot;</td><td>&quot;Jct 6 Exit Slip&quot;</td><td>&quot;Hertfordshire&quot;</td><td>&quot;Lane Closure&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 15)\n",
       "\n",
       " source_fil  reference_  start_date  end_date      road  location   local_aut  traffic_m \n",
       " ename       number      ---         ---            ---   ---        hority     anagement \n",
       " ---         ---         str         str            str   str        ---        ---       \n",
       " str         str                                                     str        str       \n",
       "\n",
       " ha-roadwor  972963      2010-07-12  2013-03-23    M1    Catthorpe  Leicester  Other     \n",
       " ks_2011_10              T07:00:00   T06:00:00                       shire /              \n",
       " _10.xml                                                             Northampt            \n",
       "                                                                     onsh                \n",
       " ha-roadwor  978905      2011-04-01  2011-12-31    M1    Jct 13 to  Bedfordsh  Contraflo \n",
       " ks_2011_10              T22:00:00   T05:00:00            Jct 12     ire / Buc  w         \n",
       " _10.xml                                                             kinghamsh            \n",
       "                                                                     ire                  \n",
       " ha-roadwor  998294      2009-09-24  2013-09-24    M1    Approach   Northampt  Lane      \n",
       " ks_2011_10              T06:00:00   T05:00:00            to         onshire    Closure   \n",
       " _10.xml                                                  Junction                        \n",
       "                                                          16                              \n",
       "                                                          (21011                         \n",
       " ha-roadwor  1172899     2011-10-10  2011-12-03    M1    Junction   Leicester  Carriagew \n",
       " ks_2011_10              T22:00:00   T06:00:00            23a        shire      ay        \n",
       " _10.xml                                                  (220116)              Closure   \n",
       " ha-roadwor  1306529     2010-08-04  2012-07-05    M1    Jct 6      Hertfords  Lane      \n",
       " ks_2011_10              T00:00:00   T00:00:00            Exit Slip  hire       Closure   \n",
       " _10.xml                                                                                  \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inspection Complete ---\n"
     ]
    }
   ],
   "source": [
    "new_table = RAW_NEW_TABLE_NAME\n",
    "old_table = RAW_OLD_TABLE_NAME\n",
    "\n",
    "print(f\"--- Inspecting DuckDB Database: {DUCKDB_FILE} ---\")\n",
    "\n",
    "if not os.path.exists(DUCKDB_FILE):\n",
    "    print(f\"Error: Database file '{DUCKDB_FILE}' not found.\")\n",
    "elif not con_check: # Check if the connection from the previous cell was successful\n",
    "     print(f\"Error: Cannot inspect database. Connection 'con_check' not established.\")\n",
    "else:\n",
    "    # Connection is already established via con_check in the previous cell\n",
    "\n",
    "    # --- Inspect NEW Raw Table ---\n",
    "    print(f\"--- Inspecting Table: {new_table} ---\")\n",
    "    try:\n",
    "        # Describe schema using run_query\n",
    "        print(f\"\\nSchema for table '{new_table}':\")\n",
    "        schema_df_new = run_query(con, f'DESCRIBE \"{new_table}\"')\n",
    "        if schema_df_new is not None and not schema_df_new.is_empty():\n",
    "            print(f\"Table '{new_table}' found.\")\n",
    "            display(schema_df_new)\n",
    "        else:\n",
    "             # If DESCRIBE fails or returns empty, the table likely doesn't exist or there was an error\n",
    "             print(f\"Could not retrieve schema for table '{new_table}'. It might not exist or there was a query error.\")\n",
    "             # Skip further inspection for this table\n",
    "             raise duckdb.CatalogException(f\"Table '{new_table}' not found or query failed.\") # Raise exception to skip next steps\n",
    "\n",
    "        # Count rows using run_query\n",
    "        count_df_new = run_query(con, f'SELECT COUNT(*) as count FROM \"{new_table}\"')\n",
    "        if count_df_new is not None and not count_df_new.is_empty():\n",
    "            count_new_val = count_df_new[0, \"count\"]\n",
    "            print(f\"\\nTotal rows in '{new_table}': {count_new_val}\")\n",
    "        else:\n",
    "            print(f\"Could not count rows for table '{new_table}'.\")\n",
    "            count_new_val = 0 # Assume 0 if count fails\n",
    "\n",
    "        # Display sample rows using run_query (only if table has rows)\n",
    "        if count_new_val > 0:\n",
    "            print(f\"\\nFirst 5 rows from '{new_table}':\")\n",
    "            sample_df_new = run_query(con, f'SELECT * FROM \"{new_table}\" LIMIT 5')\n",
    "            if sample_df_new is not None and not sample_df_new.is_empty():\n",
    "                # print(type(sample_df_new)) # Type is known to be Polars DataFrame\n",
    "                display(sample_df_new)\n",
    "            elif sample_df_new is not None and sample_df_new.is_empty():\n",
    "                 print(\"Table has rows, but could not fetch sample (LIMIT 5 returned empty).\")\n",
    "            else:\n",
    "                 print(\"Could not fetch sample rows.\")\n",
    "        elif count_new_val == 0:\n",
    "             print(\"\\nTable appears to be empty.\")\n",
    "\n",
    "\n",
    "    except duckdb.CatalogException as e: # Catch specific error if DESCRIBE failed as intended\n",
    "         print(f\"Skipping further inspection for '{new_table}' due to previous error: {e}\")\n",
    "    except Exception as e: # Catch any other unexpected errors during inspection\n",
    "         print(f\"An unexpected error occurred while inspecting '{new_table}': {e}\")\n",
    "\n",
    "\n",
    "    # --- Inspect OLD Raw Table ---\n",
    "    print(f\"\\n--- Inspecting Table: {old_table} ---\")\n",
    "    try:\n",
    "        # Describe schema using run_query\n",
    "        print(f\"\\nSchema for table '{old_table}':\")\n",
    "        schema_df_old = run_query(con, f'DESCRIBE \"{old_table}\"')\n",
    "        if schema_df_old is not None and not schema_df_old.is_empty():\n",
    "            print(f\"Table '{old_table}' found.\")\n",
    "            display(schema_df_old)\n",
    "        else:\n",
    "             print(f\"Could not retrieve schema for table '{old_table}'. It might not exist or there was a query error.\")\n",
    "             raise duckdb.CatalogException(f\"Table '{old_table}' not found or query failed.\")\n",
    "\n",
    "        # Count rows using run_query\n",
    "        count_df_old = run_query(con, f'SELECT COUNT(*) as count FROM \"{old_table}\"')\n",
    "        if count_df_old is not None and not count_df_old.is_empty():\n",
    "            count_old_val = count_df_old[0, \"count\"]\n",
    "            print(f\"\\nTotal rows in '{old_table}': {count_old_val}\")\n",
    "        else:\n",
    "            print(f\"Could not count rows for table '{old_table}'.\")\n",
    "            count_old_val = 0\n",
    "\n",
    "        # Display sample rows using run_query (only if table has rows)\n",
    "        if count_old_val > 0:\n",
    "            print(f\"\\nFirst 5 rows from '{old_table}':\")\n",
    "            sample_df_old = run_query(con, f'SELECT * FROM \"{old_table}\" LIMIT 5')\n",
    "            if sample_df_old is not None and not sample_df_old.is_empty():\n",
    "                display(sample_df_old)\n",
    "            elif sample_df_old is not None and sample_df_old.is_empty():\n",
    "                 print(\"Table has rows, but could not fetch sample (LIMIT 5 returned empty).\")\n",
    "            else:\n",
    "                 print(\"Could not fetch sample rows.\")\n",
    "        elif count_old_val == 0:\n",
    "             print(\"\\nTable appears to be empty.\")\n",
    "\n",
    "\n",
    "    except duckdb.CatalogException as e:\n",
    "         print(f\"Skipping further inspection for '{old_table}' due to previous error: {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"An unexpected error occurred while inspecting '{old_table}': {e}\")\n",
    "\n",
    "    # No need to close con_inspect as we are using the global con_check\n",
    "    # The con_check connection will be closed later after all checks are done.\n",
    "    # print(\"\\nInspection connection closed.\") # Remove this line\n",
    "\n",
    "print(\"\\n--- Inspection Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21be031",
   "metadata": {},
   "source": [
    "### Basic checks\n",
    "1. Count NULLs & empty string placeholders\n",
    "1. Check string length range of each column (e.g.: Is NEW_EVENT_NUMBER fixed length?)\n",
    "1. Examine categorical values (e.g. STATUS, EXPDEL)\n",
    "1. Check identifyer uniqueness across tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_inspect = None\n",
    "try:\n",
    "    # Connect in read-only mode\n",
    "    con_inspect = duckdb.connect(database=DUCKDB_FILE, read_only=True)\n",
    "    \n",
    "    # Analyze data...\n",
    "    \n",
    "    \n",
    "except duckdb.Error as e:\n",
    "    print(f\"Could not connect to database '{DUCKDB_FILE}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64226952",
   "metadata": {},
   "source": [
    "### Convert data types\n",
    "1. Numeric conversion (coordinates, reference number, NEW_EVENT_NUMBER?)\n",
    "1. Convert dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c070b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dae47f6d",
   "metadata": {},
   "source": [
    "### Check converted data types for plausibility\n",
    "1. Date ranges\n",
    "1. Coordinate ranges (Correct locations in the UK?)\n",
    "1. Did numeric conversions succeed?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
